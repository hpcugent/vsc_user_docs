\chapter{Running batch jobs}
\label{ch:running-batch-jobs}

In order to have access to the compute nodes of a cluster, one has to make use
of the job system. The system software that handles your batch jobs consists of
two pieces: the queue- and resource manager \strong{TORQUE} and the scheduler
\strong{Moab}. Together, TORQUE and Moab provide a suite of commands for
submitting jobs, altering some of the properties of waiting jobs (such as
reordering or deleting them), monitoring their progress and killing ones that
are having problems or are no longer needed. Only the most commonly used
commands are mentioned here.

\begin{center}
\includegraphics*[width=5.78in, height=3.94in, keepaspectratio=false]{ch4-pbs-overview}
\end{center}

When you connect to the \hpc, you have access to (one of) the \strong{login
nodes} of the cluster. There you can prepare the work you want to get done on
the cluster by, e.g., installing or compiling programs, setting up data sets,
etc. The computations however, should not be performed on this login node. The
actual work is done on the cluster's \strong{compute nodes}. These compute
nodes are managed by the job scheduling software (Moab) and a Resource Manager
(TORQUE), which decides when and on which compute nodes the jobs can run. It is
usually not necessary (and in some cases not permitted) to log on to the
compute nodes directly. Users can (and should) monitor their jobs
periodically as they run, but do not have to remain logged in the entire
time.

The documentation in this ``Running batch jobs'' section includes a description
of the general features of job scripts, how to submit them for execution and
how to monitor their progress.

\section{Defining and submitting your job}

Usually, you will want to have your program running in batch mode, as opposed
to interactively as you may be accustomed to. The point is that the program
must be able to start and run without user intervention, i.e., without you
having to enter any information or to press any buttons during program
execution. All the necessary input or required options have to be specified on
the command line, or needs to be put in input or configuration files.

As an example, we will run a perl script, which you will find in the examples
subdirectory on the \hpc. When you received an account to the \hpc a
subdirectory with examples was automatically generated for you.

Remember that you have copied the contents of the HPC examples directory to
your home directory, so that you have your \strong{own personal} copy (editable
and over-writable) and that you can start using the examples. If you haven't
done so already, run these commands now:

\begin{prompt}
%\shellcmd{cd}%
%\shellcmd{cp --r \examplesdir \tilde/}%
\end{prompt}

First go to the directory with the first examples by entering the command:

\begin{prompt}
%\shellcmd{cd ~/\exampledir}%
\end{prompt}

Each time you want to execute a program on the \hpc you'll need 2 things:

\begin{description}
  \item[The executable] The program to execute from the end-user, together with its peripheral input files, databases and/or command options.
  \item[A configuration script] (also called a job-script), which will define the computer resource requirements of the program, the required additional software packages and which will start the actual executable.  The \hpc needs to know:
    \begin{enumerate}
      \item  the type of compute nodes;
      \item  the number of CPUs;
      \item  the amount of memory;
      \item  the expected duration of the execution time (wall time: Time as mesaured by a clock on thew all);
      \item  the name of the files which will contain the output (i.e., stdout) and error (i.e., stderr) messages;
      \item  what executable to start, and its arguments.
    \end{enumerate}
\end{description}

Later on, the \hpc user shall have to define (or to adapt) his/her own
configuration scripts. For now, all required configuration scripts for the
exercises are provided for you in the examples subdirectories.

List and check the contents with:

\begin{prompt}
%\shellcmd{ls -l}%
total 512
-rw-r--r-- 1 %\userid% 193 Sep 11 10:34 fibo.pbs
-rw-r--r-- 1 %\userid% 609 Sep 11 10:25 fibo.pl
\end{prompt}

In this directory you find a Perl script (named ``fibo.pl'') and a job-script
(named ``fibo.pbs'').

\begin{enumerate}
\item  The Perl script calculates the first 30 Fibonacci numbers.
\item  The job-script is actually a standard Unix/Linux shell script that
  contains a few extra comments at the beginning that specify directives to
  PBS.  These comments all begin with \strong{\#PBS}.
\end{enumerate}

We will first execute the program locally (i.e., on your current login-node),
so that you can see what the program does.

On the command line, you would run this using:
\begin{prompt}
%\shellcmd{./fibo.pl}%
[0] -> 0
[1] -> 1
[2] -> 1
[3] -> 2
[4] -> 3
[5] -> 5
[6] -> 8
[7] -> 13
[8] -> 21
[9] -> 34
[10] -> 55
[11] -> 89
[12] -> 144
[13] -> 233
[14] -> 377
[15] -> 610
[16] -> 987
[17] -> 1597
[18] -> 2584
[19] -> 4181
[20] -> 6765
[21] -> 10946
[22] -> 17711
[23] -> 28657
[24] -> 46368
[25] -> 75025
[26] -> 121393
[27] -> 196418
[28] -> 317811
[29] -> 514229
\end{prompt}

\underbar{Remark}: Recall that you have now executed the Perl script locally on
one of the login-nodes of the \hpc cluster.  Of course, this is not our final
intention; we want to run the script on any of the compute nodes. Also, it is
not considered as good practice, if you ``abuse'' the login-nodes for testing
your scripts and executables. It will be explained later on how you can
reserve your own compute-node (by opening an interactive session) to test
your software. But for the sake of acquiring a good understanding of what is
happening, you are pardoned for this example since these jobs require very little
computing power.

The job-script contains a description of the job by specifying the command that
need to be executed on the compute node:

\examplecode{bash}{fibo.pbs}

So, jobs are submitted as scripts (bash, Perl, Python, etc.), which specify the
parameters related to the jobs such as expected runtime (walltime), e-mail
notification, etc. These parameters can also be specified on the command line.

This job script that can now be submitted to the cluster's job system for
execution, using the qsub (Queue SUBmit) command:

\begin{prompt}
%\shellcmd{qsub fibo.pbs}%
%\jobid%
\end{prompt}

The qsub command returns a job identifier on the HPC cluster. The important
part is the number (e.g., ``\jobnumber''); this is a unique identifier for the job
and can be used to monitor and manage your job.

Your job is now waiting in the queue for a free workernode to start on.

Go and drink some coffee \dots\ but not too long. If you get impatient you can
start reading the next section for more information on how to monitor jobs in the queue.

After your job was started, and ended, check the contents of the directory:

\begin{prompt}
%\shellcmd{ls -l}%
total 768
-rw-r--r-- 1 %\userid \userid%   44 Feb 28 13:33 fibo.pbs
-rw------- 1 %\userid \userid%    0 Feb 28 13:33 fibo.pbs.e%\jobnumber%
-rw------- 1 %\userid \userid% 1010 Feb 28 13:33 fibo.pbs.o%\jobnumber%
-rwxrwxr-x 1 %\userid \userid%  302 Feb 28 13:32 fibo.pl*
\end{prompt}

Explore the contents of the 2 new files:

\begin{prompt}
%\shellcmd{more fibo.pbs.o\jobnumber}%
%\shellcmd{more fibo.pbs.e\jobnumber}%
\end{prompt}

These files are used to store the standard output and error that would
otherwise be shown in the terminal window. By default, they have the same name
as that of the PBS script, i.e., ``fibo.pbs'' as base name, followed by the
extension ``.o'' (output) and ``.e'' (error), respectively, and the job number
('\jobnumber' for this example). The error file will be empty, at least if all went
well. If not, it may contain valuable information to determine and remedy the
problem that prevented a successful run. The standard output file will contain
the results of your calculation (here, the output of the perl script)

\section{Monitoring and managing your job(s)}

Using the job ID that \textit{qsub} returned, there are various ways to monitor
the status of you job, e.g.,

To get the status information on your job:

\begin{prompt}
%\shellcmd{qstat $<$jobid$>$}%
\end{prompt}

To show an estimated start time for you job (note that this may be very inaccurate,
the margin of error on this figure can be bigger then 100\% due to a sample in a
population of 1.)
This command is not available on all systems.

\begin{prompt}
%\shellcmd{showstart $<$jobid$>$}%
\end{prompt}

This is only a very rough estimate. Jobs may launch sooner than estimated if
other jobs end faster than estimated, but may also be delayed if other
higher-priority jobs enter the system.

To show the status, but also the resources required by the job, with error
messages that may prevent your job from starting:

\begin{prompt}
%\shellcmd{checkjob $<$jobid$>$}%
\end{prompt}

To show on which compute nodes you job is running, at least, when it is
running:

\begin{prompt}
%\shellcmd{qstat -n $<$jobid$>$}%
\end{prompt}

To remove a job from the queue so that it will not run, or to stop a job that
is already running.

\begin{prompt}
%\shellcmd{qdel $<$jobid$>$}%
\end{prompt}

When you have submitted several jobs (or you just forgot about the job ID), you
can retrieve the status of all your jobs that are submitted and are not yet
finished using (uid is your VSC user name on the system):

% Manually indented, please keep this as is
\begin{prompt}
%\shellcmd{qstat}%
%\pbsserver%:
Job ID      Name    User      Time Use S Queue
----------- ------- --------- -------- - -----
%\jobnumber%.%\dots% mpi     %\userid% 0        Q short
\end{prompt}

Here:
\begin{description}
  \item[Job ID] the job's unique identifier
  \item[Name] the name of the job
  \item[User] the user that owns the job
  \item[Time Use] the elapsed walltime for the job
  \item[Queue] the queue the job is in
\end{description}

The state S can be any of  the following:

\begin{tabular}{|p{0.4in}|p{3.6in}|} \hline
\strong{State} & \strong{Meaning}                                             \\ \hline
\strong{Q} & The job is \strong{queued} and is waiting to start.              \\ \hline
\strong{R} & The job is currently \strong{running}.                           \\ \hline
\strong{E} & The job is currently \strong{exiting} after having run.          \\ \hline
\strong{C} & The job is \strong{completed} after having run.                  \\ \hline
\strong{H} & The job has a user or system \strong{hold} on it and will not be
  eligible to run until the hold is removed.                                  \\ \hline
\end{tabular}

User hold means that the user can remove the hold. System hold means that the system
or an administrator has put the job on hold, very likely because something is wrong with it.
Check with your helpdesk to see why this is the case.
\section{Examining the queue}

As we learned above, Moab is the software application that actually decides
when to run your job and what resources your job will run on. You can look at
the queue by using either the PBS \strong{qstat} command or the Moab
\strong{showq} command. By default, \strong{qstat} will display the queue
ordered by \strong{JobID}, whereas \strong{showq} will display jobs grouped by
their state (``running'', ``idle'', or ``hold'') then ordered by priority.
Therefore, \strong{showq} is often more useful.
Note however that at some VSC-sites, these commands show only your jobs or may
be even disabled to not reveal what other users are doing.

% Does not work in Ghent: showq is disabled as it shows information about
% other users.
The \strong{showq} command displays information about active (``running''),
eligible (``idle''), blocked (``hold''), and/or recently completed jobs. To get
a summary:

\begin{prompt}
%\shellcmd{showq -s}%
active jobs: 163
eligible jobs: 133
blocked jobs: 243
Total jobs:  539
\end{prompt}

\ifantwerpen
And to get the full detail of all the jobs, which are in the system:

\begin{prompt}
%\shellcmd{showq}%
active jobs------------------------
JOBID     USERNAME  STATE PROCS REMAINING          STARTTIME
428024    vsc20167  Running   8   2:57:32  Mon Sep  2 14:55:05
%\dots%
153 active jobs 1307 of 3360 processors in use by local jobs (38.90%\%%)
153 of 168 nodes active      (91.07%\%%)

eligible jobs----------------------
JOBID     USERNAME  STATE PROCS   WCLIMIT            QUEUETIME
442604    vsc20167   Idle  48  7:00:00:00  Sun Sep 22 16:39:13
442605    vsc20167   Idle  48  7:00:00:00  Sun Sep 22 16:46:22
%\dots%

135 eligible jobs

blocked jobs-----------------------
JOBID   USERNAME     STATE PROCS WCLIMIT            QUEUETIME
441237  vsc20167      Idle   8 3:00:00:00 Thu Sep 19 15:53:10
442536  vsc20167  UserHold  40 3:00:00:00 Sun Sep 22 00:14:22
%\dots%
252 blocked jobs
Total jobs:  540
\end{prompt}
\fi

There are 3 categories, the \strong{active}, \strong{eligible} and \strong{blocked} jobs.

\begin{description}
  \item[Active jobs] are jobs that are running or starting and that consume computer resources. The amount of time remaining (w.r.t.\ walltime, sorted to earliest completion time) and the start time are displayed. This will give you an idea about the foreseen completion time. These jobs could be in a number of states:

  \begin{description}
    \item[Started] attempting to start, performing pre-start tasks
    \item[Running] currently executing the user application
    \item[Suspended] has been suspended by scheduler or admin (still in place
      on the allocated resources, not executing)
    \item[Cancelling] has been cancelled, in process of cleaning up
  \end{description}

  \item[Eligible jobs] are jobs that are waiting in the queues and are
    considered eligible for both scheduling and backfilling.  They are all in
    the idle job state and do not violate any fairness policies or do not have
    any job holds in place. The requested walltime is displayed, and the list
    is ordered by job priority.
  \item[Blocked jobs] are jobs that are ineligible to be run or queued.  These
    jobs could be in a number of states for the following reasons:

  \begin{description}
    \item[Idle] when the job violates a fairness policy
    \item[Userhold] or systemhold when it is user or administrative hold
    \item[Batchhold] when the requested resources are not available or the resource manager has repeatedly failed to start the job
    \item[Deferred] when a temporary hold when the job has been unable to start after a specified number of attempts
    \item[Notqueued] when scheduling daemon is unavailable
  \end{description}
\end{description}

\section{Specifying job requirements}

Without giving more information about your job upon submitting it with
\strong{qsub}, default values will be assumed that are almost never appropriate
for real jobs.

It is important to estimate the resources you need to successfully run your
program, such as the amount of time the job will require, the amount of memory
it needs, the number of CPUs it will run on, etc. This may take some work, but
it is necessary to ensure your jobs will run properly.

\subsection{Generic resource requirements}

The \strong{qsub} command takes several options to specify the requirements, of which we
list the most commonly used once below. \\

\begin{prompt}
%\shellcmd{qsub -l walltime=2:30:00}%
\end{prompt}

For the simplest cases, only the amount of maximum estimated execution time
(called ``walltime'') is really important. Here, the job will not require more
than 2 hours, 30 minutes to complete. As soon as the job would take more time,
it will be ``killed'' (terminated) by the job scheduler.  There is absolutly no
harm if you \emph{slightly} overestimate the maximum execution time. \\

\begin{prompt}
%\shellcmd{qsub -l mem=4gb}%
\end{prompt}

The job requires no more than 4 GB of memory. \\

\begin{prompt}
%\shellcmd{qsub -l nodes=5:ppn=2}%
\end{prompt}

The job requires 5 compute nodes with two cores on each node (ppn stands for
``processors per node'', where processor is used to refer to individual cores). \\

\begin{prompt}
%\shellcmd{qsub -l nodes=1:westmere}%
\end{prompt}

The job requires just one node, but it should have an Intel Westmere processor.
A list with site-specific properties can be found in the next section or in the 
User Portal 
(``Available hardware''-section)\footnote{URL: \url{https://www.vscentrum.be/infrastructure/hardware}}
of the VSC website.

These options can either be specified on the command line, e.g.

\begin{prompt}
%\shellcmd{qsub -l nodes=1:westmere,mem=2gb fibo.pbs}%
\end{prompt}

or in the job-script itself using the \#PBS-directive, so ``fibo.pbs'' could be modified to:

\begin{code}{bash}
#!/bin/bash -l
#PBS -l nodes=1:westmere
#PBS -l mem=2gb
cd $PBS_O_WORKDIR
./fibo.pl
\end{code}

Note that the resources requested on the command line will override those
specified in the PBS file.

\subsection{Available job categories (TORQUE queues)}

In order to guarantee a fair share access to the computer resources to all
users, only a limited number of jobs with certain walltimes are possible per
user.

We therefore classify the submitted jobs in categories (confusingly also called
queues), depending on the their walltime specification.  A user is allowed to
run up to a certain maximum number of jobs in each of these walltime
categories.

The currently defined walltime categories for the \hpc
are:

\inputsite{queue-table}

\underbar{Remark:} As the infrastructure of the \hpc is constantly expanding
and evolving, it can be anticipated that also the limits of the categories can
be changed over time.

\ifantwerpen
% TODO : already adapt this text according to the new common user experience rules, and make it general....
When a user submits a job with a walltime of 6 days, the queue manager will
put the job in the walltime category 3 days--7 days.  The user can submit up to 400 jobs with
this high walltimes (queueable = 400) on hopper, but only 50 of those jobs will be eligible
for execution (runnable=50) at the same time.  A detailed description of the
fair-share mechanisms will follow in \autoref{ch:hpc-policies}. For longer running jobs,
\emph{checkpointing} (See \autoref{ch:checkpointing}) is necessary.
\fi

\iffalse
Apart from specifying the \emph{walltime}, you can also explicitly define the
queue you're submitting your job to.

To specify the queue, add:
\begin{prompt}
-q queuename
\end{prompt}
to the qsub command line, or
\begin{code}{bash}
#PBS -q queuename
\end{code}

to the job-script, where \emph{queuename} is one of the possible queues shown
above.

A maximum \emph{walltime} is associated with each queue.

The queue category logic is:

\begin{tabular}{|p{0.9in}|p{2.0in}|p{2.0in}|} \hline
                                     & \strong{No walltime specified}                                          & \strong{Walltime specified} \\ \hline
\strong{No queue \newline specified} & The job is submitted in the qshort queue with a walltime of 1 hour.     & The job is submitted in the proper queue in accordance with the given walltime. \\ \hline
\strong{Queue \newline specified}    & The job is submitted in your specified
  queue with q walltime of 1 hour. & The job is submitted in the specified
  queue with the specified walltime. If the specified \textit{walltime} is
  larger than the maximal \textit{walltime} of the requested queue, the job
  cannot be submitted. \\ \hline
\end{tabular}

\underbar{Remark:} It is highly recommended to specify a walltime at all times
in all your job scripts. Only for some short test-runs, a walltime
specification could be omitted.

When a user tries to submit more jobs in a certain walltime category than the
maximum number of queable jobs, the submission will fail. The scheduler will
also enforce that no more than the maximum of runnable jobs per category are
being executed at the same time.

To get basic information about the queues, the command \strong{qstat -q} is used:

\inputsite{qstat-q-output}

The number of jobs currently running in the queue is shown in the Run column,
whereas the number of jobs waiting to get started is shown in the Queue column.
The maximum walltime that is accepted by the queue is shown in the Walltime
column.

To obtain more detailed information on the queues the following command can be
used:

\begin{prompt}
%\shellcmd{qstat -f -Q <queuename>}%
\end{prompt}

This will list additional restrictions such as the maximum walltime of the jobs
and the maximum number of jobs that a user can have in that queue.
\fi

\subsection{Node-specific properties}

The following table contains some node-specific properties that can be used to
make sure the job will run on nodes with a specific CPU or interconnect. Note
that these properties may vary over the different VSC sites.

%TODO insert specs for other sites
\ifantwerpen
\begin{tabular}{|p{0.7in}|p{5.3in}|} \hline
\strong{Property} & \strong{Explanation}                                                        \\ \hline
harpertown        & only use Intel processors from the Harpertown family (54xx, turing-only)    \\ \hline
westmere          & only use Intel processors from the Westmere family (56xx, turing-only)      \\ \hline
ivybridge         & only use Intel processors from the Ivy Bridge family (26xx-v2, hopper-only) \\ \hline
ib                & use Infiniband interconnect                                                 \\ \hline
\end{tabular}
\fi
\ifbrussel
\begin{tabular}{|p{0.7in}|p{5.3in}|} \hline
\strong{Property} & \strong{Explanation}                                        \\ \hline
shanghai          & only use AMD Shanghai processors (AMD 2378) \\ \hline
magnycours        & only use AMD Magnucours processors (AMD 6134) \\ \hline
interlagos        & only use AMD Interlagos processors (AMD 6272) \\ \hline
barcelona         & only use AMD Shanghai and Magnycours processors \\ \hline
amd               & only use AMD processors \\ \hline
ivybridge         & only use Intel Ivy Bridge processors (E5-2680-v2) \\ \hline
intel             & only use Intel processors \\ \hline
gpgpu             & only use nodes with General Purpose GPUs (GPGPUs) \\ \hline
k20x              & only use nodes with NVIDIA Tesla K20x GPGPUs \\ \hline
xeonphi           & only use nodes with Xeon Phi co-processors \\ \hline
phi5110p          & only use nodes with Xeon Phi 5110P co-processors \\ \hline
\end{tabular}
\fi

To get a list of all properties defined for all nodes, enter
\begin{prompt}
%\shellcmd{pbsnodes}%
\end{prompt}

This list will also contain properties referring to, e.g., network components,
rack number, etc.

\section{Job output and error files}

At some point your job finishes, so you may no longer see the job ID in the
list of jobs when you run \emph{qstat} (since it will only be listed for a few
minutes after completion with state ``C''). After your job finishes, you should
see the standard output and error of your job in two files, located by default
in the directory where you issued the \emph{qsub} command.


When you navigate to that directory and list its contents, you should see them:

\begin{prompt}
%\shellcmd{ls -l}%
total 1024
-rw-r--r-- 1 %\userid%  609 Sep 11 10:54 fibo.pl
-rw-r--r-- 1 %\userid%   68 Sep 11 10:53 fibo.pbs
-rw------- 1 %\userid%   52 Sep 11 11:03 fibo.pbs.e%\jobnumber%
-rw------- 1 %\userid% 1307 Sep 11 11:03 fibo.pbs.o%\jobnumber%
\end{prompt}

In our case, our job has created both output (`fibo.pbs.\strong{o}\jobnumber') and
error files (`fibo.pbs.\strong{e}\jobnumber') containing info written to
\emph{stdout} and \emph{stderr} respectively.

Inspect the generated output and error files:

\begin{prompt}
%\shellcmd{cat fibo.pbs.o\jobnumber}%
%\dots%
%\shellcmd{cat fibo.pbs.e\jobnumber}%
%\dots%
\end{prompt}

\section{E-mail notifications}

\subsection{Upon job failure}

Whenever a job fails, an e-mail will be sent to the e-mail address that's
connected to your $<$vsc-account$>$. This is the e-mail address that is linked
to the university account, which was used during the registration process.

You can force a job to fail by specifying an unrealistic wall-time for the
previous example.  Lets give the ``\emph{fibo.pbs}'' job just one second to
complete:

\begin{prompt}
%\shellcmd{qsub -l walltime=0:00:01 fibo.pbs}%
\end{prompt}

Now, lets hope that the \hpc did not manage to run the job within one second,
and you will get an e-mail informing you about this error.

\begin{flattext}
PBS Job Id: %\jobid%
Job Name:   fibo.pbs
Exec host:  %\computenode%/0
Aborted by PBS Server
Job exceeded some resource limit (walltime, mem, etc.). Job was aborted.
See Administrator for help
\end{flattext}

\subsection{Generate your own e-mail notifications}

You can instruct the \hpc to send an e-mail to your e-mail address whenever a
job \strong{b}egins, \strong{e}nds and/or \strong{a}borts, by adding the
following lines to the job-script ``fibo.pbs'':

\begin{code}{bash}
#PBS -m b
#PBS -m e
#PBS -m a
#PBS -M <your e-mail address>
\end{code}
or
\begin{code}{bash}
#PBS -m abe
#PBS -M <your e-mail address>
\end{code}

These options can also be specified on the command line.  Try it and see what
happens:

\begin{prompt}
%\shellcmd{qsub -m abe -M $<$your e-mail address$>$ fibo.pbs}%
\end{prompt}

You don't have to specify the e-mail address. The system will
use the e-mail address which is connected to your VSC account.

