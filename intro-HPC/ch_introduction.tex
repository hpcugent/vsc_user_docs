\chapter{Introduction to HPC}
\label{ch:introduction-to-hpc}

\section{What is HPC?}
\label{sec:what-is-hpc}

``\strong{High Performance Computing}'' (HPC) is computing on a
``\emph{Supercomputer}'', a computer with at the frontline of contemporary
processing capacity -- particularly speed of calculation and available memory.

While the supercomputers in the early days (around 1970) used only a few
processors, in the 1990s machines with thousands of processors began to appear
and, by the end of the 20th century, massively parallel supercomputers with
tens of thousands of ``off-the-shelf'' processors were the norm. A large number
of dedicated processors are placed in close proximity to each other in a
computer cluster.

A \strong{computer cluster} consists of a set of loosely or tightly connected
computers that work together so that in many respects they can be viewed as a
single system.

The components of a cluster are usually connected to each other through fast
local area networks (``LAN'') with each \emph{node} (computer used as a
server) running its own instance of an operating system. Computer clusters
emerged as a result of convergence of a number of computing trends including
the availability of low cost microprocessors, high-speed networks, and software
for high performance distributed computing.

Compute clusters are usually deployed to improve performance and availability
over that of a single computer, while typically being more cost-effective
than single computers of comparable speed or availability.

Supercomputers play an important role in the field of computational science,
and are used for a wide range of computationally intensive tasks in various
fields, including quantum mechanics, weather forecasting, climate research, oil
and gas exploration, molecular modelling (computing the structures and
properties of chemical compounds, biological macromolecules, polymers, and
crystals), and physical simulations (such as simulations of the early moments
of the universe, airplane and spacecraft aerodynamics, the detonation of
nuclear weapons, and nuclear fusion).%
\footnote{Wikipedia: \url{http://en.wikipedia.org/wiki/Supercomputer}}

\section{What is the \hpcInfra?}
\label{sec:what-is-the-hpc}

The \hpc is a collection of computers with
\ifbrussel
AMD and
\fi
Intel CPUs, running a Linux
operating system, shaped like pizza boxes and stored above and next
to each other in racks, interconnected with copper and fiber cables. Their
number crunching power is (presently) measured in hundreds of billions of
floating point operations (gigaflops) and even in teraflops.

\begin{center}
\includegraphics*[width=1.88in, height=1.26in, keepaspectratio=false]{ch1-cables1}
\includegraphics*[width=1.88in, height=1.26in, keepaspectratio=false]{ch1-cables2}
\includegraphics*[width=1.88in, height=1.26in, keepaspectratio=false]{ch1-cables3}
\end{center}

The \hpcInfra relies on parallel-processing technology to offer \university researchers an
extremely fast solution for all their data processing needs.


\ifantwerpen
The \hpc consists of:
\begin{center}
\begin{tabular}{|p{1.8in}|p{2.1in}|} \hline
\strong{In technical terms}         & \strong{\dots\  in human terms}                    \\ \hline
323 nodes and 7700 cores            & \dots\  or the equivalent of 1925 quad-core PCs    \\ \hline
100 Terabyte of online storage     & \dots\  or the equivalent of over 20000 DVDs            \\ \hline
up to 100 Gbit InfiniBand fiber connections & \dots\  or allowing to transfer 3 DVDs per second \\ \hline
\end{tabular}
\end{center}
\fi
%TODO insert specs for other sites

The \hpc currently consists of:

\ifantwerpen
Leibniz:
  \begin{enumerate}
  \item 144 compute nodes with 2 14-core Intel E5-2680v4 CPUs (Broadwell generation, 2.4 GHz) and 128 GB RAM, 120 GB local disk
  \item 8 compute nodes with 2 14-core Intel E5-2680v4 CPUs (Broadwell generation, 2.4 GHz) and 256 GB RAM, 120 GB local disk
  \item 24 ``hopper'' compute nodes (recovered from the former Hopper cluster) with 2 ten core Intel E5-2680v2 CPUs (Ivy Bridge generation, 2.8 GHz), 256 GB memory, 500 GB local disk
  \item 2 GPGPU nodes with 2 14-core Intel E5-2680v4 CPUs (Broadwell generation, 2.4 GHz), 128 GB RAM and two NVIDIA Tesla P100 GPUs with 16 GB HBM2 memory per GPU, 120 GB local disk
  \item 1 vector computing node with 1 12-core Intel Xeon Gold 6126 (Skylake generation, 2.6 GHz), 96 GB RAM and 2 NEC SX-Aurora Vector Engines type 10B (per card 8 cores @1.4 GHz, 48 GB HBM2), 240 GB local disk
  \item 1 Xeon Phi node with 2 14-core Intel E5-2680v4 CPUs (Broadwell generation, 2.4 GHz), 128 GB RAM and Intel Xeon Phi 7220P PCIe card with 16 GB of RAM, 120 GB local disk
  \item 1 visualisation node with 2 14-core Intel E5-2680v4 CPUs (Broadwell generation, 2.4 GHz), 256 GB RAM and with a NVIDIA P5000 GPU, 120 GB local disk
  \end{enumerate}
The nodes are connected using an InfiniBand EDR network except for the ``hopper'' compute nodes that utilize FDR10 InfiniBand. 

Vaughan:
  \begin{enumerate}
  \item 104 compute nodes with 2 32-core AMD Epyc 7452 (2.35 GHz) and 256 GB RAM, 240 GB local disk
  \end{enumerate}
The nodes are connected using an InfiniBand HDR100 network.

\fi
\ifleuven
  \begin{enumerate}
     \item ThinKing (thin node cluster)
      \begin{itemize}
       \item  176 compute nodes with 2 10-core Ivy Bridge processors, 64 GB memory,
              300 GB local disk, QDR-IB network
       \item  32 compute nodes with 2 10-core Ivy Bridge processors, 128 GB memory,
              300 GB local disk, QDR-IB network
       \item  48 compute nodes with 2 12-core Ivy Bridge processors, 64 GB memory,
              300 GB local disk, QDR-IB network
      \end{itemize}
    \item Accelerator partition
      \begin{itemize}
       \item  8 nodes with 2 8-core Sandy Bridge processors, 64 GB memory,
              300 GB local disk, DDR-IB network, 2 K20 NVIDIA GPGPUs
       \item  8 nodes with 2 8-core Sandy Bridge processors, 64 GB memory,
              300 GB local disk, DDR-IB network, 2 Intel Xeon Phis
       \item  4 nodes with 2 6-core Xeon 5650 Westmere processors, 24 GB memory,
              300 GB local disk, DDR-IB network, 2 Tesla M2070 GPGPUs
       \item  5 nodes with 2 10-core Haswell Xeon E5-2650v3 processors, 64 GB memory,
              300 GB local disk, DDR-IB network, 2 Tesla K40 GPGPUs
      \end{itemize}
    \item Visualization partition
      \begin{itemize}
       \item  2 nodes with 2 10-core Haswell Xeon E5-2650v3 processors, 128 GB memory,
              300 GB local disk, DDR-IB network, 2 NVIDIA Quadro K5200 GPUs (visualization nodes)
      \end{itemize}
    \item Cerebro (shared memory SMP section)
      \begin{itemize}
        \item  64 sockets with 10-core Xeon E5-4650 processors, total 14 TB memory,
               SGI-proprietary NUMAlink6 interconnect,
               1 partition has 480 cores and 12 TB RAM and another partition has 160 cores and 2 TB RAM,
               both partitions have 10TB local scratch space
      \end{itemize}
  \end{enumerate}
\fi
\ifbrussel
  a mix of nodes with AMD and Intel CPUs and different interconnects in different sections of the cluster. The cluster also contains a number of nodes with NVIDIA GPGPUs.  For an up to date list of all clusters and their hardware, see\\*
  \url{https://vscdocumentation.readthedocs.io/en/latest/brussels/tier2_hardware.html}.
\fi
\ifgent
  a set of different compute clusters. For an up to date list of all clusters and their hardware, see\\*
  \url{https://vscdocumentation.readthedocs.io/en/latest/gent/tier2_hardware.html}.
\fi
%TODO other sites: keep up to date, or link to vsc website?

\ifantwerpen
All the nodes in the \hpc run under the ``\operatingsystem'' operating
system, which is a clone of ``\operatingsystembase'',
with \emph{cgroups} support.
\fi
\ifleuven
All the nodes in the \hpc run under the ``\operatingsystem'' operating system.
\fi
\ifbrussel
All the nodes in the \hpc run under the ``\operatingsystem'' operating
system, which is a clone of ``\operatingsystembase'',
with \emph{cpuset} support.
\fi
\ifgent
All the nodes in the \hpc run ``\operatingsystem''
with \emph{cpuset} support and \emph{BLCR} modules.
\fi
%TODO insert specs for other sites

Two tools perform the \strong{job management} and \strong{job scheduling}:
\begin{enumerate}
  %TODO: this gives weird rendering artifacts??
  \item  TORQUE: a resource manager (based on PBS);
  \item  Moab: job scheduler and management tools.
\end{enumerate}
\ifantwerpen
%\strong{Accounting} is handled by a third tool, i.e., GOLD.
\fi
\ifleuven
\strong{Accounting} is handled by a third tool, i.e., MAM (Moab Accounting
Manager).
\fi
%TODO check whether others use accounting

\ifantwerpen
    For maintenance and monitoring, we use:
    \begin{enumerate}
      \item  Ganglia: monitoring software;
      \item  Icinga and Nagios: alert manager.
    \end{enumerate}
\fi
\ifleuven
    For maintenance and monitoring, we use:
    \begin{enumerate}
      \item  Ganglia: monitoring software;
      \item  HP Insight Cluster Management Utility.
    \end{enumerate}
\fi
\ifbrussel
    For maintenance and monitoring, we use:
    \begin{enumerate}
      \item  Bright Cluster Manager;
      \item  Zabbix: monitoring system.
    \end{enumerate}
\fi
%TODO check tools for other sites


\section{What the HPC infrastucture is \emph{not}}
\label{sec:what-is-the-hpc-not}

The HPC infrastructure is \emph{not} a magic computer that automatically:
\begin{enumerate}
  \item  runs your PC-applications much faster for bigger problems;
  \item  develops your applications;
  \item  solves your bugs;
  \item  does your thinking;
  \item  \dots
  \item  allows you to play games even faster.
\end{enumerate}
The \hpc does not replace your desktop computer.

\section{Is the \hpc a solution for my computational needs?}
\label{sec:is-the-hpc-a-solution-for-my-computational-needs}

\subsection{Batch or interactive mode?}
\label{sec:batch-or-interactive-mode}

Typically, the strength of a supercomputer comes from its ability to run a huge
number of programs (i.e., executables) in parallel without any user interaction
in real time. This is what is called ``running in batch mode''.

It is also possible to run programs at the \hpc, which require user
interaction. (pushing buttons, entering input data, etc.).  Although
technically possible, the use of the \hpc might not always be the best and
smartest option to run those interactive programs.  Each time some user
interaction is needed, the computer will wait for user input. The available
computer resources (CPU, storage, network, etc.) might not be optimally used in
those cases. A more in-depth analysis with the \hpc staff can unveil whether
the \hpc is the desired solution to run interactive programs.
Interactive mode is typically only useful for creating quick visualisations
of your data without having to copy your data to your desktop and back.
%TODO: only visualization nodes should be used interactively

\subsection{What are cores, processors and nodes?}

In this manual, the terms core, processor and node will be frequently used,
so it's useful to understand what they are.

Modern servers, also referred to as \emph{(worker)nodes} in the context of HPC,
include one or more sockets, each housing a multi-\emph{core} \emph{processor} (next to
memory, disk(s), network cards, \ldots). A modern \emph{processor} consists of
multiple CPUs or \emph{cores} that are used to execute \emph{computations}.

\subsection{Parallel or sequential programs?}
\label{sec:parallel-or-sequential-programs}

\subsubsection{Parallel programs}


\strong{Parallel computing} is a form of computation in which many calculations
are carried out simultaneously. They are based on the principle that large
problems can often be divided into smaller ones, which are then solved
concurrently (``in parallel'').

Parallel computers can be roughly classified according to the level at which
the hardware supports parallelism, with multicore computers having multiple
processing elements within a single machine, while clusters use multiple
computers to work on the same task. Parallel computing has become the dominant
computer architecture, mainly in the form of multicore processors.

The two parallel programming paradigms most used in HPC are:

\begin{itemize}
    \item OpenMP for shared memory systems (multithreading): on multiple cores of a single node
    \item MPI for distributed memory systems (multiprocessing): on multiple nodes
\end{itemize}

\strong{Parallel programs} are more difficult to write than sequential ones,
because concurrency introduces several new classes of potential software bugs,
of which race conditions are the most common. Communication and synchronisation
between the different subtasks are typically some of the greatest obstacles to
getting good parallel program performance.

\subsubsection{Sequential programs}

Sequential software does not do calculations in parallel, i.e., it only uses
\emph{one single core of a single workernode}. \strong{It does not become
faster by just throwing more cores at it}: it can only use one core.

It is perfectly possible to also run purely \strong{sequential programs} on the
\hpc.

Running your sequential programs on the most modern and fastest computers in
the \hpc can save you a lot of time.  But it also might be possible to run
multiple instances of your program (e.g., with different input parameters) on
the \hpc, in order to solve one overall problem (e.g., to perform a parameter
sweep). This is another form of running your sequential programs in parallel.

\subsection{What programming languages can I use?}
\label{sec:what-programming-languages-can-i-use}

You can use \emph{any} programming language, \emph{any} software package and
\emph{any} library provided it has a version that runs on Linux, specifically,
on the version of Linux that is installed on the compute nodes,
\operatingsystem.

For the most common \strong{programming languages}, a compiler is available on
\operatingsystem. Supported and common programming languages on the \hpc are
C/C++, FORTRAN, Java, Perl, Python, MATLAB, R, etc.

Supported and commonly used compilers are
%TODO insert compilers for other sites
\ifantwerpen
GCC, clang, J2EE and Intel Cluster Studio.
\fi
\ifleuven
GCC, Intel and PGI.
\fi
\ifbrussel
GCC, clang, J2EE and Intel Cluster Studio.
\fi
\ifgent
GCC and Intel.
\fi

%TODO insert software for other sites

\ifantwerpen
Commonly used software packages are:
\begin{itemize}
\item{in bioinformatics: beagle, Beast, bowtie, MrBayes, SAMtools}
\item{in chemistry: ABINIT, CP2K, Gaussian, Gromacs, LAMMPS, NWChem, Quantum Espresso, Siesta, VASP}
\item{in engineering: COMSOL, OpenFOAM, Telemac}
\item{in mathematics: JAGS, MATLAB, R}
\item{for visuzalization: Gnuplot, ParaView.}
\end{itemize}

Commonly used libraries are Intel MKL, FFTW, HDF5, PETSc and Intel MPI,
OpenMPI.
\fi
\ifleuven
Commonly used software packages are:
\begin{itemize}
\item{in bioinformatics: beagle, Beast, bedtools, bowtie, BWA, Mr. Bayes, TopHat, TRIQS,}
\item{in chemistry: CP2K, Gaussian, GROMACS, Molpro, NAMD, NWChem, Siesta, Turbomole, VASP, VMD,}
\item{in engineering: Abaqus, Ansys, Comsol, OpenFOAM,}
\item{in mathematics: JAGS, MATLAB, R, SAS,}
\item{for visuzalization: Gnuplot, IDL, Paraview, Tecplot, VisIt.}
%\ldots
\end{itemize}


Commonly used libraries are: Intel MKL, FFTW, HDF5, PETSc, Intel MPI,
M(VA)PICH, OpenMPI, Qt, VTK and Mesa.
\fi
\ifbrussel
Commonly used software packages are CP2K, Gaussian, MATLAB, NWChem, R, \ldots

Commonly used Libraries are Intel MKL, FFTW, HDF5, netCDF, PETSc and Intel MPI,
OpenMPI.
\fi

Additional software can be installed ``\emph{on demand}''. Please contact the
\hpc staff to see whether the \hpc can handle your specific requirements.

\subsection{What operating systems can I use?}
\label{sec:what-operating-systems-can-i-use}

All nodes in the \hpc cluster run under \operatingsystem, which is a specific
version of \operatingsystembase. This means that all programs (executables)
should be compiled for \operatingsystem.

Users can connect from any computer in the \university network to the
\hpc, regardless of the Operating System that they are using on their personal
computer.
Users can use any of the common Operating Systems (such as Windows, macOS or
any version of Linux/Unix/BSD) and run and control their programs on the \hpc.

A user does not need to have prior knowledge about Linux; all of the required
knowledge is explained in this tutorial.

\subsection{What does a typical workflow look like?}

A typical workflow looks like:

\begin{enumerate}
    \item Connect to the login nodes with SSH (see \autoref{sec:first-time-connection-to-the-hpc})
    \item Transfer your files to the cluster (see \autoref{sec:filetransfer})
    \item Optional: compile your code and test it (for compiling, see \autoref{ch:compiling-and-testing-your-software-on-the-hpc})
    \item Create a job script and submit your job (see \autoref{ch:running-batch-jobs})
    \item Get some coffee and be patient:
    \begin{enumerate}
        \item Your job gets into the queue
        \item Your job gets executed
        \item Your job finishes
    \end{enumerate}
    \item Study the results generated by your jobs, either on the cluster or after downloading them locally.
\end{enumerate}


\subsection{What is the next step?}
\label{sec:what-is-the-next-step}

When you think that the \hpc is a useful tool to support your computational
needs, we encourage you to acquire a VSC-account (as explained in
\autoref{ch:getting-a-hpc-account}), read
\autoref{ch:connecting}, ``Setting up the environment'', and
explore chapters~\ref{ch:running-interactive-jobs}
to~\ref{ch:fine-tuning-job-specifications} which will help you to transfer and
run your programs on the \hpc cluster.

Do not hesitate to contact the \hpc staff for any help.
