\chapter{Introduction to HPC}
\label{ch:introduction-to-hpc}

\section{What is HPC?}
\label{sec:what-is-hpc}

``\strong{High Performance Computing}'' (HPC) is computing on a
``\emph{Supercomputer}'', a computer with at the frontline of contemporary
processing capacity -- particularly speed of calculation and available memory.

While the supercomputers in the early days (around 1970) used only a few
processors, in the 1990s machines with thousands of processors began to appear
and, by the end of the 20th century, massively parallel supercomputers with
tens of thousands of ``off-the-shelf'' processors were the norm. A large number
of dedicated processors are placed in close proximity to each other in a
computer cluster.

A \strong{computer cluster} consists of a set of loosely or tightly connected
computers that work together so that in many respects they can be viewed as a
single system.

The components of a cluster are usually connected to each other through fast
local area networks (``LAN'') with each \emph{node} (computer used as a
server) running its own instance of an operating system. Computer clusters
emerged as a result of convergence of a number of computing trends including
the availability of low cost microprocessors, high-speed networks, and software
for high performance distributed computing.

Compute clusters are usually deployed to improve performance and availability
over that of a single computer, while typically being more cost-effective
than single computers of comparable speed or availability.

Supercomputers play an important role in the field of computational science,
and are used for a wide range of computationally intensive tasks in various
fields, including quantum mechanics, weather forecasting, climate research, oil
and gas exploration, molecular modelling (computing the structures and
properties of chemical compounds, biological macromolecules, polymers, and
crystals), and physical simulations (such as simulations of the early moments
of the universe, airplane and spacecraft aerodynamics, the detonation of
nuclear weapons, and nuclear fusion).
\footnote{Wikipedia: \url{http://en.wikipedia.org/wiki/Supercomputer}}

\section{What is the \hpcInfra?}
\label{sec:what-is-the-hpc}

The \hpc is a collection computers with
\ifbrussel
AMD and
\fi
\ifgent
AMD and
\fi
Intel processors, running a Linux
operating system, shaped like pizza boxes and stored above and next
to each other in racks, interconnected with copper and fiber cables. Their
number crunching power is (presently) measured in hundreds of billions of
floating point operations (gigaflops) and even in teraflops.

\begin{center}
\includegraphics*[width=1.88in, height=1.26in, keepaspectratio=false]{ch1-cables1}
\includegraphics*[width=1.88in, height=1.26in, keepaspectratio=false]{ch1-cables2}
\includegraphics*[width=1.88in, height=1.26in, keepaspectratio=false]{ch1-cables3}
\end{center}

The \hpcInfra relies on parallel-processing technology to offer \university researchers an
extremely fast solution for all their data processing needs.


\ifantwerpen
The \hpc consists of:
\begin{center}
\begin{tabular}{|p{1.8in}|p{2.1in}|} \hline
\strong{In technical terms}         & \strong{\dots\  in human terms}                    \\ \hline
336 nodes and 4992 cores            & \dots\  or the equivalent of 1248 quad-core PCs    \\ \hline
100 Terabyte of online storage     & \dots\  or the equivalent of over 20000 DVDs            \\ \hline
up to 40 Gbit Infiniband fiber connections & \dots\  or allowing to transfer 8 DVDs per second \\ \hline
\end{tabular}
\end{center}
\fi
%TODO insert specs for other sites

The \hpc currently consists of:

\ifantwerpen
Turing:
  \begin{enumerate}
    \item  64 compute nodes with 2 quad core Harpertown processors, 16 GB
    memory, 120 GB local disk, GbE network
    \item  32 nodes with 2 quad core Harpertown processors, 16 GB memory,
           120 GB local disk, DDR-IB network
    \item  64 nodes with 2 six core Westmere processors, 24 GB memory,
           120 GB local disk, QDR-IB network
    \item  8 nodes with 2 six core Westmere processors, 24 GB memory,
           120 GB local disk, GbE network
  \end{enumerate}
and Hopper:
  \begin{enumerate}
    \item  144 compute nodes with 2 ten core Ivy Bridge processors, 64 GB
    memory, 500 GB local disk, FDR10 network
    \item  24 nodes with 2 ten core Ivy Bridge processors, 256 GB memory,
           500 GB local disk, FDR10 network
  \end{enumerate}
\fi
\ifleuven
  \begin{enumerate}
     \item ThinKing (thin node cluster)
      \begin{itemize}
       \item  176 compute nodes with 2 10-core Ivy Bridge processors, 64 GB memory,
              300 GB local disk, QDR-IB network
       \item  32 compute nodes with 2 10-core Ivy Bridge processors, 128 GB memory,
              300 GB local disk, QDR-IB network
       \item  48 compute nodes with 2 12-core Ivy Bridge processors, 64 GB memory,
              300 GB local disk, QDR-IB network
      \end{itemize}
    \item Accelerator partition
      \begin{itemize}
       \item  8 nodes with 2 8-core Sandy Bridge processors, 64 GB memory,
              300 GB local disk, DDR-IB network, 2 K20 NVIDIA GPGPUs
       \item  8 nodes with 2 8-core Sandy Bridge processors, 64 GB memory,
              300 GB local disk, DDR-IB network, 2 Intel Xeon Phis
       \item  4 nodes with 2 6-core Xeon 5650 Westmere processors, 24 GB memory, 
              300 GB local disk, DDR-IB network, 2 Tesla M2070 GPGPUs
       \item  5 nodes with 2 10-core Haswell Xeon E5-2650v3 processors, 64 GB memory, 
              300 GB local disk, DDR-IB network, 2 Tesla K40 GPGPUs
      \end{itemize}
    \item Visualization partition
      \begin{itemize}
       \item  2 nodes with 2 10-core Haswell Xeon E5-2650v3 processors, 128 GB memory, 
              300 GB local disk, DDR-IB network, 2 NVIDIA Quadro K5200 GPUs (visualization nodes)    
      \end{itemize}
    \item Cerebro (shared memory SMP section)
      \begin{itemize}
        \item  64 sockets with 10-core Xeon E5-4650 processors, total 14 TB memory, 
               SGI-proprietary NUMAlink6 interconnect, 
               1 partition has 480 cores and 12 TB RAM and another partition has 160 cores and 2 TB RAM, 
               both partitions have 10TB local scratch space
      \end{itemize}
  \end{enumerate}
\fi
\ifbrussel
  \begin{enumerate}
    \item  48 compute nodes with 2 4-core AMD 2378 (Shanghai) processors, 32 GB
    memory, 72 GB local disk, SDR-IB network
    \item  64 compute nodes with 2 8-core AMD 6134 (Magnycours) processors, 64
    GB memory, 1 TB local disk, QDR-IB network
    \item  9 compute nodes with 2 16-core AMD 6274 (Interlagos) processors, 256
    GB memory, 900 GB local disk, QDR-IB network
    \item  29 nodes with 2 10-core INTEL E5-2680v2 (IvyBridge) processors, 64
    GB memory, 900 GB local disk, QDR-IB network
    \item  6 nodes with 2 10-core INTEL E5-2680v2 (IvyBridge) processors, 64 GB
    memory, 900 GB local disk, QDR-IB network, 2 Tesla K20x NVIDIA GPGPUs with
    6Gb memory
    \item  2 nodes with 2 10-core INTEL E5-2680v2 (IvyBridge) processors, 64 GB
    memory, 900 GB local disk, QDR-IB network, 2 Intel Xeon Phi 5110P
    co-processors with Gb memory
  \end{enumerate}
\fi
\ifgent
  A set off different clusters. For an up to date list of all clusters and their hardware, see \url{https://www.vscentrum.be/infrastructure/hardware/hardware-ugent}.
\fi
%TODO other sites: keep up to date, or link to vsc website?

\ifantwerpen
All the nodes in the \hpc run under the ``\operatingsystemSL'' operating
system, which is a clone of ``\operatingsystemRHEL'',
with \emph{cpuset} support and \emph{BLCR} modules.
\fi
\ifleuven
All the nodes in the \hpc run under the ``\operatingsystem'' operating system.
\fi
\ifbrussel
All the nodes in the \hpc run under the ``\operatingsystemSL'' operating
system, which is a clone of ``\operatingsystemRHEL'',
with \emph{cpuset} support.
\fi
\ifgent
All the nodes in the \hpc run ``\operatingsystem''
with \emph{cpuset} support and \emph{BLCR} modules.
\fi
%TODO insert specs for other sites

Two tools perform the \strong{job management} and \strong{job scheduling}:
\begin{enumerate}
  %TODO: this gives weird rendering artifacts??
  \item  TORQUE: a resource manager (based on PBS);
  \item  Moab: job scheduler and management tools.
\end{enumerate}
\ifantwerpen
%\strong{Accounting} is handled by a third tool, i.e., GOLD.
\fi
\ifleuven
\strong{Accounting} is handled by a third tool, i.e., MAM (Moab Accounting
Manager).
\fi
%TODO check whether others use accounting

\ifantwerpen
    For maintenance and monitoring, we use:
    \begin{enumerate}
      \item  Ganglia: monitoring software;
      \item  Icinga: alert manager.
    \end{enumerate}
\fi
\ifleuven
    For maintenance and monitoring, we use:
    \begin{enumerate}
      \item  Ganglia: monitoring software;
      \item  HP Insight Cluster Management Utility.
    \end{enumerate}
\fi
\ifbrussel
    For maintenance and monitoring, we use:
    \begin{enumerate}
      \item  Bright Cluster Manager;
      \item  Zabbix: monitoring system.
    \end{enumerate}
\fi
%TODO check tools for other sites


\section{What is the \hpc not!}
\label{sec:what-is-the-hpc-not}

A computer that automatically:
\begin{enumerate}
  \item  runs your PC-applications much faster for bigger problems;
  \item  develops your applications;
  \item  solves your bugs;
  \item  does your thinking;
  \item  \dots
  \item  allows you to play games even faster.
\end{enumerate}
The \hpc does not replace your desktop computer.

\section{Is the \hpc a solution for my computational needs?}
\label{sec:is-the-hpc-a-solution-for-my-computational-needs}

\subsection{Batch or interactive mode?}
\label{sec:batch-or-interactive-mode}

Typically, the strength of a supercomputer comes from its ability to run a huge
number of programs (i.e., executables) in parallel without any user interaction
in real time. This is what is called ``running in batch mode''.

It is also possible to run programs at the \hpc, which require user
interaction. (pushing buttons, entering input data, etc.).  Although
technically possible, the use of the \hpc might not always be the best and
smartest option to run those interactive programs.  Each time some user
interaction is needed, the computer will wait for user input. The available
computer resources (CPU, storage, network, etc.) might not be optimally used in
those cases. A more in-depth analysis with the \hpc staff can unveil whether
the \hpc is the desired solution to run interactive programs.
Interactive mode is typically only useful for creating quick visualisations
of your data without having to copy your data to your desktop and back.
%TODO: only visualization nodes should be used interactively

\subsection{Parallel or sequential programs?}
\label{sec:parallel-or-sequential-programs}

\strong{Parallel computing} is a form of computation in which many calculations
are carried out simultaneously. They are based on the principle that large
problems can often be divided into smaller ones, which are then solved
concurrently (``in parallel'').

Parallel computers can be roughly classified according to the level at which
the hardware supports parallelism, with multicore computers having multiple
processing elements within a single machine, while clusters use multiple
computers to work on the same task. Parallel computing has become the dominant
computer architecture, mainly in the form of multicore processors.

\strong{Parallel programs} are more difficult to write than sequential ones,
because concurrency introduces several new classes of potential software bugs,
of which race conditions are the most common. Communication and synchronisation
between the different subtasks are typically some of the greatest obstacles to
getting good parallel program performance.

It is perfectly possible to also run purely \strong{sequential programs} on the
\hpc.

Running your sequential programs on the most modern and fastest computers in
the \hpc can save you a lot of time.  But it also might be possible to run
multiple instances of your program (e.g., with different input parameters) on
the \hpc, in order to solve one overall problem (e.g., to perform a parameter
sweep). This is another form of running your sequential programs in parallel.

\subsection{What programming languages can I use?}
\label{sec:what-programming-languages-can-i-use}

You can use \emph{any} programming language, \emph{any} software package and
\emph{any} library provided it has a version that runs on Linux, specifically,
on the version of Linux that is installed on the compute nodes,
\operatingsystembase.

For the most common \strong{programming languages}, a compiler is available on
\operatingsystem. Supported and common programming languages on the \hpc are
C/C++, FORTRAN, Java, Perl, Python, MATLAB, R, etc.

Supported and commonly used compilers are 
%TODO insert compilers for other sites
\ifantwerpen
GCC, clang, J2EE and Intel Cluster Studio.
\fi
\ifleuven
GCC, Intel and PGI. 
\fi
\ifbrussel
GCC, clang, J2EE and Intel Cluster Studio.
\fi
\ifgent
GCC and Intel.
\fi

%TODO insert software for other sites

\ifantwerpen
Commonly used software packages are ABINIT, CP2K, Gaussian, Gromacs, Molpro,
NWChem, Quantum Espresso, R, Siesta, TURBOMOLE, WIEN2k, \ldots

Commonly used Libraries are Intel MKL, FFTW, HDF5, PETSc and Intel MPI,
M(VA)PICH, OpenMPI.
\fi
\ifleuven
Commonly used software packages are:
\begin{itemize}
\item{in bioinformatics: beagle, Beast, bedtools, bowtie, BWA, Mr. Bayes, TopHat, TRIQS,}
\item{in chemistry: CP2K, Gaussian, GROMACS, Molpro, NAMD, NWChem, Siesta, Turbomole, VASP, VMD,}
\item{in engineering: Abaqus, Ansys, Comsol, OpenFOAM,}
\item{in mathematics: JAGS, Matlab, R, SAS,}
\item{for visuzalization: Gnuplot, IDL, Paraview, Tecplot, VisIt.}
%\ldots
\end{itemize}


Commonly used Libraries are: Intel MKL, FFTW, HDF5, PETSc, Intel MPI,
M(VA)PICH, OpenMPI, Qt, VTK and Mesa.
\fi
\ifbrussel
Commonly used software packages are CP2K, Gaussian, MATLAB, NWChem, R, \ldots

Commonly used Libraries are Intel MKL, FFTW, HDF5, netCDF, PETSc and Intel MPI,
OpenMPI.
\fi

Additional software can be installed ``\emph{on demand}''. Please contact the
\hpc staff to see whether the \hpc can handle your specific requirements.

\subsection{What operating systems can I use?}
\label{sec:what-operating-systems-can-i-use}

All nodes in the \hpc cluster run under \operatingsystem, which is a specific
version of \operatingsystembase. This means that all programs (executables)
should be compiled for \operatingsystem.

Users can connect from any computer in the \university network to the
\hpc, regardless of the Operating System that they are using on their personal
computer.
Users can use any of the common Operating Systems (such as Windows, OS X or
any version of Linux/Unix/BSD) and run and control their programs on the \hpc.

A user does not need to have prior knowledge about Linux; all of the required
knowledge is explained in this tutorial.

\subsection{What is the next step?}
\label{sec:what-is-the-next-step}

When you think that the \hpc is a useful tool to support your computational
needs, we encourage you to acquire a VSC-account (as explained in
\autoref{ch:getting-a-hpc-account}), read
\autoref{ch:preparing-the-environment}, ``Setting up the environment'', and
explore chapters~\ref{ch:running-interactive-jobs}
to~\ref{ch:fine-tuning-job-specifications} which will help you to transfer and
run your programs on the \hpc cluster.

Do not hesitate to contact the \hpc staff for any help.
