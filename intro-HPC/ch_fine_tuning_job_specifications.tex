\chapter{Fine-tuning Job Specifications}

As \hpc systemadministrators, we often observe that the \hpc resources are
not optimally (or wisely) used. For example, we regularly notice that several
cores on a computing node are not utilized, due to the fact that one sequential
program uses only one core on the node. Or users run I/O intensive applications
on nodes with `slow' network connections.

Users often tend to run their jobs without specifying specific PBS Job
parameters.  As such, their job will automatically use the default parameters,
which are not necessarily (or rarely) the optimal ones.  This can slow down the
runtime of your application, but also block \hpc resources for other users.

Specifying the `optimal' Job Parameters requires some knowledge of your
application (e.g., how many parallel threads does my application uses, is there
a lot of inter-process communication, how much memory does my application need)
and also some knowledge about the \hpc infrastructure (e.g., what kind of
multi-core processors are available, which nodes have infiniband).

There are plenty of monitoring tools on Linux available to the user, which are
useful to analyze your individual application. The \hpc environment as a
whole often requires different techniques, metrics and time goals, which are
not discussed here. We will focus on tools that can help to optimize your Job
Specifications.

Determining the optimal computer resource specifications can be broken down
into different parts. The first is actually determining which metrics are
needed and then collecting that data from the hosts. Some of the most commonly
tracked metrics are CPU usage, memory consumption, network bandwidth, and disk
I/O stats. These provide different indications of how well a system is
performing, and may indicate where there are potential problems or performance
bottlenecks. Once the data have actually been acquired, the second task is
analyzing the data and adapting your PBS Job Specifications.

Another different task is to monitor the behavior an application at runtime and
detect anomalies or unexpected behavior. Linux provides a large number of
utilities to monitor the performance of its components.

This chapter shows you how to measure:

\begin{enumerate}
\item  Walltime
\item  Memory usage
\item  CPU usage
\item  Disk (storage) needs
\item  Network bottlenecks
\end{enumerate}

First, we allocate a compute node and move to our relevant directory:
\begin{prompt}
%\shellcmd{qsub -I}%
%\shellcmd{cd \tilde/\exampledir}%
\end{prompt}

\section{Specifying Walltime}

One of the most important and also easiest parameters to measure is the
duration of your program. This information is needed to specify the
\emph{walltime}.

The \emph{time} utility \strong{executes} and \strong{times} your application.
You can just add the time command in front of your normal command line,
including your command line options. After your executable has finished,
\strong{time} writes the total time elapsed, the time consumed by system
overhead, and the time used to execute your executable to the standard error
stream. The calculated times are reported in seconds.


Test the time command:
\begin{prompt}
%\shellcmd{time sleep 75}%
real 1m15.005s
user 0m0.001s
sys 0m0.002s
\end{prompt}

It is a good practice to correctly estimate and specify the runtime (duration)
of an application. Of course, a margin of 10\% to 20\% can be taken to be on
the safe side.

It is also wise to check the walltime on different compute nodes or to select
the ''slowest'' compute node for your walltime tests. Your estimate should
appropriate in case your application will run on the ``slowest'' (oldest)
compute nodes.

The walltime can be specified in a job scripts as:
\begin{prompt}
#PBS -l walltime=3:00:00:00
\end{prompt}

or on the command line
\begin{prompt}
%\shellcmd{qsub -l walltime=3:00:00:00}%
\end{prompt}

It is recommended to always specify the walltime for a job.

\section{Specifying memory requirements}

In many situations, it is useful to monitor the amount of memory an application
is using. You need this information to determine the characteristics of the
required compute node, where that application should run on.  Estimating the
amount of memory an application will use during execution is often non-trivial,
especially when one uses third-party software.

The ``eat\_mem'' application just consumes and then releases memory, for the
purpose of this test. It has one parameter, the amount of Gigabits of memory
which needs to be allocated.

First compile the program on your machine and then test it for 1 Gb:
\begin{prompt}
%\shellcmd{gcc -o eat\_mem eat\_mem.c}%
%\shellcmd{./eat\_mem 1}%
Consuming 1 Gigabit of memory.
\end{prompt}


\subsection{Available Memory on the machine}

The first point is to be aware of the available free memory in your computer.
The ``\emph{free}'' command displays the total amount of free and used
physical and swap memory in the system, as well as the buffers used by the
kernel. We also use the options ``-m'' to see the results expressed in
Mega-Bytes and the ``-t'' option to get totals.

\begin{prompt}
%\shellcmd{free -m -t}%
                total   used   free  shared  buffers  cached
Mem:            16049   4772  11277       0      107     161
-/+ buffers/cache:      4503  11546
Swap:           16002   4185  11816
Total:          32052   8957  23094
\end{prompt}

Important is to note the total amount of memory available in the machine (i.e.,
16Gb in this example) and the amount of used and free memory (i.e. 4,7 Gb is
used and another 11,2 Gb is free here).

It is not a good practice to use Swap-space for your computational
applications. A lot of ``swapping'' can increase the execution time of your
application tremendously.

\subsection{Checking the memory consumption}

The ``Monitor'' tool monitors applications in terms of memory and CPU usage, as
well as the size of temporary files. Note that currently only single node jobs
are supported, MPI support may be added in a future release.

To start using monitor, first load the appropriate module. Then we study the
``eat\_mem.c'' program and compile it:

\begin{prompt}
%\shellcmd{module load monitor}%
%\shellcmd{cat eat\_mem.c}%
%\shellcmd{gcc -o eat\_mem eat\_mem.c}%
\end{prompt}


Starting a program to monitor is very straightforward; you just add the
``monitor'' command before the regular command line.

\begin{prompt}
%\shellcmd{monitor ./eat\_mem 3}%
time (s) size (kb) %\%%mem %\%%cpu
Consuming 3 Gigabit of memory.
5  252900 1.4 0.6
10  498592 2.9 0.3
15  743256 4.4 0.3
20  988948 5.9 0.3
25  1233612 7.4 0.3
30  1479304 8.9 0.2
35  1723968 10.4 0.2
40  1969660 11.9 0.2
45  2214324 13.4 0.2
50  2460016 14.9 0.2
55  2704680 16.4 0.2
60  2950372 17.9 0.2
65  3167280 19.2 0.2
70  3167280 19.2 0.2
75  9264  0 0.5
80  9264  0 0.4
\end{prompt}

Whereby:

\begin{enumerate}
\item  The first column shows you the elapsed time in seconds. By default, all
  values will be displayed every 5 seconds.
\item  The second column shows you the used memory in kb. We note that the
  memory slowly increases up to 3 Gb (=  kb), and is released again.
\item  The third column shows the memory utilization, expressed in percentages
  of the full available memory.  At full memory consumption, 19,2\% of the
  memory was being used by our application. With the \emph{``free''} command,
  we have previously seen that we had a node of 16Gb in this example. 3Gb is
  indeed more or less 19,2\% of the full available memory.
\item  The fourth column shows you the CPU utilization, expressed in
  percentages of a full CPU load. As there are no computations done in our
  exercise, the value remains very low (i.e. 0.2\%).
\end{enumerate}

Monitor will write the CPU usage and memory consumption of simulation to standard error.

This is the rate at which monitor samples the program's metrics. Since
monitor's output may interfere with that of the program to monitor, it is often
convenient to use a log file. The latter can be specified as follows:

\begin{prompt}
%\shellcmd{monitor -l test1.log eat\_mem 2}%
Consuming 2 Gigabit of memory.
%\shellcmd{cat test1.log}%
\end{prompt}

For long running programs, it may be convenient to limit the output to, e.g.,
the last minute of the programs execution. Since monitor provides metrics every
5 seconds, this implies we want to limit the output to the last 12 values to
cover a minute:

\begin{prompt}
%\textbf{monitor -l test2.log -n 12 eat\_mem 4}%
Consuming 4 Gigabit of memory.
\end{prompt}

Note that this option is only available when monitor writes its metrics to a
log file, not when standard error is used.

The interval at which monitor will show the metrics can be modified by
specifying delta, the sample rate:

\begin{prompt}
%\shellcmd{monitor -d 1 ./eat\_mem}%
Consuming 3 Gigabit of memory.
\end{prompt}

Monitor will now print the program's metrics every second. Note that the
minimum delta value is 1 second.

Alternative options to monitor the memory consumption are the ``\emph{top}''
or the ``\emph{htop}'' command.

\begin{description}
  \item[top] provides an ongoing look at processor activity in real time. It displays a listing of the most CPU-intensive tasks on the system, and can provide an interactive interface for manipulating processes. It can sort the tasks by memory usage, CPU usage and runtime.
  \item[htop] is similar to top, but shows the CPU-utilization for all the CPUs in the machine and allows to scroll the list vertically and horizontally to see all processes and their full command lines.
\end{description}

\begin{prompt}
%\shellcmd{top}%
%\shellcmd{htop}%
\end{prompt}

\subsection{Setting the memory parameter}

Once you gathered a good idea of the overall memory consumption of your
application, you can define it in your job script.  It is wise to foresee a
margin of about 10\%.

\underbar{Sequential or single-node applications:}

The maximum amount of physical memory used by the job can be specified in a job
script as:

\begin{prompt}
#PBS -l mem=4gb
\end{prompt}

or on the command line
\begin{prompt}
%\shellcmd{qsub -l mem=4gb}%
\end{prompt}

This setting is ignored if the number of nodes is not 1.

\underbar{Parallel or multi-node applications:}

When you are running a parallel application over multiple cores, you can also
specify the memory requirements per processor (pmem). This directive specifies
the maximum amount of physical memory used by any process in the job.

For example, if the job would run four processes and each would use up to 2 GB
(gigabytes) of memory, then the directive would read:

\begin{prompt}
#PBS -l pmem=2gb
\end{prompt}

or on the command line
\begin{prompt}
%\shellcmd{qsub -l pmem=2gb}%
\end{prompt}

In this example, you request 8Gb of memory in total on the node.

\section{Specifying processors requirements}

Users are encouraged to fully utilize all the available cores on a certain
compute node. Once the required numbers of cores and nodes are decently
specified, it is also good practice to monitor the CPU utilization on these
cores and to make sure that all the assigned nodes are working at full load.

\subsection{Number of processors}

The number of core and nodes that a user shall request fully depends on the
architecture of the application. The developer designs his application with a
strategy for parallelization in mind. The application can be designed for a
certain fixed number or for a configurable number of nodes and cores. It is
wise to target a specific set of compute nodes (e.g., Westmere, Harpertown) for
your computing work and then to configure your software to nicely fill up all
processors on these compute nodes.

The \emph{/proc/cpuinfo} stores info about your CPU architecture like number of
CPUs, threads, cores, information about CPU caches, CPU family, model and much
more.  So, if you want to detect how many cores are available on a specific
machine:

\begin{prompt}
%\shellcmd{less /proc/cpuinfo}%
processor       : 0
vendor_id       : GenuineIntel
cpu family      : 6
model           : 23
model name      : Intel(R) Xeon(R) CPU  E5420  @ 2.50GHz
stepping        : 10
cpu MHz         : 2500.088
cache size      : 6144 KB
%\dots%
\end{prompt}

Or if you want to see it in a more readable format, execute:

\begin{prompt}
%\shellcmd{grep processor /proc/cpuinfo}%
processor : 0
processor : 1
processor : 2
processor : 3
processor : 4
processor : 5
processor : 6
processor : 7
\end{prompt}


In order to specify the number of nodes and the number of processors per node in your job script, use:

\begin{prompt}
#PBS -l nodes=N:ppn=M
\end{prompt}

or with equivalent parameters on the command line

\begin{prog}
%\shellcmd{qsub -l nodes=N:ppn=M}%
\end{prog}

In this example, you request 8Gb of memory in total on the node.

This specifies the number of nodes (nodes=N) and the number of processors per
node (ppn=M) that the job should use. PBS treats a processor core as a
processor, so a system with eight cores per compute node can have ppn=8 as its
maximum ppn request. Note that unless a job has some inherent parallelism of
its own through something like MPI or OpenMP, requesting more than a single
processor on a single node is usually wasteful and can impact the job start
time.

\subsection{Monitoring the CPU-utilization}

The previously used ``monitor'' tool also shows the overall CPU-load. The
``eat\_cpu'' program performs a multiplication of 2 randomly filled a 1500x1500
matrices and is just written to consumes a lot of ``\emph{cpu}''.

We first load the monitor modules, study the ``eat\_cpu.c'' program and compile it:
\begin{prompt}
%\shellcmd{module load monitor}%
%\shellcmd{cat eat\_cpu.c}%
%\shellcmd{gcc -o eat\_cpu eat\_cpu.c}%
\end{prompt}

And then start to monitor the \emph{eat\_cpu} program:
\begin{prompt}
%\shellcmd{monitor -d 1 ./eat\_cpu}%
time  (s) size (kb) %\%%mem %\%%cpu
1  52852  0.3 100
2  52852  0.3 100
3  52852  0.3 100
4  52852  0.3 100
5  52852  0.3  99
6  52852  0.3 100
7  52852  0.3 100
8  52852  0.3 100
\end{prompt}

We notice that it the program keeps its CPU nicely busy at 100\%.

Some processes spawn one or more sub-processes. In that case, the metrics shown
by monitor are aggregated over the process and all of its sub-processes
(recursively). The reported CPU usage is the sum of all these processes, and
can thus exceed 100 \%.

Some (well, since this is a \hpc Cluster, we hope most) programs use more
than one core to perform their computations. Hence, it should not come as a
surprise that the CPU usage is reported as larger than 100 \%. When programs of
this type are running on a computer with n cores, the CPU usage can go up to n
x 100 \%.

This could also be monitored with the \strong{\emph{htop}} command:
\begin{prompt}
%\shellcmd{htop}%
\end{prompt}
\verbatiminput{\exampledir/htop-output}

The advantage of htop is that it shows you the cpu utilization for all
processors as well as the details per application.  A nice exercise is to start
4 instances of the ``cpu\_eat'' program in 4 different terminals, and inspect
the cpu utilization per processor with monitor and htop.

If \strong{\emph{htop}} reports that your program is taking 75\% CPU on a
certain processor, it means that 75\% of the samples taken by top found your
process active on the CPU. The rest of the time your application was in a wait.
(It is important to remember that a CPU is a discrete state machine. It really
can be at only 100\%, executing an instruction, or at 0\%, waiting for
something to do. There is no such thing as using 45\% of a CPU. The CPU
percentage is a function of time.) However, it is likely that your
application's rest periods include waiting to be dispatched on a CPU and not on
external devices. That part of the wait percentage is then very relevant to
understanding your overall CPU usage pattern.

\subsection{Fine-tuning your executable and/or job-script}

It is good practice to perform a number of runtime stress tests, and to check
the CPU utilization of your nodes. We (and all other users of the \hpc) would
appreciate that you use the maximum of the CPU resources that are assigned to
you and make sure that there are no CPU's in your node who are not utilized
without reasons.

But how can you maximize?

\begin{enumerate}
\item  Configure your software. (e.g., to exactly use the available amount of processors in a node)
\item  Develop your parallel program in a smart way.
\item  Demand a specific type of compute node (e.g., Harpertown, Westmere), which have a specific number of cores.
\item  Correct your request for CPU's in your job-script.
\end{enumerate}

\section{The system load}

On top of the CPU utilization, it is also important to check the \strong{system
load}.  The system \strong{load} is a measure of the amount of computational
work that a computer system performs.

The system load is the number of applications running or waiting to run on the
compute node.  In a system with for example four CPUs, a load average of 3.61
would indicate that there were, on average, 3.61 processes ready to run, and
each one could be scheduled into a CPU.

The load averages differ from CPU percentage in two significant ways:

\begin{enumerate}
\item  ``\emph{load averages}'' measure the trend in CPU utilization (and not only an instantaneous snapshot, as does CPU percentage); and
\item  ``\emph{load averages}'' include all demand for the CPU (and not only how much was active at the time of measurement).
\end{enumerate}

\subsection{Optimal load}

What is the ``\emph{optimal load}'' rule of thumb?

The load averages tell us whether our physical CPUs are over- or
under-utilized. The \strong{point of perfect utilization}, meaning that the
CPUs are always busy and, yet, no process ever waits for one, is \strong{the
average matching the number of CPUs}.  Your load should not exceed the number
of cores available.  E.g., if there are four CPUs on a machine and the reported
one-minute load average is 4.00, the machine has been utilizing its processors
perfectly for the last 60 seconds. The "100\% utilization" mark is 1.0 on a
single-core system, 2.0 on a dual-core, 4.0 on a quad-core, etc. The optimal
load shall be between 0.7 and 1.0 per processor.

In general, the intuitive idea of load averages is the higher they rise above
the number of processors, the more demand there is for the CPUs, and the lower
they fall below the number of processors, the more untapped CPU capacity there
is.

\emph{Load averages} do not include any processes or threads waiting on I/O,
networking, databases or anything else not demanding the CPU. It narrowly
focuses on what is actively demanding CPU time. This means that the optimal
\emph{number of applications} running on a system at the same time, might be
more than one per processor.

The ``\strong{optimal number of applications}'' running on one machine at the
same time depends on the type of the applications that you are running.

\begin{enumerate}
\item  When you are running \strong{computational intensive applications}, one
  application per processor will generate the optimal load.
\item  For \strong{IO intensive applications} (e.g., applications with perform
  a lot of disk-IO), a higher number of applications can generate the optimal
  load. While some applications are reading or writing data on disks, the
  processors can serve other applications.
\end{enumerate}

The optimal number of applications on a machine could be empirically calculated
by performing a number of stress tests, whilst checking the highest throughput.
There is however no manner in the \hpc at the moment to specify the maximum
number of applications that shall run per core dynamically. The \hpc scheduler
will not launch more than one process per core.

The manner how the cores are spread out over CPUs does not matter for what
regards the load. Two quad-cores perform similar to four dual-cores, and again
perform similar to eight single-cores. It's all eight cores for these purposes.

\subsection{Monitoring the load}

The \strong{load average} represents the average system load over a period of
time. It conventionally appears in the form of three numbers, which represent
the system load during the last \strong{one}-, \strong{five}-, and
\strong{fifteen}-minute periods.

The \strong{uptime} command will show us the average load
\begin{prompt}
%\shellcmd{uptime}%
10:14:05 up 86 days, 12:01, 11 users, load average: 0.60, 0.41, 0.41
\end{prompt}

Now, start a few instances of the ``\emph{eat\_cpu}'' program in the
background, and check the effect on the load again:

\begin{prompt}
%\shellcmd{./eat\_cpu\&}%
%\shellcmd{./eat\_cpu\&}%
%\shellcmd{./eat\_cpu\&}%
%\shellcmd{uptime}%
10:14:42 up 86 days, 12:02, 11 users, load average: 2.60, 0.93, 0.58
\end{prompt}

You can also read it in the \strong{htop} command.
\subsection{Fine-tuning your executable and/or job-script}

It is good practice to perform a number of runtime stress tests, and to check
the system load of your nodes. We (and all other users of the \hpc) would
appreciate that you use the maximum of the CPU resources that are assigned to
you and make sure that there are no CPU's in your node who are not utilized
without reasons.

But how can you maximize?

\begin{enumerate}
\item  Profile your software to improve its performance.
\item  Configure your software (e.g., to exactly use the available amount of processors in a node).
\item  Develop your parallel program in a smart way, so that it fully utilizes the available processors.
\item  Demand a specific type of compute node (e.g., Harpertown, Westmere), which have a specific number of cores.
\item  Correct your request for CPU's in your job-script.
\end{enumerate}

And then check again.

\section{Checking File sizes \& Disk IO}

\subsection{Monitoring File sizes during execution}

Some programs generate intermediate or output files, the size of which may also
be a useful metric.

Remember that your available disk space on the \hpc online storage is limited,
and that you 3 environment variables which point to these directories available
(i.e. \emph{\$VSC\_DATA}, \emph{\$VSC\_SCRATCH} and \emph{\$VSC\_DATA}).
On top of those, you can also access some temporary storage (i.e., the /tmp
directory) on the compute node, which is defined by the
\emph{\$VSC\_SCRATCH\_LOCAL} environment variable.

We first load the monitor modules, study the ``eat\_disk.c'' program and
compile it:

\begin{prompt}
%\shellcmd{module load monitor}%
%\shellcmd{cat eat\_disk.c}%
%\shellcmd{gcc -o eat\_disk eat\_disk.c}%
\end{prompt}

The \emph{monitor} tool provides an option (-f) to display the size of one or
more files:

\begin{prompt}
%\shellcmd{monitor -f \$VSC\_SCRATCH/test.txt ./eat\_disk}%
time (s) size (kb) %\%%mem %\%%cpu
5  1276  0 38.6 168820736
10  1276  0 24.8 238026752
15  1276  0 22.8 318767104
20  1276  0 25 456130560
25  1276  0 26.9 614465536
30  1276  0 27.7 760217600
%\dots%
\end{prompt}

Here, the size of the file `\emph{test.txt}' in directory \$VSC\_SCRATCH will
be monitored. Files can be specified by absolute as well as relative path, and
multiple files are separated by ','.

It is important to be aware of the sizes of the file that will be generated, as
the available disk space for each user is limited.  We refer to Chapter 6.5 on
``Quota's'' to check your quota and tools to find which files consumed the
``quota''.

Several actions can be taken, to avoid storage problems:

\begin{enumerate}
\item  Be aware of all the files that are generated by your program. Also check out the hidden files.
\item  Check your quota consumption regularly.
\item  Clean up your files regularly.
\item  First work (i.e. read and write) with your big files in the local /tmp directory. Once finished, you can move your files once to the VSC\_DATA directories.
\item  Make sure your programs clean up their temporary files after execution.
\item  Move your output results to your own computer regularly.
\item  Anyone can request more disk space to the \hpc staff, but you will have to duly justify your request.
\end{enumerate}

\subsection{Monitoring Disk IO}

If your Linux system gets slow down due to heavy disk I/O activities, you
probably want to know which processes or users (in case of multi-user systems)
are the culprit for such activities.

If you want to monitor disk I/O activities of individual Linux processes, you
can try \emph{iotop}. This tool shows a sorted list of the most I/O intensive
processes in real time via a \emph{top}-like interface.

\begin{prompt}
%\shellcmd{iotop}%
\end{prompt}

Running \emph{iotop} without any argument like above shows a list of \emph{all}
existing processes regardless of their disk I/O activities. If you want
\emph{iotop} to only show processes that are actually doing disk I/O, run the
following instead.

\begin{prompt}
%\shellcmd{iotop -o}%
\end{prompt}

\underbar{Remark}: The iotop tool is not available on all compute nodes.

If you are interested in monitoring disk read/write rates of individual disks,
you can use \emph{iostat}.

\begin{prompt}
%\shellcmd{iostat}%
Linux 2.6.18-164.6.1.el5 (r1e2cn12)  11/04/2013

avg-cpu:  %\%%user   %\%%nice %\%%system %\%%iowait  %\%%steal   %\%%idle
          69.33    0.00   13.02    0.08    0.00   17.57

Device:    tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn
sda       3.05         0.27       450.05    2037907 3366173632
sda1      3.05         0.27       450.05    2036637 3366173632
sda2      0.00         0.00         0.00        851          0
\end{prompt}

\section{Specifying network requirements}

The user can examine his network activities with the htop command. When your
processors are 100\% busy, but you see a lot of red bars and only limited green
bars in the htop screen, it is mostly an indication that they loose a lot of
time with inter-process communication.

Whenever your application utilizes a lot of inter-process communication (as is
the case in most parallel programs), we strongly recommend to request nodes
with an ``infiniband'' network. The Infiniband is a specialized high bandwidth,
low latency network that enables large parallel jobs to run as efficiently as
possible.

The parameter to add in your job-script would be:
\begin{prompt}
#PBS -l ib
\end{prompt}

If for some other reasons, a user is fine with the Gigabit Ethernet network, he
can specify:

\begin{prompt}
#PBS -l gbe
\end{prompt}

\section{Some more tips on the Monitor tool}

\subsection{Command Lines arguments}

Many programs, e.g., matlab, take command line options. To make sure these do
not interfere with those of monitor and vice versa, the program can for
instance be started in the following way:

\begin{prompt}
%\shellcmd{monitor -delta 60 -- matlab -nojvm -nodisplay computation.m}%
\end{prompt}

The use of '--' will ensure that monitor does not get confused by matlab's '-nojvm' and '-nodisplay' options.

\subsection{Exit Code}

Monitor will propagate the exit code of the program it is watching. Suppose the
latter ends normally, then monitor's exit code will be 0. On the other hand,
when the program terminates abnormally with a non-zero exit code, e.g., 3, then
this will be monitor's exit code as well.

When monitor has to terminate in an abnormal state, for instance if it can't
create the log file, its exit code will be 65. If this interferes with an exit
code of the program to be monitored, it can be modified by setting the
environment variable MONITOR\_EXIT\_ERROR to a more suitable value.

\subsection{Monitoring a running process}

It is also possible to "attach" monitor to a program or process that is already
running. One simply determines the relevant process ID using the ps command,
e.g., 18749, and starts monitor:

\begin{prompt}
%\shellcmd{monitor -p 18749}%
\end{prompt}

Note that this feature can be (ab)used to monitor specific sub-processes.
