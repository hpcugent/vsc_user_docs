\chapter{Best Practices}
\label{ch:best-practices}

\section{General Best Practices}
\label{sec:general-best-practices}
\begin{enumerate}

  \item  Before starting you should always check:
  \begin{enumerate}
    \item  Are there any errors in the script?
    \item  Are the required modules loaded?
    \item  Is the correct executable used?
  \end{enumerate}

  \item  Check your computer requirements upfront, and request the correct resources in your PBS configuration script.
  \begin{enumerate}
    \item  Number of requested cores
    \item  Amount of requested memory
    \item  Requested network type
  \end{enumerate}

  \item  Check your jobs at runtime. You could login to the node and check the
    proper execution of your jobs with, e.g., ``top'' or ''vmstat''.
    Alternatively you could run an interactive job (``qsub -I'').

  \item  Try to benchmark the software for scaling issues when using MPI or for
    I/O issues.

  \item  Use the scratch file system (\$VSC\_SCRATCH\_NODE which is mapped to the
    local /tmp) whenever possible. Local disk I/O is always much faster as it
    does not have to use the network.

  \item  When your job starts, it will log on to the compute node(s) and start
    executing the commands in the job script. It will start in your home
    directory (\$VSC\_HOME), so going to the current directory with the ``cd
    \$PBS\_O\_WORKDIR'' is the first thing which needs to be done.  You will
    have your default environment, so don't forget to load the software with
    ``module load''.

  \item  In case your job not running, use ``checkjob''.  It will show why your
    job is not yet running. Sometimes commands might timeout with an overloaded
    scheduler.

  \item  Submit your job and wait (be patient) \ldots

  \item  Submit small jobs by grouping them together. The ``Worker Framework''
    has been designed for these purposes.

  \item  The runtime is limited by the maximum walltime of the queues. For
    longer walltimes, use checkpointing.

  \item  Requesting many processors could imply long queue times.

  \item  For all parallel computing, request to use ``Infiniband''.

  \item  And above all \dots\ do not hesitate to contact the \hpc staff. We're
    here to help you.
\end{enumerate}

\section{Windows / Unix}

Important note: the PBS file on the \hpc has to be in UNIX format, if it is
not, your job will fail and generate rather weird error messages.

If necessary, you can convert it using
\begin{prompt}
%\shellcmd{dos2unix file.pbs}%
\end{prompt}

% ==================================================================================================

\section{Best Practices for EasyBuild}
\label{sec:best-practices-easybuild}

\textit{(coming soon)}

% ==================================================================================================

\section{Best Practices for \texttt{mympirun}}
\label{sec:best-practices-mympirun}
\label{sec:best-practices-mympirun-module}
\label{sec:best-practices-mympirun-cores}
\label{sec:best-practices-mympirun-hybrid}
\label{sec:best-practices-mympirun-universe}

\textit{(coming soon)}

For now, see {\small\url{https://github.com/hpcugent/vsc-mympirun/blob/master/README.md}}.

% ==================================================================================================

\section{Best practices for OpenFOAM}
\label{sec:best-practices-openfoam}

In this section, we outline best practices for using the centrally provided OpenFOAM installations
on the VSC \hpc infrastructure.

\textit{last update}: September 2017

\textit{authors}: Kenneth Hoste (HPC-UGent), with feedback from Joris Degroote (UGent), Brecht Devolder (UGent),
                  Pieter Reyniers (UGent), Laurien Vandewalle (UGent)


\subsection{Different OpenFOAM releases}
\label{sec:best-practices-openfoam-releases}

There are currently three different sets of versions of OpenFOAM available, each with its own versioning scheme:

\begin{itemize}
    \item OpenFOAM versions released via \url{http://openfoam.com}: {\small\texttt{v3.0+}}, {\small\texttt{v1706}}
    \begin{itemize}
        \item see also \url{http://openfoam.com/history/}
    \end{itemize}
    \item OpenFOAM versions released via \url{https://openfoam.org}: {\small\texttt{v4.1}}, {\small\texttt{v5.0}}
    \begin{itemize}
        \item see also \url{https://openfoam.org/download/history/}
    \end{itemize}
    \item OpenFOAM versions released via \url{http://wikki.gridcore.se/foam-extend}: {\small\texttt{v3.1}}
\end{itemize}

Make sure you known which flavor of OpenFOAM you want to use, since there are important differences between
the different versions w.r.t.\ features.

If the OpenFOAM version you need is not available yet, submit a request to install it via \hpcinfo.


\subsection{Documentation \& training material}
\label{sec:best-practices-openfoam-documentation}

The best practices outlined here focus specifically on the use of OpenFOAM on the VSC \hpc infrastructure.
As such, they are intented to augment the existing OpenFOAM documentation rather than replace it.

For more general information on using OpenFOAM, please refer to:

\begin{itemize}
\item OpenFOAM websites:
\begin{itemize}
    \item \url{http://openfoam.com}
    \item \url{https://openfoam.org}
    \item \url{http://wikki.gridcore.se/foam-extend}
\end{itemize}
\item OpenFOAM user guides:
    \begin{itemize}
    \item \url{http://www.openfoam.com/documentation/user-guide}
    \item \url{https://cfd.direct/openfoam/user-guide/}
    \end{itemize}
\item recordings of "\textit{Introduction to OpenFOAM}" training session at UGent (May 2016):\\
      \small{\url{https://www.youtube.com/playlist?list=PLqxhJj6bcnY9RoIgzeF6xDh5L9bbeK3BL}}
\end{itemize}


\subsection{Preparing the environment}
\label{sec:best-practices-openfoam-environment}

To prepare the environment of your shell session or job for using OpenFOAM,
there are a couple of things to take into account.


\subsubsection{Picking and loading an {\small\texttt{OpenFOAM}} module}

First of all, you need to pick and load one of the available {\small\texttt{OpenFOAM}} modules.

To get an overview of the available modules, run `{\small\texttt{module avail OpenFOAM}}'. For example:

\begin{prompt}
%\shellcmd{module avail OpenFOAM}%
% %
%------------------ /apps/gent/CO7/sandybridge/modules/all ------------------%
%   OpenFOAM/2.4.0-intel-2017a~~~~~OpenFOAM/3.0.1-intel-2016b%
%   OpenFOAM/4.0-intel-2016b~~~~~~~OpenFOAM/4.1-intel-2017a%
\end{prompt}

To pick a module, take into account the differences between the different OpenFOAM versions w.r.t.\ features and
API (see also Section~\ref{sec:best-practices-openfoam-releases}).

If multiple modules are available that fulfill your requirements, give preference to those providing a more recent
OpenFOAM version, and to the ones that were installed with a more recent compiler toolchain; for example, prefer
a module that includes `{\small\texttt{intel-2017a}}' in its name over one that includes
`{\small\texttt{intel-2016b}}'.

To prepare your environment for using OpenFOAM, load the {\small\texttt{OpenFOAM}} module you have picked; for example:

\begin{prompt}
%module load OpenFOAM/4.1-intel-2017a%
\end{prompt}

\subsubsection{Sourcing the {\small\texttt{\$FOAM\_BASH}} script}

OpenFOAM provides a script that you should \texttt{\small{source}} to further prepare the environment.
This script will define some additional environment variables that are required to use OpenFOAM.
The {\small\texttt{OpenFOAM}} modules define an environment variable named `\texttt{\small\$FOAM\_BASH}'
that specifies the location to this script.

Assuming you are using \texttt{\small{bash}} in your shell session or job script,
you should always run the following command after loading an \texttt{\small{OpenFOAM}} module:

\begin{prompt}
%source \$FOAM\_BASH%
\end{prompt}


\subsubsection{Defining utility functions used in tutorial cases}

If you would like to use the \texttt{\small{getApplication}}, \texttt{\small{runApplication}},
\texttt{\small{runParallel}}, \texttt{\small{cloneCase}} and/or \texttt{\small{compileApplication}} functions that are
used in OpenFOAM tutorials, you also need to \texttt{\small{source}} the \texttt{\small{RunFunctions}} script:

\begin{prompt}
%source \$WM\_PROJECT\_DIR/bin/tools/RunFunctions%
\end{prompt}

Note that this needs to be done \textbf{after} sourcing \texttt{\small{\$FOAM\_BASH}} to make sure
\texttt{\small{\$WM\_PROJECT\_DIR}} is defined.


\subsubsection{Dealing with floating-point errors}

If you are seeing ``\texttt{\small{Floating Point Exception}}'' errors, you can undefine the
\texttt{\small{\$FOAM\_SIGFPE}} environment variable that is defined by the \texttt{\small{\$FOAM\_BASH}} script,
as follows:

\begin{prompt}
%unset \$FOAM\_SIGFPE%
\end{prompt}

Note that this only prevents OpenFOAM from propogating floating point exceptions, which then results in
terminating the simulation; it does not prevent that illegal operations (like a division by zero) are being executed.
As such, \textbf{you should \textit{not} use this in production runs}. Instead, you should track down the root cause
of the floating point exceptions, and try to prevent them from occuring at all.


\subsection{OpenFOAM workflow}

The general workflow for OpenFOAM consists of multiple steps.

Prior to running the actual simulaton, some \textit{pre-processing} needs to be done:

\begin{itemize}
\item generate the mesh; see also {\small\url{https://cfd.direct/openfoam/user-guide/mesh}};
\item decompose the domain into subdomains using {\small\texttt{decomposePar}} (only for parallel OpenFOAM simulations);
\end{itemize}

After running the simulation, some \textit{post-processing} steps are typically performed:

\begin{itemize}
\item reassemble the decomposed domain using {\small\texttt{reconstructPar}} (only for parallel OpenFOAM simulations);
\item evaluate or further process the simulation results, either visually using ParaView
      (for example, via the {\small\texttt{paraFoam}} tool) or using command-line tools like {\small\texttt{postProcess}};
      see also {\small\url{https://cfd.direct/openfoam/user-guide/postprocessing}}

\end{itemize}

Depending on the size of the domain and the desired format of the results, these pre- and post-processing
steps can be run either before/after the job running the actual simulation, either on the HPC infrastructure
or elsewhere, or as a part of the job that runs the OpenFOAM simulation itself.
Do make sure you are using the same OpenFOAM version in each of the steps.

One important aspect to keep in mind for `offline' pre-processing is that the domain decomposition needs to match
the number of processor cores that are used for the actual simulation,
see also Section~\ref{sec:best-practices-openfoam-domain-decomposition-processor-cores}.

For post-processing you can either download the simulation results to a local workstation,
or do the post-processing (interactively) on the HPC infrastructure, for example on the login nodes
or using an interactive session on a workernode. This may be interesting to avoid the overhead of
downloading the results locally.


\subsection{Running OpenFOAM in parallel}

For general information on running OpenFOAM in parallel,
see {\small\url{https://cfd.direct/openfoam/user-guide/running-applications-parallel/}}.

\subsubsection{The {\small\texttt{-parallel}} option}

When running OpenFOAM in parallel, \textbf{do not forget to specify the \texttt{\small{-parallel}} option},
to avoid running the same OpenFOAM simulation $N$ times, rather than running it once using $N$ processor cores.

You can check whether OpenFOAM was run in parallel in the output of the main command:
the OpenFOAM header text should only be included \textit{once} in the output, and it should specify a value different
than `\texttt{\small{1}}' in the \texttt{\small{nProcs}} field. Note that pre- and post-processing utilities like
\texttt{\small{blockMesh}}, \texttt{\small{decomposePar}} and \texttt{\small{reconstructPar}} can not be run in parallel.

\subsubsection{Using \texttt{mympirun}}

It is highly recommended to use the {\small\texttt{mympirun}} command when running parallel OpenFOAM simulations
rather than the standard {\small\texttt{mpirun}} command;
see Section~\ref{sec:best-practices-mympirun} for more information on {\small\texttt{mympirun}}.

To use {\small\texttt{mympirun}}, make sure that the \texttt{\small{vsc-mympirun}} module is loaded.

\begin{prompt}
%module load vsc-mympirun%
\end{prompt}

Note that you should \textit{not} specify a specific version here,
see also Section~\ref{sec:best-practices-mympirun-module}.

To pass down the environment variables required to run OpenFOAM (which were defined by the
\texttt{\small{\$FOAM\_BASH}} script, see Section~\ref{sec:best-practices-openfoam-environment})
to each of the MPI processes used in a parallel OpenFOAM execution,
the \texttt{\small{\$MYMPIRUN\_VARIABLESPREFIX}} environment variable must be defined as follows,
prior to running the OpenFOAM simulation with \texttt{\small{mympirun}}:

\begin{prompt}
%export MYMPIRUN\_VARIABLESPREFIX=WM\_PROJECT,FOAM,MPI%
\end{prompt}

Whenever you are instructed to use a command like `\texttt{\small{mpirun -np <N>}} ...',
use `\texttt{\small{mympirun}} ...' instead; \texttt{\small{mympirun}} will automatically detect the number of
processor cores that are available (see also Section~\ref{sec:best-practices-mympirun-cores}).


\subsubsection{Domain decomposition and number of processor cores}
\label{sec:best-practices-openfoam-domain-decomposition-processor-cores}

To run OpenFOAM in parallel, you must decompose the domain into multiple subdomains.
Each subdomain will be processed by OpenFOAM on one processor core.

Since {\small\texttt{mympirun}} will automatically use all available cores, you need to make sure
that the number of subdomains matches the number of processor cores that will be used by {\small\texttt{mympirun}}.

If not, you may run into an error message like:

\begin{prompt}
number of processor directories = 4 is not equal to the number of processors = 16
\end{prompt}

In this case, the case was decomposed in 4 subdomains, while the OpenFOAM simulation was started with 16 processes
through {\small\texttt{mympirun}}.

To match the number of subdomains and the number of processor cores used by {\small\texttt{mympirun}}, 
you should either:

\begin{itemize}
\item adjust the value for {\small\texttt{numberOfSubdomains}} in {\small\texttt{system/decomposeParDict}}
(and adjust the value for {\small\texttt{n}} accordingly in the domain decomposition coefficients),
and run {\small\texttt{decomposePar}} again; or
\item submit your job requesting exactly the same number of processor cores as there are subdomains (see the
number of {\small\texttt{processor*}} directories that were created by {\small\texttt{decomposePar}}
\end{itemize}

Alternatively you can specify to \texttt{mympirun} that it should only start a certain number of MPI processes,
via {\small\texttt{-{}-hybrid}} (see Section~\ref{sec:best-practices-mympirun-hybrid}),
taking into account the number of requested workernodes,
or {\small\texttt{-{}-universe}} (see Section~\ref{sec:best-practices-mympirun-universe}).
This is interesting if you require more memory per core than is available by default.

Note that the decomposition method being used (which is specified in {\small\texttt{system/decomposeParDict}}
has significant impact on the performance of a parallel OpenFOAM simulation. Good decomposition methods (like
{\small\texttt{metis}} or {\small\texttt{scotch}} try to limit communication overhead by minimising the number
of processor boundaries; see also Section~\ref{sec:best-practices-openfoam-scaling}.


\subsection{Running OpenFOAM on a shared filesystem}
\label{sec:best-practices-openfoam-shared-filesystems}

OpenFOAM is known to significantly stress shared filesystems, since a lot of (small) files are generated
during an OpenFOAM simulation Shared filesystems are typically optimised for dealing with (a small number of)
large files, and are usually a poor match for workloads that involve a (very) large number of small files.
See also {\small\url{http://www.prace-ri.eu/IMG/pdf/IO-profiling_with_Darshan-2.pdf}}.

Take into account the following guidelines for your OpenFOAM jobs, which all relate to input parameters
for the OpenFOAM simulation that you can specify in \texttt{\small{system/controlDict}};
see also {\small\url{https://cfd.direct/openfoam/user-guide/controldict}}.

\begin{itemize}
\item instruct OpenFOAM to write out results at a reasonable frequency, \textbf{certainly \textit{not}
for every single time step}; you can control this using the \texttt{\small{writeControl}},
\texttt{\small{writeInterval}}, etc.\ keywords;
\item consider only retaining results for the last couple of time steps, see the \texttt{\small{purgeWrite}} keyword;
\item if you do not plan to change the parameters of the OpenFOAM simulation while it is running,
      set \texttt{\small{runTimeModifiable}} to \texttt{\small{false}} to avoid that OpenFOAM re-reads each of
      the \texttt{\small{system/*Dict}} files at every time step;
\item if the results per individual time stamp are large, consider setting \texttt{\small{writeCompression}} to
      \texttt{\small{true}};
\end{itemize}

For modest OpenFOAM simulations where a single workernode suffices, consider using the local disk of the
workernode as working directory (accessible via \texttt{\small{\$VSC\_SCRATCH\_NODE}}),
rather than the shared \texttt{\small{\$VSC\_SCRATCH}} filesystem. \textbf{Certainly
do not use a subdirectory in \texttt{\small{\$VSC\_HOME}} or \texttt{\small{\$VSC\_DATA}} as working directory for
OpenFOAM simulations}, since these shared filesystems are too slow for these type of workloads.

\ifgent
For large parallel OpenFOAM simulations on the \university Tier-2 clusters, consider using the
alternative shared scratch filesystem {\small\texttt{\$VSC\_SCRATCH\_PHANPY}} that is powered by
solid-state drives (SSDs) and is significantly faster than the standard {\small\texttt{\$VSC\_SCRATCH}}
shared filesystems, especially for workloads involving lots of (small) files. Note that fast access to
{\small\texttt{\$VSC\_SCRATCH\_PHANPY}} is available on each of the \university Tier-2 clusters except
{\small\texttt{raichu}}, not only on {\small\texttt{phanpy}}.
\fi

These guidelines are especially important for large-scale OpenFOAM simulations that involve
more than a couple of dozen of processor cores.


\subsection{Scaling of OpenFOAM on VSC HPC clusters}
\label{sec:best-practices-openfoam-scaling}

\textit{coming soon}


\subsection{Using own solvers with OpenFOAM}
\label{sec:best-practices-openfoam-own-solvers-libraries}

\textit{coming soon}

See also {\small\url{https://cfd.direct/openfoam/user-guide/compiling-applications/}}.


\subsection{Example OpenFOAM job script}
\label{sec:best-practices-openfoam-example-script}

Example job script for {\small\texttt{damBreak}} OpenFOAM tutorial,
see also {\small\url{https://cfd.direct/openfoam/user-guide/dambreak}}.

\examplecode{bash}{OpenFOAM_damBreak.sh}
