\chapter{Best Practices}
\label{ch:best-practices}

\section{General Best Practices}
\label{sec:general-best-practices}
\begin{enumerate}

  \item  Before starting you should always check:
  \begin{enumerate}
    \item  Are there any errors in the script?
    \item  Are the required modules loaded?
    \item  Is the correct executable used?
  \end{enumerate}

  \item  Check your computer requirements upfront, and request the correct resources in your PBS configuration script.
  \begin{enumerate}
    \item  Number of requested cores
    \item  Amount of requested memory
    \item  Requested network type
  \end{enumerate}

  \item  Check your jobs at runtime. You could login to the node and check the
    proper execution of your jobs with, e.g., ``top'' or ''vmstat''.
    Alternatively you could run an interactive job (``qsub -I'').

  \item  Try to benchmark the software for scaling issues when using MPI or for
    I/O issues.

  \item  Use the scratch file system (\$VSC\_SCRATCH\_NODE which is mapped to the
    local /tmp) whenever possible. Local disk I/O is always much faster as it
    does not have to use the network.

  \item  When your job starts, it will log on to the compute node(s) and start
    executing the commands in the job script. It will start in your home
    directory (\$VSC\_HOME), so going to the current directory with the ``cd
    \$PBS\_O\_WORKDIR'' is the first thing which needs to be done.  You will
    have your default environment, so don't forget to load the software with
    ``module load''.

  \item  In case your job not running, use ``checkjob''.  It will show why your
    job is not yet running. Sometimes commands might timeout with an overloaded
    scheduler.

  \item  Submit your job and wait (be patient) \ldots

  \item  Submit small jobs by grouping them together. The ``Worker Framework''
    has been designed for these purposes.

  \item  The runtime is limited by the maximum walltime of the queues. For
    longer walltimes, use checkpointing.

  \item  Requesting many processors could imply long queue times.

  \item  For all parallel computing, request to use ``Infiniband''.

  \item  And above all \dots\ do not hesitate to contact the \hpc staff. We're
    here to help you.
\end{enumerate}

\section{Windows / Unix}

Important note: the PBS file on the \hpc has to be in UNIX format, if it is
not, your job will fail and generate rather weird error messages.

If necessary, you can convert it using
\begin{prompt}
%\shellcmd{dos2unix file.pbs}%
\end{prompt}


\section{Software-Specific Best Practices: Tools}
\label{sec:software-specific-best-practices-tools}

\subsection{EasyBuild}
\label{sec:best-practices-easybuild}

\textit{(coming soon)}

\section{Software-Specific Best Practices: Scientific Software}
\label{sec:software-specific-best-practices-scientific-software}

\subsection{OpenFOAM}
\label{sec:best-practices-openfoam}

\subsubsection{Documentation}

\begin{itemize}
\item \url{http://www.openfoam.com/documentation/user-guide}
\item \url{https://cfd.direct/openfoam/user-guide/}
\item \url{https://www.youtube.com/playlist?list=PLqxhJj6bcnY9RoIgzeF6xDh5L9bbeK3BL}
\item \url{https://github.com/openfoamtutorials/openfoam\_tutorials} + \url{youtube.com}
\end{itemize}

\subsubsection{Using the provided OpenFOAM installations}

\begin{itemize}
\item picking \& loading an \texttt{OpenFOAM} module
\item \texttt{source \$FOAM\_BASHRC}
\item \texttt{source \$WM\_PROJECT\_DIR/bin/tools/RunFunctions}
\item \texttt{unset FOAM\_SIGFPE}
\end{itemize}

\subsubsection{Running OpenFOAM in parallel using \texttt{mympirun}}

\begin{itemize}
\item how (and why) to use \texttt{mympirun}
\item passing down OpenFOAM environment variables to MPI processes
\end{itemize}

\subsubsection{OpenFOAM vs shared filesystems}

\subsubsection{OpenFOAM Workflow}

\begin{itemize}
\item meshing (\texttt{decomposePar}): as a part of job script vs up front, attention points (e.g., \# cores used in job)
\item running simulation
\item post-processing (e.g.\ \texttt{reconstructPar}, ParaView)
\end{itemize}

\subsubsection{Scaling of OpenFOAM on VSC HPC clusters}

\begin{itemize}
\item scaling on relevant (HPC-UGent) Tier-2 systems
\item scaling on Tier-1b
\end{itemize}

using which input(s)?

\subsubsection{Using own solvers with OpenFOAM}

\begin{itemize}
\item compiling own solvers
\item using own solvers in jobs
\end{itemize}

\subsubsection{Example OpenFOAM job script}

tutorial example?

\textit{(coming soon)}
