Take into account the following guidelines for your OpenFOAM jobs, which
all relate to input parameters for the OpenFOAM simulation that you can
specify in system/controlDict (see also
https://cfd.direct/openfoam/user-guide/controldict).
-   instruct OpenFOAM to write out results at a reasonable frequency, **certainly *not*** for every single time step}; you can control this using the writeControl, writeInterval, etc.Â keywords;
-   consider only retaining results for the last couple of time steps,
    see the purgeWrite keyword;
-   consider writing results for only part of the domain (e.g., a line
    of plane) rather than the entire domain;
-   if you do not plan to change the parameters of the OpenFOAM
    simulation while it is running, set runTimeModifiable to false to avoid that OpenFOAM re-reads each
    of the system/*Dict files at every time step;
-   if the results per individual time step are large, consider setting
    writeCompression to true;
For modest OpenFOAM simulations where a single workernode suffices,
consider using the local disk of the workernode as working directory
(accessible via $VSC_SCRATCH_NODE), rather than the shared
$VSC_SCRATCH filesystem. **Certainly do not use a subdirectory in $VSC_HOME or $VSC_DATA, since these shared filesystems are too slow
for these types of workloads.
For large parallel OpenFOAM simulations on the UGent Tier-2 clusters, consider
using the alternative shared scratch filesystem $VSC_SCRATCH_ARCANINE
(see Pre-defined user directories).
These guidelines are especially important for large-scale OpenFOAM
simulations that involve more than a couple of dozen of processor cores.
Using own solvers with OpenFOAM 
See https://cfd.direct/openfoam/user-guide/compiling-applications/.
Example OpenFOAM job script
Example job script for damBreak OpenFOAM tutorial (see also
https://cfd.direct/openfoam/user-guide/dambreak):
#!/bin/bash
#PBS -l walltime=1:0:0
#PBS -l nodes=1:ppn=4
# check for more recent OpenFOAM modules with 'module avail OpenFOAM'
module load OpenFOAM/6-intel-2018a
source $FOAM_BASH
# purposely not specifying a particular version to use most recent mympirun
module load vsc-mympirun
# let mympirun pass down relevant environment variables to MPI processes
export MYMPIRUN_VARIABLESPREFIX=WM_PROJECT,FOAM,MPI
# set up working directory
# (uncomment one line defining $WORKDIR below)
#export WORKDIR=$VSC_SCRATCH/$PBS_JOBID  # for small multi-node jobs
#export WORKDIR=$VSC_SCRATCH_ARCANINE/$PBS_JOBID  # for large multi-node jobs (not on available victini)
export WORKDIR=$VSC_SCRATCH_NODE/$PBS_JOBID  # for single-node jobs
mkdir -p $WORKDIR
# damBreak tutorial, see also https://cfd.direct/openfoam/user-guide/dambreak
cp -r $FOAM_TUTORIALS/multiphase/interFoam/laminar/damBreak/damBreak $WORKDIR
cd $WORKDIR/damBreak
echo "working directory: $PWD"
# pre-processing: generate mesh
echo "start blockMesh: $(date)"
blockMesh &> blockMesh.out
# pre-processing: decompose domain for parallel processing
echo "start decomposePar: $(date)"
decomposePar &> decomposePar.out
# run OpenFOAM simulation in parallel
# note:
#  * the -parallel option is strictly required to actually run in parallel!
#    without it, the simulation is run N times on a single core...
#  * mympirun will use all available cores in the job by default,
#    you need to make sure this matches the number of subdomains!
echo "start interFoam: $(date)"
mympirun --output=interFoam.out interFoam -parallel
# post-processing: reassemble decomposed domain
echo "start reconstructPar: $(date)"
reconstructPar &> reconstructPar.out
# copy back results, i.e. all time step directories: 0, 0.05, ..., 1.0 and inputs
export RESULTS_DIR=$VSC_DATA/results/$PBS_JOBID
mkdir -p $RESULTS_DIR
cp -a *.out [0-9.]* constant system $RESULTS_DIR
echo "results copied to $RESULTS_DIR at $(date)"
# clean up working directory
cd $HOME
rm -rf $WORKDIR
