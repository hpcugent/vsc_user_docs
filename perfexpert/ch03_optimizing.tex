\chapter{Optimising}
\label{ch:ch03_optimizing}

\renewcommand{\exampledir}{examples/ch03-optimizing}

\begin{tip}
Find the source code for the examples in this chapter in the directory:  ``\tilde/\exampledir''.
\end{tip}

First we, go to the directory of the examples of Chapter03:

\begin{prompt}
%\shellcmd{cd ~/\exampledir{}}%
\end{prompt}

The blocks of code where time is being wasted during execution are called \emph{hotspots}.

Modern compilers are good at low-level code optimisation but they are limited by their ability to analyse the code and identify opportunities for optimisation. The programmer can help the compiler by using a clear and simple programming style and by avoiding code constructs that are unnecessary complicated. All more advanced optimisations have to be done by the programmer on the level of the source code, when he designs and develops the data structures and algorithms. Therefore, the developer needs an understanding of the program, how the data is stored in the computer memory, how it is accessed, and most important the impact of the data access in the memory subsystem (cache and TLB). Code optimisation is therefore best done as part of the program development process.

In this chapter, PerfExpert will

\begin{enumerate}
  \item  find the hotspots in several code samples, and will also
  \item  guide us in the optimisation of the code by suggestion optimisations.
\end{enumerate}

\section{Optimisation Strategy}
\label{sec:Optimization_Strategy}

\subsection{Find the Hotspots}
\label{subsec:Hotspots}

In order to optimise a program, we need to know where the program spends most of its execution time. These are called \emph{hotspots} in the execution. Optimising code that is not a hotspot doesn't speed up the execution and may instead make the program more difficult to understand and maintain.

When profiling and measuring the execution of a program, we should execute the program to solve a \emph{typical case}, i.e., a problem instance that is highly representative for most use of the program.

PerfExpert has one single mandatory option: the \emph{threshold} (relevance in \% of the total runtime) to take hotspots into consideration. Its value range varies between 0 and 1. A value of 0.25 means that PerfExpert will only investigate code blocks that are responsible for more than 25\% of the execution time.

PerfExpert can sort the hotspots according to:

\begin{enumerate}
  \item  \strong{Relevance}: Sorted according to their relevance in \% of the total runtime. The code blocks were the execution time is higher will be presented on top. But those code blocks might already be programmed in a full professional and well-performing manner, and maybe cannot be optimised anymore.
  \item  \strong{Performance}: Sorted according to their performance. The code blocks with the worst performance will be presented on top. These code blocks bears the potential to generate the best performance improvement.
  \item  \strong{Mixed or Impact:} Sorted according to the formula \emph{Relevance $\times$ Performance}. The code blocks with a combination of the worst performance and longest execution times (thus having the highest \emph{impact}) are presented on top. Those are the best candidates for optimisation.
\end{enumerate}

By default, PerfExpert will list its hotspots unsorted.

The test.c program has several slow (bad programmed) functions, i.e., compute1, compute2 and compute3. Explore and compile the test.c program, and explore the 3 results:

\begin{prompt}
%\shellcmd{more test.c}%
%\shellcmd{gcc -g --o test test.c}%
%\shellcmd{perfexpert --order=relevance -c 0.2 ./test}%
%\shellcmd{perfexpert --order=performance -c 0.2 ./test}%
%\shellcmd{perfexpert --order=mixed -c 0.1 ./test}%
\end{prompt}

\subsection{Optimisation Priority}
\label{subsec:Optimization_Priority}

A good optimisation strategy is to run PerfExpert with a high threshold (e.g., 0.3) and first focus on those functions, which consumes most of the computation time. You can then systematically decrease it (0.3 >> 0.2 >> 0.1) to also cover other functions.

Some LCPI categories have a higher potential to speed up your code as compared to other. When optimising, experience has learnt that it is wise to focus on the following order of importance:

\begin{enumerate}
  \item  \strong{Data Access} \& \strong{Data TLB} issues
  \item  \strong{Vectorisation}
  \item  \strong{Others} such as:
  \begin{enumerate}
    \item  Branches
    \item  FP operations
    \item  Instructions (which are unlikely to have a huge performance impact)
  \end{enumerate}
\end{enumerate}

The Working Method to optimise is an iterative process:

\begin{enumerate}
  \item  \strong{Step \#1 -- Measure \& Analyse:} The first step is to run PerfExpert and to understand and interpret the report. This analysis should give you an idea where the bottlenecks are.
  \item  \strong{Step \#2 - Adapt:} With this information, you can (try to) improve the performance by adapting the program. In order to help you in the right directions, some recommendations are given for each hotspot.
  \item  \strong{Step \#3 - Re-test:} Subsequent invocations of PerfExpert shall be run in order to quantify the effect of implementing a change to the code.
\end{enumerate}

You can repeat these steps in an iterative way until you're comfortable with the performance of your program.

\subsection{PerfExpert Recommendations}
\label{subsec:Recommendations}

Based on the characteristics revealed during profiling, PerfExpert is able to provide specific recommendations for changes that could lead to improved performance. These recommendations come from a database of scenarios that correspond to certain patterns of CPU resources utilisation.

These recommendations are chosen from an extensive database of scenarios based upon the profiling results, not necessarily code analysis. Certain recommendations may not be applicable to the code base being analysed, nor may they necessarily improve performance. It is up to the user to understand the code being analysed, and then determine if a given recommendation makes sense.

All the possible recommendations are listed and described in Annex 2: Optimisation. Future version of PerfExpert may extend this list of recommendations.

\section{Optimising the efficiency of a subroutine}
\label{sec:Optimizing_the_efficiency_of_a_subroutine}

\subsection{Optimising Instruction Execution Ratios}
\label{subsec:Optimizing_Instruction_Execution_Ratios}

Optimising the ``Instruction Ratios'' means most of the time increase vectorisation and lower the instruction percentages for ``Data access''. You will use the specific metrics of the Data Access categories (L1, L2, L3, LLC misses and Data TLB), which will be discussed later on.

But we want to show you the effect that ``Accessing Data'' has on the overall performance. We have slightly adapted the matrix multiplication program, so that it uses the values of a[i][k] (which was i+k) and b[k][j] (which was k-j) directly:

Explore the mm1.c program.

\begin{prompt}
%\shellcmd{cat mm1.c}%
...
void compute()
{
 register int i, j, k;
 for (i = 0; i <{} n; i++)
   for (j = 0; j <{} n; j++)
     for (k = 0; k <{} n; k++)
       c[i][j] = (i+k) * (k-j);
}
...
\end{prompt}

And compile and run PerfExpert.

The ``-c'' option in PerfExpert will print colours.

\begin{prompt}
%\shellcmd{gcc -g -o mm1 mm1.c}%
%\shellcmd{perfexpert -c 0.9 ./mm1}%
...
Loop in function compute in mm1.c:12 (99.76%\%% of the total runtime)
============================================================================
The performance of this code section is good!
----------------------------------------------------------------------------
ratio to total instrns    %\%%  0..........25.........50.........75.........100
 - floating point       32.1 ****************
 - data accesses         0.0
...
\end{prompt}

We have now avoided the reference to our matrices ``a'' and ``b'' inside the loop. You see that the percentage of time needed for data accesses was reduced to zero.

\strong{Note:} Of course, your program needs to perform the work. It is not always acceptable to reduce the amount of time spent on data access by just not doing any useful computation!

\subsection{Optimising the Computational Efficiency (GFLOPS)}
\label{subsec:Optimizing_Computational_Efficiancy}

In our Matrix-Multiplication example of \autoref{ch:ch02_profiling}, we noticed that the Matrix Multiplication program was not able to vectorise (Packed was 0\%), all computation were sequential (Scalar was 12.5\%).

But remember that we have compiled the program without any specific \textit{Optimisation} option. The GCC compiler uses level 1 (-O1) as its default ``Optimisation level''!

Now, lets see what happens if we would compile the same program with FULL optimisation (the maximum optimisation level is ``-O3'') and run our profiler again:

\begin{prompt}
%\shellcmd{gcc -g -O3 -o mm mm.c}%
%\shellcmd{perfexpert -c 0.9 mm}%
...
* GFLOPS (%\%% max)        31.9 ****************
.- packed               19.4 **********
 - scalar               12.5 ******
...
\end{prompt}

The program is now able to ``\textit{vectorise}'' much better.

Of course, in the first place we need to design our program in such a way that the compiler is able to vectorise. A situation where we create a read/write data dependency within the loop (e.g., for \ldots\  \{ a[i+1] = a[i] * c[i]; \} ) shall be avoided, as such program logic may not allow the compiler to ``vectorise'' this loop.

\section{Optimising the LCPI Performance Categories}
\label{sec:Optimizing_the_LCPI_Performance_Categories}

\subsection{Category \#1: Data Access}
\label{ch03:subsec:CAT1_Data_Access}

These kinds of issues occur regularly. The \emph{typical issues} are that a high number of CPU cycles are being spent, whilst waiting for LLC load misses to be serviced.

In general, \emph{some possible optimisations} to improve \emph{data access locality} are:

\begin{enumerate}
  \item  to redesign the data structure;
  \item  to reduce data working set size;
  \item  to reorder or interchange the loops, (this works when the loop execution is not important, but one must be aware that Fortran accesses storage by column, C by row);
  \item  to perform loop fusion (Takes two adjacent loops that have same iteration and combines them into a single loop);
  \item  to block and consuming data in chunks that fit in the LLC;
  \item  to explore hardware pre-fetchers. (Frequently, the CPU is not fed fast enough with data from memory, which causes a bottleneck resulting in a CPU waiting for data to process).
\end{enumerate}

We start again from the ``test1.c'' program, which gave bad results on Data Access. We compile and run it again and we will now focus on the recommendations PerfExpert suggests. We use the --r6 option of PerfExpert, so that we get 6 recommendations.

\begin{prompt}
%\shellcmd{more test1.c}%
...
void compute()
{
register int i, j, k;
for (i = 0; i < n; i++)
  for (j = 0; j < n; j++)
    for (k = 0; k < n; k++)
      d[i][j] += a[i][j] + (b[i][k] * c[k][j]);
}
...
%\shellcmd{gcc -g -o test1 test1.c}%
%\shellcmd{perfexpert -c -r6 0.9 ./test1}%
...
Performance assessment  LCPI good.......okay.......fair.......poor.......bad
* overall              2.47 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
* data accesses        2.94 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>+
- L1d hits             0.42 >>>>>>>>
- L2d hits             1.03 >>>>>>>>>>>>>>>>>>>>>
- L3d hits             1.22 >>>>>>>>>>>>>>>>>>>>>>>>
- LLC misses           0.26 >>>>>
...
\#
\# Here is a possible recommendation for this code segment
\#
Description: %\strong{move loop invariant memory accesses out of loop}%
Reason: this optimisation reduces the number of executed (slow) memory accesses
Code example:
loop i {
  a[i] = b[i] * c[j]
}
=====>
temp = c[j];
loop i {
 a[i] = b[i] * temp;
}
...
%\shellcmd{gcc --g -o test1 test1.c}%
%\shellcmd{perfexpert -c -r6 0.9 ./test1}%
\end{prompt}

There is clearly a problem with our data access LCPI. Several recommendations might not look relevant or applicable, and can be ignored. But the recommendation to move loop invariant memory access out of the loop looks promising.

We have implemented this first improvement in the file ``test1\_i1.c''.

\strong{Good Practice:} We keep track of the iterations by adding ``\_i1'', ``\_i2'', etc. to the original filename.

\begin{prompt}
%\shellcmd{more test1\_i1.c}%
...
void compute()
{
register int i, j, k;
double tmp;
for (i = 0; i < n; i++)
  for (j = 0; j < n; j++)
  {
    tmp = a[i][j];
    for (k = 0; k < n; k++)
      d[i][j] += tmp + (b[i][k] * c[k][j]);
  }
}
%\shellcmd{gcc -g -o test1\_i1 test1\_i1.c}%
%\shellcmd{perfexpert -c 0.5 ./test1\_i1}%
\end{prompt}

Check the result, admire the performance improvement and be invited to optimise further.

\subsection{Category \#2: Instruction Access}
\label{ch03:subsec:CAT2_Instruction_Access}

The Instruction accesses measures the cost of fetching instructions from memory. This metric will rarely cause problems in scientific programming. Programs with a high(er) values of this metric are typically huge (in bytes) and may have a lot of function calls (mostly) to statically linked libraries.

Potential solutions are:

\begin{enumerate}
  \item  Link with dynamic libraries.
  \item  Check for dead code / limit your code.
  \item  Keep often used subroutines together.
\end{enumerate}

\subsection{Category \#3: Data TLB}
\label{ch03:subsec:CAT3_Data_TLB}

These kinds of issues also occur regularly and are often related with our problems in category \#1. The likely \emph{problem} is that a significant proportion of cycles are being spent handling first-level data TLB misses.

As with ordinary data caching, \emph{potential solutions} are:

\begin{enumerate}
  \item  to focus on improving data locality;
  \item  to reducing working-set size to reduce DTLB overhead;
  \item  to collocate frequently-used data on the same page;
  \item  to try using larger page sizes for large amounts of frequently-used data.
\end{enumerate}

Let's see what recommendations are suggested for our previous test3 example:

\begin{prompt}
%\shellcmd{more test3.c}%
...
void compute()
{
register int i, j, k;
for (i = 0; i < n; i++)
  for (j = 0; j < n; j++)
    for (k = 0; k < n; k++)
      c[i][j] += a[i][k] * b[k][j];
}
...
\#
\# Here is a possible recommendation for this code segment
\#
Description: %\strong{change the order of loops}%
Reason: this optimisation may improve the memory access pattern and make it more cache and TLB friendly
Code example:
loop i {
  loop j {...}
}
=====>
loop j {
  loop i {...}
}
...
%\shellcmd{gcc -g -o test3 test3.c}%
%\shellcmd{perfexpert -c -r6 0.5 ./test3}%
\end{prompt}

Explore the suggested recommendations.

The recommendation to change \emph{the order of the loops} (as shown above) takes our interest. We have prepared a modified source file for you, called ``test3\_i1.c'', where we implemented this suggested recommendation.

Explore the new file, compile and run:

\begin{prompt}
%\shellcmd{cat test3\_i1.c}%
...
void compute()
{
register int i, j, k;
for (i = 0; i < n; i++)
  for (k = 0; k < n; k++)
    for (j = 0; j < n; j++)
      c[i][j] = a[i][k] * b[k][j];
}
...
%\shellcmd{gcc -g -o test3\_i1 test3\_i1.c}%
%\shellcmd{perfexpert -c 0.6 ./test3\_i1}%
\end{prompt}

And note the improvement, isn't it quite \ldots\  spectacular?

\subsection{Category \#4: Instruction TLB}
\label{subsec:CAT4_Optimizing_Instruction_TLB}

As for Category \#2 (Instruction Access), high values for this metric do rarely occur. You can expect higher values in big programs (i.e., programs which the binary size is in the order of hundred of megabytes).

Potential solutions are:

\begin{enumerate}
  \item  Link with dynamic libraries.
  \item  Check for dead code / limit your code.
  \item  Keep often used subroutines together.
\end{enumerate}

\subsection{Category \#5: Branches}
\label{subsec:CAT5_Optimizing_Branches}

Branch prediction is a feature of the hardware, and not of the compiler.\footnote{Although it is possible to insert compiler-specific annotations (aka pragmas) to tell the compiler which is the most occurring branch result.}

In general, some \emph{possible optimisations} are:

\begin{enumerate}
  \item  Avoid or limit branches in your code (A common branch is a check to see whether you're at the end of a loop);
  \item  To put the most likely branch upfront;
  \item  Apply code patterns to allow the CPU to make better branch prediction;
  \item  Use special tags in your code to force the compiler to a certain prediction (e.g., GCC has macros for encoding branch prediction information, where you can define ``likely'' and ``unlikely'' paths).
\end{enumerate}

\subsection{Category \#6: Floating-Point Operations}
\label{subsec:CAT6_Optimizing_FP_Operations}

These kinds of issues also occur regularly. The likely \emph{problem} is that a significant proportion of cycles are being spent on expensive FP operations (such as division and square root). Of course, as computing is often the core of the program, we cannot always avoid those issues.



In general, some \emph{possible optimisations} are:

\begin{enumerate}
  \item  to redesign the code and check if the number of ``sqrt'' and/or ``division'' calculations can be reduced (by using a pre-calculated value for instance);
  \item  to explore specific arithmetic compiler options (e.g., -ffast-math). ``-ffast-math'' allows the compiler to use faster hardware floating point instructions, at the potential expense of IEEE floating point compliance. If your program doesn't need strict compliance, this setting should theoretically net a boost in floating point performance for ``\textit{free}''.
  \item  To use \textit{invsqrt}: many modern platforms also offer inverse square root, which has the speed approximately the same as \textit{sqrt}, but is often more useful (e.g., by having \textit{invsqrt} you can compute both \textit{sqrt} and \textit{div} with one multiplication for each).
\end{enumerate}

Optimising floating point performance requires the right combination of good programming techniques and compiler optimisations.

But, as the computations are often unavoidable, you'll sometimes find yourself in a position that you are not able to optimise.

Lets see what recommendations are suggested for our test6 example:

\begin{prompt}
%\shellcmd{more test6.c}%
...
void compute()
{
register int i, j, k;
for (i = 0; i < n; i++)
  for (j = 0; j < n; j++) {
    for (k = 0; k < n; k++) {
      d[i][j] += (a[i][k] * b[k][j]) / c[i][j];
    }
  }
}
...
%\shellcmd{gcc -g -o test6 test6.c}%
%\shellcmd{perfexpert -c -r12 0.5 ./test6}%
...
\#
\# Here is a possible recommendation for this code segment
\#
Description: %\strong{Accumulate and then normalize instead of normalizing each element}%
Code example:
loop i {
 x = x + a[i] / b;
}
=====>
loop i {
 x = x + a[i];
}
x = x / b;
...
\end{prompt}

Explore the suggested recommendations.

Use the -r option to select the number of recommendations for optimisation you want for each code section, which is a performance bottleneck. By default, PerfExpert provides you with 3 optimisations.
The recommendation to change (as shown above) takes our interest. We have prepared a modified source file for you, called ``test6\_i1.c'', where we implemented this suggested recommendation.

Explore the new file, compile and run:

\begin{prompt}
%\shellcmd{cat test6\_i1.c}%
...
void compute()
{
register int i, j, k;
for (i = 0; i < n; i++)
  for (j = 0; j < n; j++) {
    for (k = 0; k < n; k++) {
      d[i][j] += a[i][k] * b[k][j];
    }
    d[i][j] /= c[i][j];
  }
}
...
%\shellcmd{gcc -g -o test6\_i1 test6\_i1.c}%
%\shellcmd{perfexpert -c 0.3 ./test6\_i1}%
\end{prompt}

And note the improvement.
