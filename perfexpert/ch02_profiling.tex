\chapter{Profiling}
\label{ch:ch02_profiling}

\renewcommand{\exampledir}{examples/ch02-profiling}

\begin{tip}
Find the source code for the examples in this Chapter in the directory:  ``\tilde/\exampledir''.
\end{tip}

\section{Profiling Strategy}
\label{sec:Profiling_Strategy}

\subsection{Introduction}
\label{subsec:Introduction}

\emph{Profiling} (``program profiling'', ``software profiling'') is a form of dynamic program analysis that measures, for example, the space (memory) or time complexity of a program, the usage of particular instructions, or frequency and duration of function calls. The most common use of profiling information is to aid program optimisation. Non-optimised programs will needlessly consume more computer resources on Stampede (computational, storage, network), which could otherwise reduce the execution time of your application or be made available to other users. Profiling can also help you find many other statistics through which many potential bugs can be spotted and sorted out. All users of Stampede are highly encouraged to profile their applications, before using the computing facilities.

Profiling is achieved by instrumenting either the program source code or its binary executable form using a tool called a \emph{profiler} (or \emph{code profiler}). Profilers, which are also programs themselves, analyse target programs by collecting information on their execution. Since profilers interrupt program execution to collect information, they have a finite resolution in the time measurements, which should be taken with a grain of salt.

Different profiling tools exist. These tools do not compete, but complement each other. The user can use \emph{any} (or multiple) of these profiling tools to check and improve his application.

First we, go to the directory of the examples of Chapter02:

\begin{prompt}
%\shellcmd{cd ~/\exampledir}%
\end{prompt}

\subsection{Profiling with PerfExpert}
\label{subsec:Profiling_with_PerfExpert}

Source-code performance optimisation has four stages:

\begin{enumerate}
  \item  \emph{measurement},
  \item  \emph{analysis and diagnosis of bottlenecks},
  \item  determination of \emph{optimisations}, and
  \item  \emph{rewriting source code}.
\end{enumerate}

Executing these steps for today's complex many processor and heterogeneous computer architectures requires a wide spectrum of knowledge and tools that many application developers would rather not have to learn.

PerfExpert utilises knowledge of architectures and compilers to implement (partial) automation of performance optimisation for multicore chips and heterogeneous nodes of cluster computers. PerfExpert automates the first three performance optimisation stages then implements those optimisations as part of the fourth stage.

Remember to load the appropriate PerfExpert modules:

\iftacc
\begin{prompt}
%\shellcmd{module load papi hpctoolkit perfexpert}%
%\shellcmd{module load GCC}%
\end{prompt}
\fi
\ifvsc
\begin{prompt}
%\shellcmd{module load PerfExpert}%
\end{prompt}
\fi

For our first example, we'll use a simple serial 1000 $\times$ 1000 matrix multiplication program. Check out the program ``mm.c'', compile and run it.

\begin{prompt}
%\shellcmd{cat mm.c}%
%\shellcmd{gcc -o mm mm.c}%
%\shellcmd{./mm}%
\end{prompt}

In order to see the total execution time of the program, you could time it:

\begin{prompt}
%\shellcmd{time ./mm}%
real 0m7.554s
user 0m7.537s
sys 0m0.010s
\end{prompt}

Just adding ``perfexpert'' in front of your normal command line will activate the PerfExpert profiler.  It will start the PerfExpert profiling tool, which will run the executable multiple times and collect the performance metrics.

We also have to define a threshold, which defines the relevance (in \% of runtime) of the code fragments that PerfExpert should investigate. We want to focus our performance checks to those code segments, which consume the majority of the computer resources.  This threshold value should be between 0 and 1. (> 0 and $\leq 1$).  A threshold of 0.3 means that PerfExpert will only focus on those functions, which are responsible for more than 30\% of the runtime.

\begin{prompt}
%\shellcmd{perfexpert 0.3 ./mm}%
[perfexpert] Collecting measurements [hpctoolkit]
[perfexpert]    [1] 8.270767878 seconds (includes measurement overhead)
[perfexpert]    [2] 9.886446243 seconds (includes measurement overhead)
[perfexpert]    [3] 8.258720295 seconds (includes measurement overhead)
[perfexpert] Analysing measurements
--------------------------------------------------------------------------
Total running time for source is 8.66 seconds between all 16 cores
The wall-clock time for source is approximately 0.54 seconds
Module source takes 100.00 of the total runtime
--------------------------------------------------------------------------
Loop in function compute in ~unknown-file~:0 (99.96%\%% of the total runtime)
==========================================================================
ratio to total instrns     %\%% 0.........25.........50.........75.........100
- floating point       93.3 *******************************************
- data accesses        12.0 ******
* GFLOPS (%\%% max)        12.5 ******
- packed                0.0
- scalar               12.5 ******
--------------------------------------------------------------------------
performance assessment LCPI good..........okay..........fair........... poor...........bad
* overall              0.93 >>>>>>>>>>>>>>>>>>>
* data accesses        1.08 >>>>>>>>>>>>>>>>>>>>>>
- L1d hits             0.42 >>>>>>>>
- L2d hits             0.57 >>>>>>>>>>>
- L3d hits             0.09 >>
- LLC misses           0.00
* instruction accesses 0.01
- L1i hits             0.00
- L2i hits             0.00
- L2i misses           0.01
* data TLB             0.57 >>>>>>>>>>>
* instruction TLB      0.00
* branch instructions  0.04 >
- correctly predicted  0.04 >
- mispredicted         0.00
* floating-point instr 1.63 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
- slow FP instr        0.00
- fast FP instr        1.63 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
==========================================================================
[perfexpert] Selecting optimizations
...
\end{prompt}

You notice that:

\begin{enumerate}
  \item  PerfExpert used the HPCToolkit to collect the performance measurements. It is using statistical sampling of timers and hardware performance counters to collect accurate measurements of the program.
  \item  The matrix multiplication program ran 3 times.
  \item  Each run took 8 to 9 seconds to complete. The average execution time was 8.66 seconds, which is a bit more as the execution time without profiling. It shows that profiling causes some extra execution overheads.
  \item  One function was detected in the program that took more than 30\% of the total execution time. The ``compute'' function in an ``~unknown file~'' used 99.96\% of the runtime.
  \item  A detailed analysis of the ``compute'' function was presented. We'll discuss this in depth in the next chapter.
  \item  Three recommendations to improve the speed of the ``compute'' function were suggested.
\end{enumerate}

In this run, PerfExpert was not able to relate each hotspot with its exact location (line number) in source file because this information was not available in the object file. Programs compiled with the ``-g'' option will have debug information in the generated binary files, and PerfExpert will be able to report very specific on the line-number and source-file where a certain hotspot occurred.

\begin{tip}
Always compile your programs with the debug option (-g) on.
\end{tip}

\begin{tip}
If you are using gcc 4.8 or newer, you should use -gdwarf-3 option!
\end{tip}


Apart from the total running time, PerfExpert performance analysis report includes, for each hotspot (i.e., for each subroutine above the threshold):

\begin{enumerate}
  \item  \emph{Efficiency of the Subroutine}
  \begin{enumerate}
    \item Instruction execution ratios (with respect to total instructions);
    \item  Computational efficiency (GFLOPs measurements);
  \end{enumerate}
  \item  \emph{LCPI Performance assessment}
  \begin{enumerate}
    \item Overall performance;
    \item  Values for 6 categories:
    \begin{enumerate}
      \item  Data access
      \item  Data TLB
      \item  Instruction Access
      \item  Instruction TLB
      \item  Branches
      \item  FP instructions
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

\section{Profiling the Efficiency of a subroutine}
\label{sec:Profiling_Efficiency_of_a_subroutine}

\subsection{Instruction Execution Rations}
\label{subsec:Instruction_Execution_Ratios}

The program composition part shows what percentages of the total instructions were \emph{computational} (floating-point instructions) and what percentage were instructions that \emph{accessed data.} Although there are other kinds of instructions, these two groups should be the most relevant on any given scientific application.

\begin{prompt}
ratio to total instrns     %\%% 0.........25.........50.........75.........100
- floating point       93.3 ******************************************
- data accesses        12.0 ******
\end{prompt}

A software program spend its time on 3 types of instructions:

\begin{enumerate}
  \item  \strong{Computations}: Typically, we want our program to be busy with computations (floating-point or other) to solve our scientific problem, and as such a high percentage for FP computations is considered to be good and desired.
  \item  \strong{Data Access:} The computations will need to read input data and will generate output data. Data access (preferably from the fastest lowest cache levels) is needed, but the overall times spend on reading and writing data shall be limited as it is not the core goal of our program. A high percentage of time spend on data access might indicate data access issues.
  \item  \strong{Program logic:} The time spent on other instructions (control flow, testing, loops, branching) can be considered as a ``waste'' of time and should be low. In most applications, this amount is very low, and as such we do not generate metrics for them. The percentages for \emph{Computations}, \emph{Data Access} and \emph{Program Logic}
  should total 100\%\footnote{Some architectures such as Intel Sandy Bridge, Intel Ivy Bridge, and Intel Haswell does not provide accurate values in their performance counter events, specially for the floating-point instructions. For that reason, it is possible to see application with more than 100\% of floating-point instruction, with is obviously not possible.}, so we can easily estimate the \% for this category.
\end{enumerate}

These ratios gives a rough estimate in trying to understand whether optimising the program for either \emph{data accesses} or \emph{floating-point instructions} would have a significant impact on the total running time of the program.

\subsection{Computational Efficiency (GFLOPS)}
\label{subsec:Computational_Efficiancy}

The PerfExpert performance analysis report also shows the GFLOPs rating, which is the number of floating-point operations executed per second in multiples of 10${}^{9}$. The value for the GFLOPS metric is displayed as a percentage of the maximum possible achievable GFLOP rate for that particular machine.

In the previous Matrix-Multiplication example, our GFLOP values were:
\begin{prompt}
* GFLOPS (%\%% max)        12.5 ******
  - packed                0.0
  - scalar               12.5 ******
\end{prompt}

The higher the GFLOPS percentage, the more efficient are computations are done. Although it is rare for real-world programs to match even 50\% of the maximum value, this metric can serve as an estimate of how efficiently the code performs computations.

This performance report gives us also an indication about the ``vectorisation degree'', i.e., the percentage of the computations that were ``vectorised'' ($=$ packed) and run in parallel.

\begin{prompt}
* GFLOPS (%\%% max)        12.5 ******
  - packed                0.0
  - scalar               12.5 ******
\end{prompt}

\begin{enumerate}
  \item  \emph{Packed}: This represents the \% of FP instructions which were vectorised. A value of 0.0 means that no vector FP computations were done. We want this value to be high.
  \item  \emph{Scalar}: This represents the \% of FP instructions which were not vectorised. These computations were executed one by one in serial mode and the value should be low.
\end{enumerate}

You noticed that the Matrix Multiplication program was not able to ``\emph{vectorise}'' (Packed was 0\%) and that all our FP computations were executed sequentially (Scalar was 12.5\%).

\section{Profiling the LCPI Performance Categories}
\label{sec:Profiling_the_LCPI_Performance_Categories}

The next, and major, section of the PerfExpert performance analysis report shows the LCPI values, which is the ratio of cycles spent in the code segment for a specific category, divided by the total number of instructions in the code segment.

\subsection{Overall Performance of the Function}
\label{subsec:Overall_Performance}

The overall value is the ratio of the total cycles taken by the code segment to the total instructions executed in the code segment.~It gives a sense of the average cost in cycles of all instructions in a code segment.

\begin{prompt}
performance assessment  LCPI good........okay.........fair.........poor............bad
* overall               0.93 >>>>>>>>>>>>>>>>>>>
\end{prompt}

The LCPI value is a good indicator of the cost arising from instructions of the specific category.

\emph{Hence, the higher the LCPI, the slower the program.}

Generally, a value of 0.5 or lower for an LCPI is considered to be good. However, it is only necessary to look at the ratings (good, okay, \ldots, bad).

The rest of the report maps this overall LCPI, into the six constituent categories: \emph{data accesses}, \emph{instruction accesses}, \emph{data TLB accesses}, \emph{instruction TLB accesses}, \emph{branches} and \emph{floating point computations}. Without getting into the details of instruction operation on Intel and AMD chips, one can say that these six categories record performance in non-overlapping ways. That is, they roughly represent six separate categories of performance for any application.

Individual machine instructions or events can vary between architectures, and are generally not common knowledge. In order to cope with this, PerfExpert groups instructions into categories and displays LCPI values in terms of these categories. There is an architecture-dependent mapping between specific instructions and PerfExpert categories behind the scenes, but a PerfExpert user does not need to be aware of these details.

\strong{Remarks}

In each case, the classification (data access, instruction access, data TLB, etc.) is shown so that it is easy to understand which category is responsible for the performance slowdown. For instance if the overall LCPI is poor and the data access LCPI is high, then you should concentrate on improving the access to program variables and memory. Additional LCPI details help in relating performance numbers to the process architecture.

While high LCPI values may indicate costly and inefficient use of certain computing resources, they do not specify what should be done to make the code segment more efficient. PerfExpert uses a set of rules, which map the LCPI metric values to one or more specific modifications (recommendations or suggestions for optimisation) to the code segment to correct the inefficiency in the code segment. We will discuss this recommendation system later in~\autoref{ch:ch04_multi_core_multi_node_profiling}.

Before we can enhance or speed-up your own program, you must fully be able to interpret the performance report. We will use some lab exercises to build up this competence.

We will change the matrix multiplication program in an ``evil'' manner, to deliberately generate bad performance reports. The goal is to let the user understand the potential causes of bad performance and to teach him to interpret the performance report.

\subsection{Category \#1: Data Access}
\label{ch02:subsec:CAT1_Data_Access}

The Data accesses counts the LCPI arising from accesses to memory for program variables. Measures the cost of memory and cache accesses for variables.

High LCPI values in this category generally indicate poor memory access patterns. L1 and L2 cache hits and LLC misses are included in this category.

\begin{prompt}
* data accesses          1.08 >>>>>>>>>>>>>>>>>>>>>>
  - L1d hits             0.42 >>>>>>>>
  - L2d hits             0.57 >>>>>>>>>>>
  - L3d hits             0.09 >>
  - LLC misses           0.00
\end{prompt}

We will start from the previous matrix multiplication program and generate a program with inefficient data-access. Instead of looping correctly through the matrices (``row from matrix a'' $\times$ ``column from matrix b''), we will now access both matrices ``a'' and ``b'' in a vertical manner (column by column). \emph{We do of course not aim to perform a correct matrix-multiplication}, as we are not interested in the results. We just want to show the effect of accessing data in a scattered (non-linear) way and causing \emph{non-optimal data access locality}.

Explore the test1.c program.

\begin{prompt}
%\shellcmd{cat test1.c}%
\end{prompt}
\examplecode{c}{test1.c}

And compile and run PerfExpert.

We will now compile with the ``-g'' option, which will include debug information in the generated binary files. With this debug information, PerfExpert will be able to relate each function with a location (line number) in a specific file. A Makefile is provided in the examples directory, so as from now, one could also use ``make'' to compile all the programs at once.

The ``-c'' option in PerfExpert will print colours.

\begin{prompt}
%\shellcmd{gcc -g -o test1 test1.c}%
%\shellcmd{perfexpert -c 0.9 ./test1}%
[perfexpert] Collecting measurements [hpctoolkit]
[perfexpert]    [1] 20.045568079 seconds (includes measurement overhead)
[perfexpert]    [2] 21.401645102 seconds (includes measurement overhead)
[perfexpert]    [3] 16.191897395 seconds (includes measurement overhead)
[perfexpert] Analysing measurements
----------------------------------------------------------------------------
Total running time for test1 is 22.94 seconds between all 16 cores
The wall-clock time for test1 is approximately 1.43 seconds
Module test1 takes 100.00%\%% of the total runtime
----------------------------------------------------------------------------
Loop in function compute in test1.c:21 (99.98%\%% of the total runtime)
============================================================================
ratio to total instrns    %\%%  0..........25.........50.........75.........100
 - floating point      100.0 ***********************************************
 - data accesses        12.0 ******
* GFLOPS (%\%% max)        10.2 *****
 - packed                0.0
 - scalar               10.2 *****
----------------------------------------------------------------------------
performance assessment  LCPI good..........okay........s.fair........poor..............bad
%{color{red}* overall               2.47 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>}%
* data accesses         2.94 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
 - L1d hits             0.42 >>>>>>>>
 - L2d hits             1.03 >>>>>>>>>>>>>>>>>>>>>
 - L3d hits             1.22 >>>>>>>>>>>>>>>>>>>>>>>>
 - LLC misses           0.26 >>>>>
* instruction accesses  0.02
 - L1i hits              0.00
 - L2i hits             0.00
 - L2i misses           0.02
* data TLB              1.43 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>
* instruction TLB       0.00
* branch instructions   0.04 >
 - correctly predicted  0.04 >
 - mispredicted         0.00
* floating-point instr  3.50 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>+
 - slow FP instr        0.00
 - fast FP instr        3.50 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>+
============================================================================
\end{prompt}

We notice that:

\begin{enumerate}
  \item The average \emph{execution time} more than doubles, from 8 seconds to 20 seconds.
  \item The \emph{overall} performance is bad. (The line in red)
  \item And we see a lot of \emph{data access} spread over the L1, L2 and L3 data spaces, and a lot of LCC (last Level Cache) misses. It is disastrous.
  \item The \emph{data TLB} is high again, which is related to non-optimal data access.
  \item The \emph{instruction TLB and instruction accesses} are good. This means that our (part of the) program is loaded in the instruction cache once, and remains there. This behaviour is expected as we have a very small program.
  \item There are little \emph{branch instructions}, all of which have been correctly predicted. There are no complex \emph{branch} instructions (if, else, switch or goto statements) in our small program.
  \item We have quite a lot of \emph{floating point instructions}. This was expected, as it was the core functionality of our program. We only have \emph{fast} Floating Point instructions, as we did not program many ``expensive'' (slow) divisions and/or square-roots calculations.
\end{enumerate}

This \emph{analysis} indicates a \emph{performance problem related to data access} in the ``compute'' function. This function is responsible for 99.98\% of the total runtime, so an improvement might heavily impact to total runtime.

\begin{tip}
It is interesting to run this example with full (-O3) and without (-O0) compiler optimisation and check the difference in runtime.
\end{tip}

\begin{prompt}
%\shellcmd{gcc -O3 -g -o test1 test1.c}%
%\shellcmd{perfexpert -c 0.8 ./test1}%
%\shellcmd{gcc -O0 -g -o test1 test1.c}%
%\shellcmd{perfexpert -c 0.7 ./test1}%
\end{prompt}

You will notice that our performance issue is design-related, and thus the compiler is not (or hardly) able to optimise our program.

\subsection{Category \#2: Instruction Access}
\label{ch02:subsec:CAT2_Instruction_Access}

The Instruction accesses count the LCPI arising from memory accesses for code (functions and loops) and it measures the cost of fetching instructions from memory. Tight, small, non-branchy loops tend to have the best LCPI values.

\begin{prompt}
* instruction accesses  0.01
 - L1i hits             0.00
 - L2i hits             0.00
 - L2i misses           0.01
\end{prompt}

This metric will rarely cause problems in scientific programming. Programs with a high(er) values of this metric are typically huge (in bytes) and may have a lot of function calls (mostly) to statically linked libraries. It is however difficult to simulate this behaviour in a small comprehensive program.

\subsection{Category \#3: Data TLB}
\label{ch02:subsec:CAT3_Data_TLB}

The Data Translation Lookaside Buffer (TLB) is used for mapping physical memory address to pages in a virtual memory subsystem. Typically, it can only contain a mapping for a small subset of pages. Access to pages outside of this buffer requires an update of the buffer, as well as other memory accounting, and it is expensive. Typically, this happens when memory accesses are not close to one another, such as random access or non-unit stride.

\begin{prompt}
* data TLB              0.57 >>>>>>>>>>>
\end{prompt}

High LCPI values indicate lots of TLB updates, and imply poor memory locality. The Data TLB provides an approximate measure of penalty arising from long strides in accesses or irregularity of accesses.

Probably the most common cause for TLB misses is due to memory being accesses with large, often constant, distances between them. The distance between memory accesses is usually referred to as a stride. Unitary stride is used to describe memory accesses that are sequential, i.e., the next element is exactly one element away. A stride of 5 refers to a memory access pattern where every 5${}^{th}$ element in a list is accessed, skipping over 4 elements at a time. Multi-dimensional arrays have inherently long strides, depending on how they are accessed.

Explore the test3.c program.

\begin{prompt}
%\shellcmd{cat test3.c}%
...
for (j = 0; j <{} MAX_J; j++)
 for (i = 0; i <{} MAX_I; i++)
   a[i*STRIDE+j] += 2;
...
\end{prompt}

And compile and run PerfExpert.

We do not want to see the recommendations yet, so we add the ``-r0'' option.  (r0 $=$ give me zero recommendations)

\begin{prompt}
%\shellcmd{gcc -g -o test3 test3.c}%
%\shellcmd{perfexpert --r0 -c 0.9 ./test3}%
[perfexpert] Collecting measurements [hpctoolkit]
[perfexpert]    [1] 6.756021233 seconds (includes measurement overhead)
[perfexpert]    [2] 8.094997327 seconds (includes measurement overhead)
[perfexpert]    [3] 6.352170922 seconds (includes measurement overhead)
[perfexpert] Analysing measurements
----------------------------------------------------------------------------
Total running time for test3 is 5.37 seconds between all 16 cores
The wall-clock time for test3 is approximately 0.34 seconds
Module test3 takes 100.00%\%% of the total runtime
----------------------------------------------------------------------------
Loop in function compute in test3.c:0 (95.24%\%% of the total runtime)
============================================================================
ratio to total instrns    %\%%  0..........25.........50.........75.........100
 - floating point      100.0 ***********************************************
 - data accesses         7.3 ****
* GFLOPS (%\%% max)        12.5 ******
 - packed                0.0
 - scalar               12.5 ******
----------------------------------------------------------------------------
performance assessment  LCPI good.......okay.......fair.......poor.......bad
%{\color{red}* overall               3.66 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>+}%
* data accesses        19.48 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>+
 - L1d hits             0.26 >>>>>
 - L2d hits             0.94 >>>>>>>>>>>>>>>>>>>
 - L3d hits             0.08 >>
 - LLC misses          18.21 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>+
* instruction accesses  0.00
 - L1i hits             0.00
 - L2i hits             0.00
 - L2i misses           0.00
* data TLB              1.03 >>>>>>>>>>>>>>>>>>>>>
* instruction TLB       0.00
* branch instructions   0.07 >
 - correctly predicted  0.07 >
 - mispredicted         0.00
* floating-point instr  6.36 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
 - slow FP instr        0.00
 - fast FP instr        6.36 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
\end{prompt}

============================================================================

As aforementioned, mappings between virtual and physical memory are facilitated by a page table, which is kept in memory. To minimise references to this table, recently-used portions of the page table are cached in a hierarchy of ``translation lookaside buffers'', or TLBs, which are consulted on every virtual address translation. As with data caches, the farther a request has to go to be satisfied, the worse the performance impact. These metric estimates the performance penalty paid for missing the first-level data TLB (DTLB) that includes hitting in the second-level data TLB (STLB) as well as performing a hardware page walks on an STLB miss.

We notice that:

\begin{enumerate}
  \item The \emph{overall} performance is bad. (The line in red)
  \item And we see many \emph{data access} spread over the L1, L2 and L3 data spaces, and a lot of LCC (last Level Cache) misses.
  \item We see a lot of data TLB misses too, which are very related to the other cache misses.
  \item We see high LCPI values for floating-point instructions, however, as floating-point instructions usually requires data access, the high LCPI values in this case are mostly a consequence of the high data access LPCI values.
\end{enumerate}

\subsection{Category \#4: Instruction TLB}
\label{subsec:CAT4_Instruction_TLB}

The Instruction TLB reflects cost of fetching instructions due to irregular accesses. High LCPI values here indicate poor memory locality when fetching instructions.

\begin{prompt}
* instruction TLB       0.00
\end{prompt}

As for Category \#2 (Instruction Access), high values for this metric do rarely occur. You can expect higher values in big programs (i.e., programs which the binary size is in the order of hundred of megabytes).

\subsection{Category \#5: Branch Instructions}
\label{subsec:CAT5_Branch_Instructions}

The Branch Instructions counts cost of jumps (i.e., if statements, loop conditions, etc.). This measures the penalty from jumps as a result of conditionals or loops. High LCPI values may indicate branchy, unpredictable code. Branch LCPIs can be divided into LCPIs from correctly predicted and from mispredicted branch instructions.

\begin{prompt}
* branch instructions   0.04 >
 - correctly predicted  0.04 >
 - mispredicted         0.00
\end{prompt}

The performance of a branch statement (e.g., if-elseif-else, switch) depends on whether its condition has a predictable pattern. If the condition is mostly true or mostly false, the branch predictor logic in the processor will pick up the pattern. On the other hand, if the pattern is unpredictable, the execution of such branch-statement will be much more expensive.

In modern CPUs, the processor tries and fetches instructions from memory before they are needed as otherwise the CPU has to wait for the instruction (stall). This is called pre-fetching and the instructions are held in an instruction pipeline. Of course, the flow of the application is not always known at that moment, e.g., in case of a branch. Therefore, a specific digital circuit was designed (called a \emph{branch predictor}) that tries to guess which way a branch (e.g., an if-then-else structure) will go before this is known for sure. The purpose of the branch predictor is to improve the instruction pipeline throughput. Branch predictors play a critical role in achieving high effective performance in many modern pipelined microprocessor architectures.

Branches (i.e., conditional jumps) present a difficulty for the processor pipeline. After fetching a branch instruction, the processor needs to fetch the next instruction. But, there are \emph{multiple possible} ``next'' instructions! The processor won't be sure which instruction is the next one until the branching instruction makes it to the end of the pipeline.

Instead of stalling the pipeline until the branching instruction is fully executed, modern processors attempt to \emph{predict} whether the jump will or will not be taken. Then, the processor can fetch the instruction that it thinks is the next one. If the prediction turns out wrong, the processor will simply discard the partially executed instructions that are in the pipeline and the pipeline starts over with the correct branch,
incurring a delay.\footnote{There is another technique known as ``speculative execution'' which actually executes more than one branch possibility, however, this tutorial is not intended to cover this technique.}

The \emph{test5a} program will create a large array of randomly obtained values of 0's, 1's, 2's and 3's. We loop through the array and branch for each value, doing time-wasting work. As all the values were randomly obtained, we expect that the computer is not able to predict (find a pattern) in the branches.

Explore the test5a.c program:

\begin{prompt}
%\shellcmd{cat test5a.c}%
\end{prompt}
\examplecode{c}{test5a.c}

In our example, all the values in the branch are completely randomly obtained, so the computer has a chance of 1 out of 4 to make a correct prediction.

Lets compile and run the example:

\begin{prompt}
%\shellcmd{gcc -g -o test5a test5a.c}%
%\shellcmd{perfexpert --c 0.1 ./test5a}%
[perfexpert] Collecting measurements [hpctoolkit]
[perfexpert]    [1] 8.639570165 seconds (includes measurement overhead)
[perfexpert]    [2] 8.567133423 seconds (includes measurement overhead)
[perfexpert]    [3] 8.294661367 seconds (includes measurement overhead)
[perfexpert] Analysing measurements
----------------------------------------------------------------------------
Total running time for test5a is 9.06 seconds between all 16 cores
The wall-clock time for test5a is approximately 0.57 seconds
Module test5a takes 64.77%\%% of the total runtime
Module libc-2.12.so takes 35.23%\%% of the total runtime
----------------------------------------------------------------------------
Loop in function compute in test5a.c:12 (49.20%\%% of the total runtime)
============================================================================
ratio to total instrns    %\%%  0..........25.........50.........75.........100
 - floating point      100.0 ***********************************************
 - data accesses        46.8 ***********************
* GFLOPS (%\%% max)       12.5 ******
 - packed                0.0
 - scalar               12.5 ******
-------------------------------------------------------------------------------
performance assessment  LCPI good.......okay.......fair.......poor.......bad
%{\color{red}* overall               1.88 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>}%
* data accesses         1.67 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
 - L1d hits             1.64 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
 - L2d hits             0.01
 - L3d hits             0.00
 - LLC misses           0.02
* instruction accesses  0.00
 - L1i hits             0.00
 - L2i hits             0.00
 - L2i misses           0.00
* data TLB              0.00
* instruction TLB       0.00
* branch instructions   1.14 >>>>>>>>>>>>>>>>>>>>>>>
 - correctly predicted  0.25 >>>>>
 - mispredicted         0.89 >>>>>>>>>>>>>>>>>>
* floating-point instr  3.27 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
 - slow FP instr        0.00
 - fast FP instr        3.27 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
\end{prompt}

We notice that:

\begin{enumerate}
  \item  The \emph{overall} performance is poor. (The line in red)
  \item  Data access, instruction access, data TLB and Instruction TLB are all fine.
  \item  As expected, we see a lot of mispredicted branches. Around 25\% is correctly predicted while 75\% is mispredicted.
\end{enumerate}

The time that is wasted in the case of a branch misprediction is equal to the number of stages in the pipeline from the fetch stage to the execute stage. Modern microprocessors tend to have quite long pipelines so that the misprediction delay is between 10 and 20 clock cycles. The longer the pipeline the greater the need for a good branch prediction.

In order to compare the effect of the branch predictor when it is able to provide good predictions, we also prepared a similar program (i.e., test5b.c), but now with completely predictable branches. We alternatively put the values 0, 1, 2 and 3 ($=$ i\%4) in the array.

Explore, compile, run the example and check the results, paying particular attention to the overall execution time and the mispredicted branches.

\begin{prompt}
%\shellcmd{cat test5b.c}%
...
 for(i = 0; i <{} ARRAY\_LEN; ++i)
   a[i] = i PERCENTAGE-SIGN 4;
...
%\shellcmd{gcc -g -o test5b test5b.c}%
%\shellcmd{perfexpert --c 0.3 ./test5b}%
...
[perfexpert] Collecting measurements [hpctoolkit]
[perfexpert]    [1] 6.687911876 seconds (includes measurement overhead)
[perfexpert]    [2] 6.677188251 seconds (includes measurement overhead)
[perfexpert]    [3] 6.660273106 seconds (includes measurement overhead)
[perfexpert] Analysing measurements
* branch instructions   0.17 >>>
 - correctly predicted  0.17 >>>
 - mispredicted         0.00
...
\end{prompt}

The branch predictor in the processor has detected the pattern and is now able to completely predict the branches, which has its effect on the execution time of the program.

How come? The first time a conditional jump instruction is encountered, there is not much information to base a prediction on. But the branch predictor keeps records of whether branches are taken or not taken. When it encounters a conditional jump that has been seen several times before then it can base the prediction on the history. The branch predictor may, for example, recognise that the conditional jump is taken more often than not, or that it is taken every second time. In our case, the branch predictor was able to follow the sequential 1-2-3-4 logic.

\subsection{Category \#6: Floating Point instructions}
\label{subsec:CAT6_FP_Instructions}

The floating-point instructions count LCPI from executing computational (floating-point) instructions. High LCPI values can indicate non-vectorised floating-point operations, expensive operations like calculating square roots, and most commonly stalls caused by poor data access.

\begin{prompt}
* floating-point instr  1.63 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
 - slow FP instr        0.00
 - fast FP instr        1.63 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
\end{prompt}

For floating-point instructions, the division is based on floating-point instructions that take few cycles to execute (e.g., add, subtract and multiply instructions) and on floating-point instructions that take longer to execute (e.g., divide and square-root instructions).

We will start again from the previous matrix multiplication program and generate a program with ``expensive'' slow floating-point instructions. We have added a number of square-root (``sqrt'') and division (``/'') operations in the computation. We just want to show the effect of these ``slow'' instructions on the overall performance.

Explore the test6.c program.

\begin{prompt}
%\shellcmd{cat test6.c}%
\end{prompt}
\examplecode{c}{test6.c}

We have to compile with the ``--lm'' option, to make sure that the mathematical library is linked into our executable.

\begin{prompt}
%\shellcmd{gcc -lm -g -o test6 test6.c}%
%\shellcmd{perfexpert -c 0.5 ./test6}%
[perfexpert] Collecting measurements [hpctoolkit]
[perfexpert]    [1] 38.660883281 seconds (includes measurement overhead)
[perfexpert]    [2] 36.109102082 seconds (includes measurement overhead)
[perfexpert]    [3] 31.387831572 seconds (includes measurement overhead)
[perfexpert] Analysing measurements
----------------------------------------------------------------------------
Total running time for test6 is 28.57 seconds between all 16 cores
The wall-clock time for test6 is approximately 1.79 seconds
Module test6 takes 71.60%\%% of the total runtime
Module libm-2.12.so takes 27.86%\%% of the total runtime
Module libc-2.12.so takes 0.54%\%% of the total runtime
----------------------------------------------------------------------------
Loop in function compute in test6.c:17 (71.47%\%% of the total runtime)
============================================================================
ratio to total instrns    %\%%  0.........25.........50.........75..........100
 - floating point       63.7 ********************************
 - data accesses        31.2 ****************
* GFLOPS (%\%% max)         9.5 *****
 - packed                0.0
 - scalar                9.5 *****
----------------------------------------------------------------------------
performance assessment  LCPI good.......okay.......fair.......poor.......bad
%{\color{yellow}* overall               0.84 >>>>>>>>>>>>>>>>>}%
* data accesses         1.38 >>>>>>>>>>>>>>>>>>>>>>>>>>>>
 - L1d hits             1.09 >>>>>>>>>>>>>>>>>>>>>>
 - L2d hits             0.03 >
 - L3d hits             0.00
 - LLC misses           0.26 >>>>>
* instruction accesses  0.01
 - L1i hits             0.00
 - L2i hits             0.00
 - L2i misses           0.01
* data TLB              0.14 >>>
* instruction TLB       0.00
* branch instructions   0.35 >>>>>>>
 - correctly predicted  0.23 >>>>>
 - mispredicted         0.12 >>
* floating-point instr  1.73 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
 - slow FP instr        0.62 >>>>>>>>>>>>
 - fast FP instr        1.11 >>>>>>>>>>>>>>>>>>>>>>
============================================================================
\end{prompt}

We notice in this example that:
\begin{enumerate}
  \item  The average \emph{execution time} more than quadruples, from 8 seconds to 33 seconds.
  \item  The square-root function (``sqrt'') in the mathematical library (libm) consumes about 27\% of the total runtime.
  \item  The \emph{overall} performance is okay to fair. (The line in yellow).
  \item  Most \emph{data access} occurs in the L1 cache (which is perfect) and we only have limited LCC (last Level Cache) misses.
  \item  The \emph{data TLB} is low.
  \item  The \emph{instruction TLB and instruction accesses} are perfect.
  \item  Also the \emph{branch instructions} look okay. Although we do not written branches in our code, loop control flow generates a number of intrinsic branches.
  \item  But we have a lot of \emph{floating point instructions}. This was expected, as it was the core functionality of our program. We have a considerable amount of \emph{slow} Floating Point instructions (division and square-root), which will be causing the slow performance.
\end{enumerate}

This \emph{analysis} indicates a \emph{performance} issue related to the \emph{slow floating-point calculations}. Just the square root operations (calls to libm) are responsible for 28\% of the runtime, so an improvement might heavily impact to total runtime.

As a general rule of thumb: Both floating-point division and square root are considered as slow operations. Addition, subtraction and multiplication are fast FP operations. Square root can be expect to be approximately the same speed or somewhat slower (i.e., approx. 1$\times$ - 2$\times$ lower performance) compared to a division.

