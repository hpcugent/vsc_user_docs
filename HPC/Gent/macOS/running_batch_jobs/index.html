
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.3.9">
    
    
      
        <title>Running batch jobs - VSC User Documentation - Gent (macOS)</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.1d29e8d0.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.cbb835fc.min.css">
        
      
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#running-batch-jobs" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="VSC User Documentation - Gent (macOS)" class="md-header__button md-logo" aria-label="VSC User Documentation - Gent (macOS)" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            VSC User Documentation - Gent (macOS)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Running batch jobs
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="VSC User Documentation - Gent (macOS)" class="md-nav__button md-logo" aria-label="VSC User Documentation - Gent (macOS)" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    VSC User Documentation - Gent (macOS)
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        Introduction to HPC
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../account/" class="md-nav__link">
        Getting an HPC Account
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../connecting/" class="md-nav__link">
        Connecting to the HPC infrastructure
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Running batch jobs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Running batch jobs
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#modules" class="md-nav__link">
    Modules
  </a>
  
    <nav class="md-nav" aria-label="Modules">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#environment-variables" class="md-nav__link">
    Environment Variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-module-command" class="md-nav__link">
    The module command
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#available-modules" class="md-nav__link">
    Available modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#organisation-of-modules-in-toolchains" class="md-nav__link">
    Organisation of modules in toolchains
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loading-and-unloading-modules" class="md-nav__link">
    Loading and unloading modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#purging-all-modules" class="md-nav__link">
    Purging all modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-explicit-version-numbers" class="md-nav__link">
    Using explicit version numbers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#search-for-modules" class="md-nav__link">
    Search for modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get-detailed-info" class="md-nav__link">
    Get detailed info
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save-and-load-collections-of-modules" class="md-nav__link">
    Save and load collections of modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#getting-module-details" class="md-nav__link">
    Getting module details
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#getting-system-information-about-the-hpc-infrastructure" class="md-nav__link">
    Getting system information about the HPC infrastructure
  </a>
  
    <nav class="md-nav" aria-label="Getting system information about the HPC infrastructure">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#checking-the-general-status-of-the-hpc-infrastructure" class="md-nav__link">
    Checking the general status of the HPC infrastructure
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#getting-cluster-state" class="md-nav__link">
    Getting cluster state
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#defining-and-submitting-your-job" class="md-nav__link">
    Defining and submitting your job
  </a>
  
    <nav class="md-nav" aria-label="Defining and submitting your job">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#when-will-my-job-start" class="md-nav__link">
    When will my job start?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#specifying-the-cluster-on-which-to-run" class="md-nav__link">
    Specifying the cluster on which to run
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#monitoring-and-managing-your-jobs" class="md-nav__link">
    Monitoring and managing your job(s)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#examining-the-queue" class="md-nav__link">
    Examining the queue
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-job-requirements" class="md-nav__link">
    Specifying job requirements
  </a>
  
    <nav class="md-nav" aria-label="Specifying job requirements">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generic-resource-requirements" class="md-nav__link">
    Generic resource requirements
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-output-and-error-files" class="md-nav__link">
    Job output and error files
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#e-mail-notifications" class="md-nav__link">
    E-mail notifications
  </a>
  
    <nav class="md-nav" aria-label="E-mail notifications">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generate-your-own-e-mail-notifications" class="md-nav__link">
    Generate your own e-mail notifications
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#running-a-job-after-another-job" class="md-nav__link">
    Running a job after another job
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../running_interactive_jobs/" class="md-nav__link">
        Running interactive jobs
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../running_jobs_with_input_output_data/" class="md-nav__link">
        Running jobs with input/output data
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../multi_core_jobs/" class="md-nav__link">
        Multi core jobs/Parallel Computing
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../web_portal/" class="md-nav__link">
        Using the HPC-UGent web portal
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../xdmod/" class="md-nav__link">
        XDMoD portal
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../troubleshooting/" class="md-nav__link">
        Troubleshooting
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../sites/hpc_policies/" class="md-nav__link">
        HPC Policies
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../FAQ/" class="md-nav__link">
        Frequently Asked Questions
      </a>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_13" type="checkbox" id="__nav_13" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_13">
          Advanced topics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Advanced topics" data-md-level="1">
        <label class="md-nav__title" for="__nav_13">
          <span class="md-nav__icon md-icon"></span>
          Advanced topics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../fine_tuning_job_specifications/" class="md-nav__link">
        Fine-tuning Job Specifications
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../multi_job_submission/" class="md-nav__link">
        Multi-job submission
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../compiling_your_software/" class="md-nav__link">
        Compiling and testing your software on the HPC
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../program_examples/" class="md-nav__link">
        Program examples
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../jobscript_examples/" class="md-nav__link">
        Job script examples
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../best_practices/" class="md-nav__link">
        Best Practices
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../VNC/" class="md-nav__link">
        Graphical applications with VNC
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../x2go/" class="md-nav__link">
        Graphical applications with X2Go
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../gpu_gent/" class="md-nav__link">
        HPC-UGent GPU clusters
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../interactive_gent/" class="md-nav__link">
        HPC-UGent interactive and debug cluster
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../crontab_gent/" class="md-nav__link">
        Cron scripts
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_14" type="checkbox" id="__nav_14" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_14">
          Software-specific Best Practices
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Software-specific Best Practices" data-md-level="1">
        <label class="md-nav__title" for="__nav_14">
          <span class="md-nav__icon md-icon"></span>
          Software-specific Best Practices
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../apptainer/" class="md-nav__link">
        Apptainer/Singularity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../easybuild/" class="md-nav__link">
        EasyBuild
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../MATLAB/" class="md-nav__link">
        MATLAB
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../mympirun/" class="md-nav__link">
        mympirun
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../openFOAM/" class="md-nav__link">
        OpenFOAM
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../scoop/" class="md-nav__link">
        SCOOP
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle="__nav_15" type="checkbox" id="__nav_15" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_15">
          Appendices
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Appendices" data-md-level="1">
        <label class="md-nav__title" for="__nav_15">
          <span class="md-nav__icon md-icon"></span>
          Appendices
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../quick_reference_guide/" class="md-nav__link">
        Appendix A - HPC Quick Reference Guide
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../torque_options/" class="md-nav__link">
        Appendix B - TORQUE options
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../useful_linux_commands/" class="md-nav__link">
        Appendix C - Useful Linux Commands
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#modules" class="md-nav__link">
    Modules
  </a>
  
    <nav class="md-nav" aria-label="Modules">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#environment-variables" class="md-nav__link">
    Environment Variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-module-command" class="md-nav__link">
    The module command
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#available-modules" class="md-nav__link">
    Available modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#organisation-of-modules-in-toolchains" class="md-nav__link">
    Organisation of modules in toolchains
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loading-and-unloading-modules" class="md-nav__link">
    Loading and unloading modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#purging-all-modules" class="md-nav__link">
    Purging all modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-explicit-version-numbers" class="md-nav__link">
    Using explicit version numbers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#search-for-modules" class="md-nav__link">
    Search for modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get-detailed-info" class="md-nav__link">
    Get detailed info
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save-and-load-collections-of-modules" class="md-nav__link">
    Save and load collections of modules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#getting-module-details" class="md-nav__link">
    Getting module details
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#getting-system-information-about-the-hpc-infrastructure" class="md-nav__link">
    Getting system information about the HPC infrastructure
  </a>
  
    <nav class="md-nav" aria-label="Getting system information about the HPC infrastructure">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#checking-the-general-status-of-the-hpc-infrastructure" class="md-nav__link">
    Checking the general status of the HPC infrastructure
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#getting-cluster-state" class="md-nav__link">
    Getting cluster state
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#defining-and-submitting-your-job" class="md-nav__link">
    Defining and submitting your job
  </a>
  
    <nav class="md-nav" aria-label="Defining and submitting your job">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#when-will-my-job-start" class="md-nav__link">
    When will my job start?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#specifying-the-cluster-on-which-to-run" class="md-nav__link">
    Specifying the cluster on which to run
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#monitoring-and-managing-your-jobs" class="md-nav__link">
    Monitoring and managing your job(s)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#examining-the-queue" class="md-nav__link">
    Examining the queue
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-job-requirements" class="md-nav__link">
    Specifying job requirements
  </a>
  
    <nav class="md-nav" aria-label="Specifying job requirements">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generic-resource-requirements" class="md-nav__link">
    Generic resource requirements
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-output-and-error-files" class="md-nav__link">
    Job output and error files
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#e-mail-notifications" class="md-nav__link">
    E-mail notifications
  </a>
  
    <nav class="md-nav" aria-label="E-mail notifications">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generate-your-own-e-mail-notifications" class="md-nav__link">
    Generate your own e-mail notifications
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#running-a-job-after-another-job" class="md-nav__link">
    Running a job after another job
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="running-batch-jobs">Running batch jobs<a class="headerlink" href="#running-batch-jobs" title="Permanent link">#</a></h1>
<p>In order to have access to the compute nodes of a <abbr title="A group of compute nodes.">cluster</abbr>, you have to
use the job system. The system software that handles your batch jobs
consists of two pieces: the <abbr title="PBS/TORQUE queues, or &quot;classes&quot; as Moab refers to them, represent groups of computing resources with specific parameters. A queue with a 12-hour runtime or &quot;walltime&quot; would allow jobs requesting 12 hours or less to use this queue.">queue</abbr>- and resource manager <strong>TORQUE</strong> and the
scheduler <strong><abbr title="Moab is a job scheduler, which allocates resources for jobs that are requesting resources.">Moab</abbr></strong>. Together, TORQUE and <abbr title="Moab is a job scheduler, which allocates resources for jobs that are requesting resources.">Moab</abbr> provide a suite of commands for
submitting jobs, altering some of the properties of waiting jobs (such
as reordering or deleting them), monitoring their progress and killing
ones that are having problems or are no longer needed. Only the most
commonly used commands are mentioned here.</p>
<p><img alt="image" src="../img/ch4-pbs-overview.png" style="display: block; margin: 0 auto" /></p>
<p>When you connect to the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr>, you have access to (one of) the <strong>login nodes</strong> of the
<abbr title="A group of compute nodes.">cluster</abbr>. There you can prepare the work you want to get done on the
<abbr title="A group of compute nodes.">cluster</abbr> by, e.g., installing or compiling programs, setting up data
sets, etc. The computations however, should not be performed on this
<abbr title="On HPC clusters, login nodes serve multiple functions. From a login node you can submit and monitor batch jobs, analyse computational results, run editors, plots, debuggers, compilers, do housekeeping chores as adjust shell settings, copy files and in general manage your account. You connect to these servers when want to start working on the HPC Infrastructure.">login node</abbr>. The actual work is done on the <abbr title="A group of compute nodes.">cluster</abbr>'s <strong>compute nodes</strong>. Each <abbr title="The computational units on which batch or interactive jobs are processed. A compute node is pretty much comparable to a single personal computer. It contains one or more sockets, each holding a single CPU. Some nodes also contain one or more GPGPUs. The compute node is equipped with memory (RAM) that is accessible by all its CPUs.">compute node</abbr> contains a number of <abbr title="A central processing unit. A CPU is a consumable resource. A compute node typically contains one or more CPUs">CPU</abbr> <strong>cores</strong>. The compute nodes are managed by the job scheduling software (<abbr title="Moab is a job scheduler, which allocates resources for jobs that are requesting resources.">Moab</abbr>) and a Resource Manager (TORQUE), which
decides when and on which compute nodes the jobs can run. It is usually
not necessary to log on to the compute nodes directly 
and is only allowed on the nodes where you have a job running 
. Users can (and should) monitor
their jobs periodically as they run, but do not have to remain connected
to the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> the entire time.</p>
<p>The documentation in this "Running batch jobs" section includes a
description of the general features of job scripts, how to submit them
for execution and how to monitor their progress.</p>
<h2 id="modules">Modules<a class="headerlink" href="#modules" title="Permanent link">#</a></h2>
<p>Software installation and maintenance on a <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> <abbr title="A group of compute nodes.">cluster</abbr> such as the VSC
clusters poses a number of challenges not encountered on a workstation
or a departmental <abbr title="A group of compute nodes.">cluster</abbr>. We therefore need a system on the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr>, which is
able to easily activate or deactivate the software packages that you
require for your program execution.</p>
<h3 id="environment-variables">Environment Variables<a class="headerlink" href="#environment-variables" title="Permanent link">#</a></h3>
<p>The program environment on the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> is controlled by pre-defined settings,
which are stored in environment (or shell) variables. For more
information about environment variables, see <a href="">the chapter "Getting started", section "Variables" in the intro to <abbr title="An operating system, similar to UNIX.">Linux</abbr></a>.</p>
<p>All the software packages that are installed on the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> <abbr title="A group of compute nodes.">cluster</abbr> require
different settings. These packages include compilers, interpreters,
mathematical software such as MATLAB and SAS, as well as other
applications and libraries.</p>
<h3 id="the-module-command">The module command<a class="headerlink" href="#the-module-command" title="Permanent link">#</a></h3>
<p>In order to administer the active software and their environment
variables, the module system has been developed, which:</p>
<ol>
<li>
<p>Activates or deactivates <em>software packages</em> and their dependencies.</p>
</li>
<li>
<p>Allows setting and unsetting of <em>environment variables</em>, including
    adding and deleting entries from list-like environment variables.</p>
</li>
<li>
<p>Does this in a <em>shell-independent</em> fashion (necessary information is
    stored in the accompanying module file).</p>
</li>
<li>
<p>Takes care of <em>versioning aspects</em>: For many libraries, multiple
    versions are installed and maintained. The module system also takes
    care of the versioning of software packages. For instance, it does
    not allow multiple versions to be loaded at same time.</p>
</li>
<li>
<p>Takes care of <em>dependencies</em>: Another issue arises when one
    considers library versions and the dependencies they require. Some
    software requires an older version of a particular library to run
    correctly (or at all). Hence a variety of version numbers is
    available for important libraries. Modules typically load the
    required dependencies automatically.</p>
</li>
</ol>
<p>This is all managed with the <code>module</code> command, which is explained in the
next sections.</p>
<p>There is also a shorter <code>ml</code> command that does exactly the same as the
<code>module</code> command and is easier to type. Whenever you see a <code>module</code>
command, you can replace <code>module</code> with <code>ml</code>.</p>
<h3 id="available-modules">Available <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr><a class="headerlink" href="#available-modules" title="Permanent link">#</a></h3>
<p>A large number of software packages are installed on the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> clusters. A
list of all currently available software can be obtained by typing:</p>
<pre><code><b>$ module available</b>
</code></pre>

<p>It's also possible to execute <code>module av</code> or <code>module avail</code>, these are
shorter to type and will do the same thing.</p>
<p>This will give some output such as:</p>
<pre><code>$ <b>module av 2>\&1 | more</b>
--- /apps/gent/SL6/sandybridge/modules/all ---
ABAQUS/6.12.1-linux-x86_64
AMOS/3.1.0-ictce-4.0.10
ant/1.9.0-Java-1.7.0_40
ASE/3.6.0.2515-ictce-4.1.13-Python-2.7.3
ASE/3.6.0.2515-ictce-5.5.0-Python-2.7.6
...
</code></pre>

<p>Or when you want to check whether some specific software, some compiler or some
application (e.g., MATLAB) is installed on the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr>.</p>
<pre><code>$ <b>module av 2>\&1 | grep -i -e "matlab"</b>
MATLAB/2010b
MATLAB/2012b
MATLAB/2013b
</code></pre>

<p>As you are not aware of the capitals letters in the module name, we looked for
a case-insensitive name with the "-i" option.</p>
<p>This gives a full list of software packages that can be loaded.</p>
<p><strong>The casing of module names is important</strong>: lowercase and uppercase letters matter in module names.</p>
<h3 id="organisation-of-modules-in-toolchains">Organisation of <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> in toolchains<a class="headerlink" href="#organisation-of-modules-in-toolchains" title="Permanent link">#</a></h3>
<p>The amount of <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> on the VSC systems can be overwhelming, and it is
not always immediately clear which <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> can be loaded safely together
if you need to combine multiple programs in a single job to get your
work done.</p>
<p>Therefore the VSC has defined so-called **. A toolchain contains a C/C++
and Fortran compiler, a <abbr title="MPI stands for Message-Passing Interface. It supports a parallel programming method designed for distributed memory systems, but can also be used well on shared memory systems.">MPI</abbr> library and some basic math libraries for
(dense matrix) linear algebra and FFT. Two toolchains are defined on
most VSC systems. One, the <code>intel</code> toolchain, consists of the Intel
compilers, <abbr title="MPI stands for Message-Passing Interface. It supports a parallel programming method designed for distributed memory systems, but can also be used well on shared memory systems.">MPI</abbr> library and math libraries. The other one, the <code>foss</code>
toolchain, consists of Open Source components: the GNU compilers,
OpenMPI, OpenBLAS and the standard LAPACK and ScaLAPACK libraries for
the linear algebra operations and the FFTW library for FFT. The
toolchains are refreshed twice a year, which is reflected in their name.</p>
<p>E.g., <code>foss/2023a</code> is the first version of the <code>foss</code> toolchain in 2023.</p>
<p>The toolchains are then used to compile a lot of the software installed
on the VSC clusters. You can recognise those packages easily as they all
contain the name of the toolchain after the version number in their name
(e.g., <code>Python/2.7.12-intel-2016b</code>). Only packages compiled with the
same toolchain name and version can work together without conflicts.</p>
<h3 id="loading-and-unloading-modules">Loading and unloading <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr><a class="headerlink" href="#loading-and-unloading-modules" title="Permanent link">#</a></h3>
<h4 id="module-load">module load<a class="headerlink" href="#module-load" title="Permanent link">#</a></h4>
<p>To "activate" a software package, you load the corresponding module file
using the <code>module load</code> command:</p>
<pre><code><b>$ module load example</b>
</code></pre>

<p>This will load the most recent version of <em>example</em>.</p>
<p>For some packages, multiple versions are installed; the load command
will automatically choose the default version (if it was set by the
system administrators) or the most recent version otherwise (i.e., the
lexicographical last after the <code>/</code>).</p>
<p>**However, you should specify a particular version to avoid surprises when newer versions are installed:</p>
<pre><code><b>$ module load secondexample/2.7-intel-2016b</b>
</code></pre>

<p>The <code>ml</code> command is a shorthand for <code>module load</code>: <code>ml example/1.2.3</code> is
equivalent to <code>module load example/1.2.3</code>.</p>
<p>Modules need not be loaded one by one; the two <code>module load</code> commands
can be combined as follows:</p>
<pre><code><b>$ module load example/1.2.3 secondexample/2.7-intel-2016b</b>
</code></pre>

<p>This will load the two <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> as well as their dependencies (unless
there are conflicts between both <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr>).</p>
<h4 id="module-list">module list<a class="headerlink" href="#module-list" title="Permanent link">#</a></h4>
<p>Obviously, you need to be able to keep track of the <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> that are
currently loaded. Assuming you have run the <code>module load</code> commands
stated above, you will get the following:</p>
<pre><code><b>$ module list</b>
Currently Loaded Modulefiles: 
1) example/1.2.3                                        6) imkl/11.3.3.210-iimpi-2016b 
2) GCCcore/5.4.0                                        7) intel/2016b 
3) icc/2016.3.210-GCC-5.4.0-2.26                        8) examplelib/1.2-intel-2016b 
4) ifort/2016.3.210-GCC-5.4.0-2.26                      9) secondexample/2.7-intel-2016b 
5) impi/5.1.3.181-iccifort-2016.3.210-GCC-5.4.0-2.26
</code></pre>

<p>You can also just use the <code>ml</code> command without arguments to list loaded <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr>.</p>
<p>It is important to note at this point that other <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> (e.g.,
<code>intel/2016b</code>) are also listed, although the user did not explicitly
load them. This is because <code>secondexample/2.7-intel-2016b</code> depends on it
(as indicated in its name), and the system administrator specified that
the <code>intel/2016b</code> module should be loaded whenever <em>this</em>
<code>secondexample</code> module is loaded. There are advantages and disadvantages
to this, so be aware of automatically loaded <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> whenever things go wrong: they may have something to do with it!</p>
<h4 id="module-unload">module unload<a class="headerlink" href="#module-unload" title="Permanent link">#</a></h4>
<p>To unload a module, one can use the <code>module unload</code> command. It works
consistently with the <code>load</code> command, and reverses the latter's effect.
However, the dependencies of the package are NOT automatically unloaded;
you will have to unload the packages one by one. When the
<code>secondexample</code> module is unloaded, only the following <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> remain:</p>
<pre><code><b>$ module unload secondexample</b>
<b>$ module list</b>
Currently Loaded Modulefiles: 
Currently Loaded Modulefiles: 
1) example/1.2.3                        5) impi/5.1.3.181-iccifort-2016.3.210-GCC-5.4.0-2.26 
2) GCCcore/5.4.0                        6) imkl/11.3.3.210-iimpi-2016b 
3) icc/2016.3.210-GCC-5.4.0-2.26        7) intel/2016b 
4) ifort/2016.3.210-GCC-5.4.0-2.26      8) examplelib/1.2-intel-2016b
</code></pre>

<p>To unload the <code>secondexample</code> module, you can also use
<code>ml -secondexample</code>.</p>
<p>Notice that the version was not specified: there can only be one version
of a module loaded at a time, so unloading <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> by name is not
ambiguous. However, checking the list of currently loaded <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> is
always a good idea, since unloading a module that is currently not
loaded will <em>not</em> result in an error.</p>
<h3 id="purging-all-modules">Purging all <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr><a class="headerlink" href="#purging-all-modules" title="Permanent link">#</a></h3>
<p>In order to unload all <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> at once, and hence be sure to start in a
clean state, you can use:</p>
<pre><code><b>$ module purge</b>
</code></pre>
<p>This is always safe: the <code>cluster</code> module (the module that specifies
which <abbr title="A group of compute nodes.">cluster</abbr> jobs will get submitted to) will not be unloaded (because
it's a so-called "sticky" module). </p>
<h3 id="using-explicit-version-numbers">Using explicit version numbers<a class="headerlink" href="#using-explicit-version-numbers" title="Permanent link">#</a></h3>
<p>Once a module has been installed on the <abbr title="A group of compute nodes.">cluster</abbr>, the executables or
libraries it comprises are never modified. This policy ensures that the
user's programs will run consistently, at least if the user specifies a
specific version. <strong>Failing to specify a version may result in unexpected behviour.</strong></p>
<p>Consider the following example: the user decides to use the <code>example</code>
module and at that point in time, just a single version 1.2.3 is
installed on the <abbr title="A group of compute nodes.">cluster</abbr>. The user loads the module using:</p>
<pre><code><b>$ module load example</b>
</code></pre>

<p>rather than</p>
<pre><code><b>$ module load example/1.2.3</b>
</code></pre>

<p>Everything works fine, up to the point where a new version of <code>example</code>
is installed, 4.5.6. From then on, the user's <code>load</code> command will load
the latter version, rather than the intended one, which may lead to
unexpected problems. See for example <a href="../troubleshooting/#module-conflicts">the following section on Module Conflicts</a>.</p>
<p>Consider the following <code>example</code> <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr>:</p>
<pre><code><b>$ module avail example/</b>
example/1.2.3 
example/4.5.6
</code></pre>

<p>Let's now generate a version conflict with the <code>example</code> module, and see
what happens.</p>
<pre><code><b>$ module av example/</b>
example/1.2.3       example/4.5.6
<b>$ module load example/1.2.3  example/4.5.6</b>
Lmod has detected the following error: A different version of the 'example' module is already loaded (see output of 'ml').
<b>$ module swap example/4.5.6</b>
</code></pre>

<!-- ::: prompt
example/1.2.3 example/4.5.6 example/4.5.6(12):ERROR:150: Module
'example/4.5.6' conflicts with the currently loaded module(s)
'example/1.2.3' example/4.5.6(12):ERROR:102: Tcl command execution
failed: conflict example
::: -->

<p>Note: A <code>module swap</code> command combines the appropriate <code>module unload</code>
and <code>module load</code> commands.</p>
<h3 id="search-for-modules">Search for <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr><a class="headerlink" href="#search-for-modules" title="Permanent link">#</a></h3>
<p>With the <code>module spider</code> command, you can search for <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr>:</p>
<pre><code><b>$ module spider example</b>
--------------------------------------------------------------------------------
  example:
--------------------------------------------------------------------------------
    Description: 
        This is just an example

    Versions: 
        example/1.2.3 
        example/4.5.6
--------------------------------------------------------------------------------
  For detailed information about a specific "example" module (including how to load the modules) use the module's full name. 
  For example:

    module spider example/1.2.3
--------------------------------------------------------------------------------
</code></pre>

<p>It's also possible to get detailed information about a specific module:</p>
<pre><code><b>$ module spider example/1.2.3</b>
------------------------------------------------------------------------------------------
  example: example/1.2.3
------------------------------------------------------------------------------------------
  Description: 
    This is just an example 

    You will need to load all module(s) on any one of the lines below before the "example/1.2.3" module is available to load.

        cluster/doduo 
        cluster/joltik 
        cluster/kirlia 
        cluster/skitty
        cluster/swalot 
        cluster/victini
Help:

        Description 
        =========== 
        This is just an example

        More information 
        ================ 
         - Homepage: https://example.com
</code></pre>

<h3 id="get-detailed-info">Get detailed info<a class="headerlink" href="#get-detailed-info" title="Permanent link">#</a></h3>
<p>To get a list of all possible commands, type:</p>
<pre><code><b>$ module help</b>
</code></pre>

<p>Or to get more information about one specific module package:</p>
<pre><code><b>$ module help example/1.2.3</b>
----------- Module Specific Help for 'example/1.2.3' --------------------------- 
  This is just an example - Homepage: https://example.com/
</code></pre>

<h3 id="save-and-load-collections-of-modules">Save and load collections of <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr><a class="headerlink" href="#save-and-load-collections-of-modules" title="Permanent link">#</a></h3>
<p>If you have a set of <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> that you need to load often, you can save
these in a <em>collection</em>. This will enable you to load all the <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr>
you need with a single command.</p>
<p>In each <code>module</code> command shown below, you can replace <code>module</code> with
<code>ml</code>.</p>
<p>First, load all <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> you want to include in the collections:</p>
<pre><code><b>$ module load example/1.2.3 secondexample/2.7-intel-2016b</b>
</code></pre>

<p>Now store it in a collection using <code>module save</code>. In this example, the
collection is named <code>my-collection</code>.</p>
<pre><code><b>$ module save my-collection</b>
</code></pre>

<p>Later, for example in a jobscript or a new session, you can load all
these <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> with <code>module restore</code>:</p>
<pre><code><b>$ module restore my-collection</b>
</code></pre>

<p>You can get a list of all your saved collections with the
<code>module savelist</code> command:</p>
<pre><code><b>$ module savelistr</b>
Named collection list (For LMOD_SYSTEM_NAME = "CO7-sandybridge"):
  1) my-collection
</code></pre>

<p>To get a list of all <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> a collection will load, you can use the
<code>module describe</code> command:</p>
<pre><code><b>$ module describe my-collection</b>
1) example/1.2.3                                        6) imkl/11.3.3.210-iimpi-2016b 
2) GCCcore/5.4.0                                        7) intel/2016b 
3) icc/2016.3.210-GCC-5.4.0-2.26                        8) examplelib/1.2-intel-2016b 
4) ifort/2016.3.210-GCC-5.4.0-2.26                      9) secondexample/2.7-intel-2016b 
5) impi/5.1.3.181-iccifort-2016.3.210-GCC-5.4.0-2.26
</code></pre>

<p>To remove a collection, remove the corresponding file in
<code>$HOME/.lmod.d</code>:</p>
<pre><code><b>$ rm $HOME/.lmod.d/my-collection</b>
</code></pre>

<h3 id="getting-module-details">Getting module details<a class="headerlink" href="#getting-module-details" title="Permanent link">#</a></h3>
<p>To see how a module would change the environment, you can use the
<code>module show</code> command:</p>
<pre><code><b>$ module show Python/2.7.12-intel-2016b</b>
whatis("Description: Python is a programming language that lets youwork more quickly and integrate your systems more effectively. - Homepage: http://python.org/ ") 
conflict("Python")
load("intel/2016b") 
load("bzip2/1.0.6-intel-2016b") 
...
prepend_path(...)
setenv("EBEXTSLISTPYTHON","setuptools-23.1.0,pip-8.1.2,nose-1.3.7,numpy-1.11.1,scipy-0.17.1,ytz-2016.4", ...)
</code></pre>

<p>It's also possible to use the <code>ml show</code> command instead: they are
equivalent.</p>
<p>Here you can see that the <code>Python/2.7.12-intel-2016b</code> comes with a whole
bunch of extensions: <code>numpy</code>, <code>scipy</code>, ...</p>
<p>You can also see the <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> the <code>Python/2.7.12-intel-2016b</code> module
loads: <code>intel/2016b</code>, <code>bzip2/1.0.6-intel-2016b</code>, ...</p>
<!-- ::: prompt
module-whatis Description: Python is a programming language that lets
you work more quickly and integrate your systems more effectively. -
Homepage: http://python.org/ conflict Python module load foss/2014a
module load bzip2/1.0.6-foss-2014a \... prepend-path \... \... setenv
EBVERSIONPYTHON 3.2.5 setenv EBEXTSLISTPYTHON
distribute-0.6.26,pip-1.1,nose-1.1.2,numpy-1.6.1,scipy-0.10.1
::: -->

<!-- Here you can see that the `Python/3.2.5-foss-2014a` comes with a whole
bunch of extensions: `numpy`, `scipy`, ...

You can also see the modules the `Python/3.2.5-foss-2014a` module loads:
`foss/2014a`, `bzip2/1.0.6-foss-2014a`, ...  -->
<p>If you're not sure what all of this means: don't worry, you don't have to know; just load the module and try to use the software.</p>
<h2 id="getting-system-information-about-the-hpc-infrastructure">Getting system information about the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> infrastructure<a class="headerlink" href="#getting-system-information-about-the-hpc-infrastructure" title="Permanent link">#</a></h2>
<h3 id="checking-the-general-status-of-the-hpc-infrastructure">Checking the general status of the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> infrastructure<a class="headerlink" href="#checking-the-general-status-of-the-hpc-infrastructure" title="Permanent link">#</a></h3>
<p>To check the general system state, check
<a href="https://www.ugent.be/hpc/en/infrastructure/status">https://www.ugent.be/hpc/en/infrastructure/status</a>. This has
information about scheduled downtime, status of the system, ...</p>
<h3 id="getting-cluster-state">Getting <abbr title="A group of compute nodes.">cluster</abbr> state<a class="headerlink" href="#getting-cluster-state" title="Permanent link">#</a></h3>
<p>You can check <a href="http://hpc.ugent.be/clusterstate">http://hpc.ugent.be/clusterstate</a> to see information
about the clusters: you can see the nodes that are down, free, partially
filled with jobs, completely filled with jobs, ....</p>
<p>You can also get this information in text form (per <abbr title="A group of compute nodes.">cluster</abbr> separately)
with the <code>pbsmon</code> command:</p>
<pre><code><b>$ module swap cluster /kirlia</b>
<b>$ pbsmon</b>
 3401 3402 3403 3404 3405 3406 3407
    J    j    j    J    J    j    J

 3408 3409 3410 3411 3412 3413 3414
    J    J    J    J    J    J    J

 3415 3416
    J    J

   _ free                 : 0   |   X down                 : 0   |
   j partial              : 3   |   x down_on_error        : 0   |
   J full                 : 13  |   m maintenance          : 0   |
                                |   . offline              : 0   |
                                |   o other (R, *, ...)    : 0   |

Node type:
 ppn=36, mem=751GB
</code></pre>

<p><code>pbsmon</code> only outputs details of the <abbr title="A group of compute nodes.">cluster</abbr> corresponding to the
currently loaded <code>cluster</code> module see <a href="./#specifying-the-cluster-on-which-to-run">the section on Specifying the <abbr title="A group of compute nodes.">cluster</abbr> on which to run</a>.
It also shows details about the nodes in a <abbr title="A group of compute nodes.">cluster</abbr>. In the example, all
nodes have 36 cores and 751 GB of <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr>.</p>
<h2 id="defining-and-submitting-your-job">Defining and submitting your job<a class="headerlink" href="#defining-and-submitting-your-job" title="Permanent link">#</a></h2>
<p>Usually, you will want to have your program running in batch mode, as
opposed to interactively as you may be accustomed to. The point is that
the program must be able to start and run without user intervention,
i.e., without you having to enter any information or to press any
buttons during program execution. All the necessary input or required
options have to be specified on the command line, or needs to be put in
input or configuration files.</p>
<p>As an example, we will run a Perl script, which you will find in the
examples subdirectory on the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr>. When you received an account to the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> a subdirectory with examples was automatically generated for you.</p>
<p>Remember that you have copied the contents of the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> examples directory
to your home directory, so that you have your <strong>own personal</strong> copy (editable and
over-writable) and that you can start using the examples. If you haven't
done so already, run these commands now:</p>
<pre><code><b>$ cd</b>
<b>$ cp -r /apps/gent/tutorials/Intro-HPC/examples ~/</b>
</code></pre>

<p>First go to the directory with the first examples by entering the
command:</p>
<pre><code><b>$ cd ~/examples/Running-batch-jobs</b>
</code></pre>

<p>Each time you want to execute a program on the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> you'll need 2 things:</p>
<p><strong>The executable</strong>  The program to execute from the end-user, together with its
    peripheral input files, databases and/or command options.</p>
<p><strong>A batch job script</strong> , which will define the computer resource requirements of the
    program, the required additional software packages and which will
    start the actual executable. The <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> needs to know:</p>
<pre><code>1.  the type of compute nodes;

2.  the number of CPUs;

3.  the amount of memory;

4.  the expected duration of the execution time (wall time: Time as
    measured by a clock on the wall);

5.  the name of the files which will contain the output (i.e.,
    stdout) and error (i.e., stderr) messages;

6.  what executable to start, and its arguments.
</code></pre>
<p>Later on, the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> user shall have to define (or to adapt) his/her own job
scripts. For now, all required job scripts for the exercises are
provided for you in the examples subdirectories.</p>
<p>List and check the contents with:</p>
<pre><code><b>$ ls -l</b>
total 512
-rw-r--r-- 1 vsc40000 193 Sep 11 10:34 fibo.pbs
-rw-r--r-- 1 vsc40000 609 Sep 11 10:25 fibo.pl
</code></pre>

<p>In this directory you find a Perl script (named "fibo.pl") and a job
script (named "fibo.pbs").</p>
<ol>
<li>
<p>The Perl script calculates the first 30 Fibonacci numbers.</p>
</li>
<li>
<p>The job script is actually a standard Unix/<abbr title="An operating system, similar to UNIX.">Linux</abbr> shell script that
    contains a few extra comments at the beginning that specify
    directives to PBS. These comments all begin with <strong>#PBS</strong>.</p>
</li>
</ol>
<p>We will first execute the program locally (i.e., on your current
login-<abbr title="A node attribute is a non-quantitative aspect of a node. Attributes typically describe the node itself or possibly aspects of various node resources such as processors or memory. While it is probably not optimal to aggregate node and resource attributes together in this manner, it is common practice. Common node attributes include processor architecture, operating system, and processor speed. Jobs often specify that resources be allocated from nodes possessing certain node attributes.">node</abbr>), so that you can see what the program does.</p>
<p>On the command line, you would run this using:</p>
<pre><code><b>$ ./fibo.pl</b>
[0] -> 0
[1] -> 1
[2] -> 1
[3] -> 2
[4] -> 3
[5] -> 5
[6] -> 8
[7] -> 13
[8] -> 21
[9] -> 34
[10] -> 55
[11] -> 89
[12] -> 144
[13] -> 233
[14] -> 377
[15] -> 610
[16] -> 987
[17] -> 1597
[18] -> 2584
[19] -> 4181
[20] -> 6765
[21] -> 10946
[22] -> 17711
[23] -> 28657
[24] -> 46368
[25] -> 75025
[26] -> 121393
[27] -> 196418
[28] -> 317811
[29] -> 514229
</code></pre>

<p><u>Remark</u>: Recall that you have now executed the Perl script locally on one of
the login-nodes of the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> <abbr title="A group of compute nodes.">cluster</abbr>. Of course, this is not our final
intention; we want to run the script on any of the compute nodes. Also,
it is not considered as good practice, if you "abuse" the login-nodes
for testing your scripts and executables. It will be explained later on
how you can reserve your own compute-<abbr title="A node attribute is a non-quantitative aspect of a node. Attributes typically describe the node itself or possibly aspects of various node resources such as processors or memory. While it is probably not optimal to aggregate node and resource attributes together in this manner, it is common practice. Common node attributes include processor architecture, operating system, and processor speed. Jobs often specify that resources be allocated from nodes possessing certain node attributes.">node</abbr> (by opening an interactive
session) to test your software. But for the sake of acquiring a good
understanding of what is happening, you are pardoned for this example
since these jobs require very little computing power.</p>
<p>The job script contains a description of the job by specifying the
command that need to be executed on the <abbr title="The computational units on which batch or interactive jobs are processed. A compute node is pretty much comparable to a single personal computer. It contains one or more sockets, each holding a single CPU. Some nodes also contain one or more GPGPUs. The compute node is equipped with memory (RAM) that is accessible by all its CPUs.">compute node</abbr>:</p>
<p><center>-- fibo.pbs --</center></p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash -l</span>
<span class="nb">cd</span> <span class="nv">$PBS_O_WORKDIR</span>
./fibo.pl
</code></pre></div>
<p>So, jobs are submitted as scripts (bash, Perl, Python, etc.), which
specify the parameters related to the jobs such as expected runtime
(<abbr title="Walltime is the length of time specified in the job script for which the job will run on a batch system, you can visualise walltime as the time measured by a wall mounted clock (or your digital wristwatch). This is a computational resource.">walltime</abbr>), e-mail notification, etc. These parameters can also be
specified on the command line.</p>
<p>This job script that can now be submitted to the <abbr title="A group of compute nodes.">cluster</abbr>'s job system
for execution, using the qsub (Queue SUBmit) command:</p>
<pre><code><b>$ qsub fibo.pbs</b>
123456
</code></pre>

<p>The qsub command returns a job identifier on the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> <abbr title="A group of compute nodes.">cluster</abbr>. The
important part is the number (e.g., "123456 "); this is a unique identifier for
the job and can be used to monitor and manage your job.</p>
<p><u>Remark</u>: the <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> that were loaded when you submitted the job will <em>not</em> be
loaded when the job is started. You should always specify the
<code>module load</code> statements that are required for your job in the job
script itself.</p>
<p>To faciliate this, you can use a pre-defined module collection which you
can restore using <code>module restore</code>, see <a href="./#save-and-load-collections-of-modules">the section on Save and load collections of <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr></a> for more information.</p>
<p>Your job is now waiting in the <abbr title="PBS/TORQUE queues, or &quot;classes&quot; as Moab refers to them, represent groups of computing resources with specific parameters. A queue with a 12-hour runtime or &quot;walltime&quot; would allow jobs requesting 12 hours or less to use this queue.">queue</abbr> for a free workernode to start on.</p>
<p>Go and drink some coffee ...but not too long. If you get impatient you
can start reading the next section for more information on how to
monitor jobs in the <abbr title="PBS/TORQUE queues, or &quot;classes&quot; as Moab refers to them, represent groups of computing resources with specific parameters. A queue with a 12-hour runtime or &quot;walltime&quot; would allow jobs requesting 12 hours or less to use this queue.">queue</abbr>.</p>
<p>After your job was started, and ended, check the contents of the
directory:</p>
<pre><code><b>$ ls -l</b>
total 768
-rw-r--r-- 1 vsc40000 vsc40000   44 Feb 28 13:33 fibo.pbs
-rw------- 1 vsc40000 vsc40000    0 Feb 28 13:33 fibo.pbs.e123456
-rw------- 1 vsc40000 vsc40000 1010 Feb 28 13:33 fibo.pbs.o123456
-rwxrwxr-x 1 vsc40000 vsc40000  302 Feb 28 13:32 fibo.pl
</code></pre>

<p>Explore the contents of the 2 new files:</p>
<pre><code><b>$ more fibo.pbs.o123456</b>
<b>$ more fibo.pbs.e123456</b>
</code></pre>

<p>These files are used to store the standard output and error that would
otherwise be shown in the terminal window. By default, they have the
same name as that of the PBS script, i.e., "fibo.pbs" as base name,
followed by the extension ".o" (output) and ".e" (error), respectively,
and the job number ('123456' for this example). The error file will be empty,
at least if all went well. If not, it may contain valuable information
to determine and remedy the problem that prevented a successful run. The
standard output file will contain the results of your calculation (here,
the output of the Perl script)</p>
<h3 id="when-will-my-job-start">When will my job start?<a class="headerlink" href="#when-will-my-job-start" title="Permanent link">#</a></h3>
<p>In practice it's impossible to predict when your job(s) will start,
since most currently running jobs will finish before their requested
<abbr title="Walltime is the length of time specified in the job script for which the job will run on a batch system, you can visualise walltime as the time measured by a wall mounted clock (or your digital wristwatch). This is a computational resource.">walltime</abbr> expires, and new jobs by may be submitted by other users that
are assigned a higher priority than your job(s).</p>
<p>The UGent-<abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> clusters use a fair-<abbr title="A share is a remote, mountable file system. Users can mount and access a share on several hosts at a time.">share</abbr> scheduling policy (see <a href="../sites/hpc_policies"><abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> Policies</a>). There is no
guarantee on when a job will start, since it depends on a number of
factors. One of these factors is the priority of the job, which is
determined by</p>
<ul>
<li>
<p>historical use: the aim is to balance usage over users, so
    infrequent (in terms of total compute time used) users get a higher
    priority</p>
</li>
<li>
<p>requested resources (amount of cores, <abbr title="Walltime is the length of time specified in the job script for which the job will run on a batch system, you can visualise walltime as the time measured by a wall mounted clock (or your digital wristwatch). This is a computational resource.">walltime</abbr>, <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr>, ...)</p>
</li>
<li>
<p>time waiting in <abbr title="PBS/TORQUE queues, or &quot;classes&quot; as Moab refers to them, represent groups of computing resources with specific parameters. A queue with a 12-hour runtime or &quot;walltime&quot; would allow jobs requesting 12 hours or less to use this queue.">queue</abbr>: queued jobs get a higher priority over time</p>
</li>
<li>
<p>user limits: this avoids having a single user use the entire
    <abbr title="A group of compute nodes.">cluster</abbr>. This means that each user can only use a part of the
    <abbr title="A group of compute nodes.">cluster</abbr>.</p>
</li>
</ul>
<p>Some other factors are how busy the <abbr title="A group of compute nodes.">cluster</abbr> is, how many workernodes are
active, the resources (e.g., number of cores, <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr>) provided by each
workernode, ...</p>
<p>It might be beneficial to request less resources (e.g., not requesting
all cores in a workernode), since the scheduler often finds a "gap" to
fit the job into more easily.</p>
<p>Sometimes it happens that couple of nodes are free and a job would not
start. Empty nodes are not necessary empty for your job(s). Just
imagine, that an <em>N</em>-<abbr title="A node attribute is a non-quantitative aspect of a node. Attributes typically describe the node itself or possibly aspects of various node resources such as processors or memory. While it is probably not optimal to aggregate node and resource attributes together in this manner, it is common practice. Common node attributes include processor architecture, operating system, and processor speed. Jobs often specify that resources be allocated from nodes possessing certain node attributes.">node</abbr>-job (with a higher priority than your waiting
job(s)) should run. It is quite unlikely that <em>N</em> nodes would be empty
at the same moment to accommodate this job, so while fewer than <em>N</em>
nodes are empty, you can see them as being empty. The moment the <em>N</em>th
<abbr title="A node attribute is a non-quantitative aspect of a node. Attributes typically describe the node itself or possibly aspects of various node resources such as processors or memory. While it is probably not optimal to aggregate node and resource attributes together in this manner, it is common practice. Common node attributes include processor architecture, operating system, and processor speed. Jobs often specify that resources be allocated from nodes possessing certain node attributes.">node</abbr> becomes empty the waiting <em>N</em>-<abbr title="A node attribute is a non-quantitative aspect of a node. Attributes typically describe the node itself or possibly aspects of various node resources such as processors or memory. While it is probably not optimal to aggregate node and resource attributes together in this manner, it is common practice. Common node attributes include processor architecture, operating system, and processor speed. Jobs often specify that resources be allocated from nodes possessing certain node attributes.">node</abbr>-job will consume these <em>N</em> free
nodes.</p>
<h3 id="specifying-the-cluster-on-which-to-run">Specifying the <abbr title="A group of compute nodes.">cluster</abbr> on which to run<a class="headerlink" href="#specifying-the-cluster-on-which-to-run" title="Permanent link">#</a></h3>
<p>To use other clusters, you can swap the <code>cluster</code> module. This is a
special module that change what <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> are available for you, and what
<abbr title="A group of compute nodes.">cluster</abbr> your jobs will be queued in.</p>
<p>By default you are working on victini. To switch to, e.g., skitty you need to redefine
the environment so you get access to all <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> installed on the skitty
<abbr title="A group of compute nodes.">cluster</abbr>, and to be able to submit jobs to the skitty scheduler so your jobs
will start on skitty instead of the default victini <abbr title="A group of compute nodes.">cluster</abbr>.</p>
<pre><code><b>$ module swap cluster/skitty</b>
</code></pre>

<p>Note: the skitty <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr> may not work directly on the login nodes, because the
login nodes do not have the same architecture as the skitty <abbr title="A group of compute nodes.">cluster</abbr>, they have
the same architecture as the victini <abbr title="A group of compute nodes.">cluster</abbr> however, so this is why by default
software works on the login nodes. See <a href="../intro-HPC/troubleshooting/#running-software-that-is-incompatible-with-host">the section on Running software that is incompatible with host</a> for why this is and how to fix
this.</p>
<p>To list the available <abbr title="A group of compute nodes.">cluster</abbr> <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr>, you can use the
<code>module avail cluster/</code> command:</p>
<pre><code><b>$ module avail cluster/</b>
------------------------------------------------------------------------------------ 
/etc/modulefiles/vsc
------------------------------------------------------------------------------------
   cluster/doduo (S)    cluster/joltik (S)    cluster/kirlia (S)    
   cluster/skitty (S)    cluster/swalot (S)    cluster/victini (S,L)

  Where:
   S:  Module is Sticky, requires --force to unload or purge
   L:  Module is loaded

If you need software that is not listed, 
request it via <a href="https://www.ugent.be/hpc/en/support/software-installation-request">https://www.ugent.be/hpc/en/support/software-installation-request</a>
</code></pre>

<p>As indicated in the output above, each <code>cluster</code> module is a so-called
sticky module, i.e., it will not be unloaded when <code>module purge</code> (see <a href="./#purging-all-modules">the section on purging <abbr title="HPC uses an open source software package called &quot;Environment Modules&quot; (Modules for short) which allows you to add various path definitions to your shell environment.">modules</abbr></a>)
is used.</p>
<p>The output of the various commands interacting with jobs (<code>qsub</code>,
<code>stat</code>, ...) all depend on which <code>cluster</code> module is loaded.</p>
<h2 id="monitoring-and-managing-your-jobs">Monitoring and managing your job(s)<a class="headerlink" href="#monitoring-and-managing-your-jobs" title="Permanent link">#</a></h2>
<p>Using the job ID that <code>qsub</code> returned, there are various ways to monitor
the status of your job. In the following commands, replace <code>12345</code> with
the job ID <code>qsub</code> returned.</p>
<pre><code><b>$ qstat 12345</b>
</code></pre>

<p>To show on which compute nodes your job is running, at least, when it is
running:</p>
<pre><code><b>$ qstat -n 12345</b>
</code></pre>

<p>To remove a job from the <abbr title="PBS/TORQUE queues, or &quot;classes&quot; as Moab refers to them, represent groups of computing resources with specific parameters. A queue with a 12-hour runtime or &quot;walltime&quot; would allow jobs requesting 12 hours or less to use this queue.">queue</abbr> so that it will not run, or to stop a job
that is already running.</p>
<pre><code><b>$ qdel 12345</b>
</code></pre>

<p>When you have submitted several jobs (or you just forgot about the job
ID), you can retrieve the status of all your jobs that are submitted and
are not yet finished using:</p>
<pre><code><b>$ qstat</b>
:
Job ID      Name    User      Time Use S Queue
----------- ------- --------- -------- - -----
123456 ....     mpi  vsc40000     0    Q short
</code></pre>

<p>Here:</p>
<p><strong>Job ID</strong>      the job's unique identifier</p>
<p><strong>Name</strong>        the name of the job</p>
<p><strong>User</strong>        the user that owns the job</p>
<p><strong>Time Use</strong>    the elapsed <abbr title="Walltime is the length of time specified in the job script for which the job will run on a batch system, you can visualise walltime as the time measured by a wall mounted clock (or your digital wristwatch). This is a computational resource.">walltime</abbr> for the job</p>
<p><strong>Queue</strong>       the <abbr title="PBS/TORQUE queues, or &quot;classes&quot; as Moab refers to them, represent groups of computing resources with specific parameters. A queue with a 12-hour runtime or &quot;walltime&quot; would allow jobs requesting 12 hours or less to use this queue.">queue</abbr> the job is in</p>
<p>The state S can be any of the following:</p>
<table>
<thead>
<tr>
<th align="left"><a href=""></a> State</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><strong>Q</strong></td>
<td align="left">The job is <strong>queued</strong> and is waiting to start.</td>
</tr>
<tr>
<td align="left"><strong>R</strong></td>
<td align="left">The job is currently <strong>running</strong>.</td>
</tr>
<tr>
<td align="left"><strong>E</strong></td>
<td align="left">The job is currently <strong>exit</strong> after having run.</td>
</tr>
<tr>
<td align="left"><strong>C</strong></td>
<td align="left">The job is <strong>completed</strong> after having run.</td>
</tr>
<tr>
<td align="left"><strong>H</strong></td>
<td align="left">The job has a user or system <strong>hold</strong> on it and will not be eligible to run until the hold is removed.</td>
</tr>
</tbody>
</table>
<p>User hold means that the user can remove the hold. System hold means
that the system or an administrator has put the job on hold, very likely
because something is wrong with it. Check with your helpdesk to see why
this is the case.</p>
<h2 id="examining-the-queue">Examining the <abbr title="PBS/TORQUE queues, or &quot;classes&quot; as Moab refers to them, represent groups of computing resources with specific parameters. A queue with a 12-hour runtime or &quot;walltime&quot; would allow jobs requesting 12 hours or less to use this queue.">queue</abbr><a class="headerlink" href="#examining-the-queue" title="Permanent link">#</a></h2>
<p>There is currently (since May 2019) no way to get an overall view of the
state of the <abbr title="A group of compute nodes.">cluster</abbr> queues for the UGent-<abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> infrastructure, due to changes to
the <abbr title="A group of compute nodes.">cluster</abbr> resource management software (and also because a general
overview is mostly meaningless since it doesn't give any indication of
the resources requested by the queued jobs).</p>
<h2 id="specifying-job-requirements">Specifying job requirements<a class="headerlink" href="#specifying-job-requirements" title="Permanent link">#</a></h2>
<p>Without giving more information about your job upon submitting it with <strong>qsub</strong>,
default values will be assumed that are almost never appropriate for
real jobs.</p>
<p>It is important to estimate the resources you need to successfully run
your program, such as the amount of time the job will require, the
amount of <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr> it needs, the number of CPUs it will run on, etc. This
may take some work, but it is necessary to ensure your jobs will run
properly.</p>
<h3 id="generic-resource-requirements">Generic resource requirements<a class="headerlink" href="#generic-resource-requirements" title="Permanent link">#</a></h3>
<p>The <strong>qsub</strong> command takes several options to specify the requirements, of which
we list the most commonly used ones below.</p>
<pre><code><b>$ qsub -l walltime=2:30:00</b>
</code></pre>

<p>For the simplest cases, only the amount of maximum estimated execution
time (called "<abbr title="Walltime is the length of time specified in the job script for which the job will run on a batch system, you can visualise walltime as the time measured by a wall mounted clock (or your digital wristwatch). This is a computational resource.">walltime</abbr>") is really important. Here, the job requests 2
hours, 30 minutes. As soon as the job exceeds the requested <abbr title="Walltime is the length of time specified in the job script for which the job will run on a batch system, you can visualise walltime as the time measured by a wall mounted clock (or your digital wristwatch). This is a computational resource.">walltime</abbr>, it
will be "killed" (terminated) by the job scheduler. There is no harm if
you <em>slightly</em> overestimate the maximum execution time. If you omit this
option, the <abbr title="PBS/TORQUE queues, or &quot;classes&quot; as Moab refers to them, represent groups of computing resources with specific parameters. A queue with a 12-hour runtime or &quot;walltime&quot; would allow jobs requesting 12 hours or less to use this queue.">queue</abbr> manager will not complain but use a default value (one
hour on most clusters).</p>
<p>The maximum <abbr title="Walltime is the length of time specified in the job script for which the job will run on a batch system, you can visualise walltime as the time measured by a wall mounted clock (or your digital wristwatch). This is a computational resource.">walltime</abbr> for <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr>-UGent clusters is <strong>72 hours</strong>.</p>
<p>If you want to run some final steps (for example to copy files back)
before the <abbr title="Walltime is the length of time specified in the job script for which the job will run on a batch system, you can visualise walltime as the time measured by a wall mounted clock (or your digital wristwatch). This is a computational resource.">walltime</abbr> kills your main process, you have to kill the main
command yourself before the <abbr title="Walltime is the length of time specified in the job script for which the job will run on a batch system, you can visualise walltime as the time measured by a wall mounted clock (or your digital wristwatch). This is a computational resource.">walltime</abbr> runs out and then copy the file
back. See <a href="../jobscript_examples/#running-a-command-with-a-maximum-time-limit">the section on Running a command with a maximum time limit</a> for how to do this.</p>
<pre><code><b>$ qsub -l mem=4gb</b>
</code></pre>

<p>The job requests 4 GB of RAM <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr>. As soon as the job tries to use
more <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr>, it will be "killed" (terminated) by the job scheduler.
There is no harm if you <em>slightly</em> overestimate the requested <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr>.</p>
<p>The default <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr> reserved for a job on any given <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr>-UGent <abbr title="A group of compute nodes.">cluster</abbr> is
the "usable <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr> per <abbr title="A node attribute is a non-quantitative aspect of a node. Attributes typically describe the node itself or possibly aspects of various node resources such as processors or memory. While it is probably not optimal to aggregate node and resource attributes together in this manner, it is common practice. Common node attributes include processor architecture, operating system, and processor speed. Jobs often specify that resources be allocated from nodes possessing certain node attributes.">node</abbr>" divided by the "numbers of cores in a <abbr title="A node attribute is a non-quantitative aspect of a node. Attributes typically describe the node itself or possibly aspects of various node resources such as processors or memory. While it is probably not optimal to aggregate node and resource attributes together in this manner, it is common practice. Common node attributes include processor architecture, operating system, and processor speed. Jobs often specify that resources be allocated from nodes possessing certain node attributes.">node</abbr>"
multiplied by the requested <abbr title="A processing unit. A processor is a consumable resource. A processor can be a CPU or a (GP)GPU.">processor</abbr> <abbr title="An individual compute unit inside a CPU. A CPU typically contains one or more cores.">core</abbr>(s) (ppn). Jobs will request
the default <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr> without defining <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr> for the job, either as a
command line option or as a <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr> directive in the job script. Please
note that using the default <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr> is recommended. For "usable <abbr title="A quantity of physical memory (RAM). Memory is provided by compute nodes. It is required as a constraint or consumed as a consumable resource by jobs. Within Moab, memory is tracked and reported in megabytes (MB).">memory</abbr>
per <abbr title="A node attribute is a non-quantitative aspect of a node. Attributes typically describe the node itself or possibly aspects of various node resources such as processors or memory. While it is probably not optimal to aggregate node and resource attributes together in this manner, it is common practice. Common node attributes include processor architecture, operating system, and processor speed. Jobs often specify that resources be allocated from nodes possessing certain node attributes.">node</abbr>" and "number of cores in a <abbr title="A node attribute is a non-quantitative aspect of a node. Attributes typically describe the node itself or possibly aspects of various node resources such as processors or memory. While it is probably not optimal to aggregate node and resource attributes together in this manner, it is common practice. Common node attributes include processor architecture, operating system, and processor speed. Jobs often specify that resources be allocated from nodes possessing certain node attributes.">node</abbr>" please consult
<a href="https://www.ugent.be/hpc/en/infrastructure">https://www.ugent.be/hpc/en/infrastructure</a>.</p>
<pre><code><b>$ qsub -l nodes=5:ppn=2</b>
</code></pre>

<p>The job requests 5 compute nodes with two cores on each <abbr title="A node attribute is a non-quantitative aspect of a node. Attributes typically describe the node itself or possibly aspects of various node resources such as processors or memory. While it is probably not optimal to aggregate node and resource attributes together in this manner, it is common practice. Common node attributes include processor architecture, operating system, and processor speed. Jobs often specify that resources be allocated from nodes possessing certain node attributes.">node</abbr> (ppn stands
for "processors per <abbr title="A node attribute is a non-quantitative aspect of a node. Attributes typically describe the node itself or possibly aspects of various node resources such as processors or memory. While it is probably not optimal to aggregate node and resource attributes together in this manner, it is common practice. Common node attributes include processor architecture, operating system, and processor speed. Jobs often specify that resources be allocated from nodes possessing certain node attributes.">node</abbr>", where "processors" here actually means
"<abbr title="A central processing unit. A CPU is a consumable resource. A compute node typically contains one or more CPUs">CPU</abbr> cores").</p>
<pre><code><b>$ qsub -l nodes=1:westmere</b>
</code></pre>

<p>The job requests just one <abbr title="A node attribute is a non-quantitative aspect of a node. Attributes typically describe the node itself or possibly aspects of various node resources such as processors or memory. While it is probably not optimal to aggregate node and resource attributes together in this manner, it is common practice. Common node attributes include processor architecture, operating system, and processor speed. Jobs often specify that resources be allocated from nodes possessing certain node attributes.">node</abbr>, but it should have an Intel Westmere
<abbr title="A processing unit. A processor is a consumable resource. A processor can be a CPU or a (GP)GPU.">processor</abbr>. A list with site-specific properties can be found in the next
section or in the User Portal ("VSC hardware" section)<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup> of the VSC
website.</p>
<p>These options can either be specified on the command line, e.g.</p>
<pre><code><b>$ qsub -l nodes=1:ppn,mem=2gb fibo.pbs</b>
</code></pre>

<p>or in the job script itself using the #PBS-directive, so "fibo.pbs"
could be modified to:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash -l</span>
<span class="c1">#PBS -l nodes=1:ppn=1</span>
<span class="c1">#PBS -l mem=2gb</span>
<span class="nb">cd</span> <span class="nv">$PBS_O_WORKDIR</span>
./fibo.pl
</code></pre></div>
<p>Note that the resources requested on the command line will override
those specified in the PBS file.</p>
<h2 id="job-output-and-error-files">Job output and error files<a class="headerlink" href="#job-output-and-error-files" title="Permanent link">#</a></h2>
<p>At some point your job finishes, so you may no longer see the job ID in
the list of jobs when you run <em>qstat</em> (since it will only be listed for
a few minutes after completion with state "C"). After your job finishes,
you should see the standard output and error of your job in two files,
located by default in the directory where you issued the <em>qsub</em> command.</p>
<p>When you navigate to that directory and list its contents, you should
see them:</p>
<pre><code><b>$ ls -l</b>
total 1024
-rw-r--r-- 1 vsc40000  609 Sep 11 10:54 fibo.pl
-rw-r--r-- 1 vsc40000   68 Sep 11 10:53 fibo.pbs
-rw------- 1 vsc40000   52 Sep 11 11:03 fibo.pbs.e123456
-rw------- 1 vsc40000 1307 Sep 11 11:03 fibo.pbs.o123456
</code></pre>

<p>In our case, our job has created both output ('fibo.pbs.') and error
files ('fibo.pbs.') containing info written to <em>stdout</em> and <em>stderr</em>
respectively.</p>
<p>Inspect the generated output and error files:</p>
<pre><code><b>$ cat fibo.pbs.o123456</b>
...
<b>$ cat fibo.pbs.e123456</b>
...
</code></pre>

<h2 id="e-mail-notifications">E-mail notifications<a class="headerlink" href="#e-mail-notifications" title="Permanent link">#</a></h2>
<h3 id="generate-your-own-e-mail-notifications">Generate your own e-mail notifications<a class="headerlink" href="#generate-your-own-e-mail-notifications" title="Permanent link">#</a></h3>
<p>You can instruct the <abbr title="High Performance Computing, high performance computing and multiple-task computing on a supercomputer.">HPC</abbr> to send an e-mail to your e-mail address whenever a
job <strong>b</strong>egins, <strong>e</strong>nds and/or <strong>a</strong>borts, by adding the following lines to the job
script <code>fibo.pbs</code>:</p>
<div class="highlight"><pre><span></span><code><span class="c1">#PBS -m b </span>
<span class="c1">#PBS -m e </span>
<span class="c1">#PBS -m a</span>
</code></pre></div>
<p>or</p>
<div class="highlight"><pre><span></span><code><span class="c1">#PBS -m abe</span>
</code></pre></div>
<p>These options can also be specified on the command line. Try it and see
what happens:</p>
<pre><code><b>$ qsub -m abe fibo.pbs</b>
</code></pre>

<p>The system will use the e-mail address that is connected to your VSC
account. You can also specify an alternate e-mail address with the <code>-M</code>
option:</p>
<pre><code><b>$ qsub -m b -M john.smith@example.com fibo.pbs</b>
</code></pre>

<p>will send an e-mail to john.smith@example.com when the job begins.</p>
<h2 id="running-a-job-after-another-job">Running a job after another job<a class="headerlink" href="#running-a-job-after-another-job" title="Permanent link">#</a></h2>
<p>If you submit two jobs expecting that should be run one after another
(for example because the first generates a file the second needs), there
might be a problem as they might both be run at the same time.</p>
<p>So the following example might go wrong:</p>
<pre><code><b>$ qsub job1.sh</b>
<b>$ qsub job2.sh</b>
</code></pre>

<p>You can make jobs that depend on other jobs. This can be useful for
breaking up large jobs into smaller jobs that can be run in a pipeline.
The following example will submit 2 jobs, but the second job (<code>job2.sh</code>)
will be held (<code>H</code> status in <code>qstat</code>) until the first job successfully
completes. If the first job fails, the second will be cancelled.</p>
<pre><code><b>$ FIRST_ID=$ (qsub job1.sh)</b>
<b>$ qsub -W depend=afterok:$FIRST_ID job2.sh</b>
</code></pre>

<p><code>afterok</code> means "After OK", or in other words, after the first job
successfully completed.</p>
<p>It's also possible to use <code>afternotok</code> ("After not OK") to run the
second job only if the first job exited with errors. A third option is
to use <code>afterany</code> ("After any"), to run the second job after the first
job (regardless of success or failure).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>URL:
<a href="https://vscdocumentation.readthedocs.io/en/latest/hardware.html">https://vscdocumentation.readthedocs.io/en/latest/hardware.html</a>&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>

              
            </article>
            
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../connecting/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Connecting to the HPC infrastructure" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Connecting to the HPC infrastructure
            </div>
          </div>
        </a>
      
      
        
        <a href="../running_interactive_jobs/" class="md-footer__link md-footer__link--next" aria-label="Next: Running interactive jobs" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Running interactive jobs
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.top", "navigation.expand", "navigation.tracking", "toc.follow", "navigation.sections", "navigation.instant", "search.suggest", "search.highlight"], "search": "../assets/javascripts/workers/search.b97dbffb.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.6c7ad80a.min.js"></script>
      
    
  </body>
</html>